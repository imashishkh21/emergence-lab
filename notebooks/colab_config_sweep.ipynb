{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Config Sweep: Finding Optimal Specialization Parameters\n\nSystematically test **34 training configurations** across specialization mechanisms:\n- Diversity bonus & niche pressure (Groups A, B)\n- Freeze-Evolve phase ratios & mutation boost (Groups D, E)\n- Pure evolution baselines (Group F)\n- Field ON/OFF baselines (Group G)\n\nEach config runs **3 seeds** with variable iterations per training mode:\n- GRADIENT: 5 iters (~1.3M agent-steps)\n- FREEZE_EVOLVE: 15 iters (~3.9M agent-steps)\n- EVOLVE: 10 iters (~2.6M agent-steps)\n\n## Metrics Tracked\n| Metric | Description |\n|--------|-------------|\n| Hidden food collected | Coordination task success |\n| Weight divergence | Genetic differentiation |\n| Final population | Population stability |\n| Regular food collected | Basic foraging |\n\n## Runtime\n- ~3 hours on TPU v6e + High-RAM\n- Resume-safe: re-run skips completed configs\n- Results saved to Drive after every config"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import drive\ndrive.mount('/content/drive')\n\nimport os\nCHECKPOINT_BASE = '/content/drive/MyDrive/emergence-lab'\nSWEEP_DIR = os.path.join(CHECKPOINT_BASE, 'config_sweep')\nos.makedirs(SWEEP_DIR, exist_ok=True)\n\nREPO_DIR = '/content/emergence-lab'\nGITHUB_USERNAME = \"imashishkh21\"\n\nif not os.path.exists(REPO_DIR):\n    !git clone https://github.com/{GITHUB_USERNAME}/emergence-lab.git {REPO_DIR}\nelse:\n    !cd {REPO_DIR} && git pull\n\nos.chdir(REPO_DIR)\n!pip install -e \".[dev,phase5]\" -q\n!pip install rliable -q\n\nimport jax\nprint(f\"JAX version: {jax.__version__}\")\nprint(f\"Devices: {jax.devices()}\")\nprint(f\"Device count: {jax.device_count()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import traceback\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.configs import Config, TrainingMode\n",
    "from src.training.parallel_train import ParallelTrainer, sample_actions_per_agent\n",
    "from src.agents.network import ActorCritic\n",
    "from src.agents.policy import get_deterministic_actions\n",
    "from src.environment.env import reset, step\n",
    "from src.environment.obs import get_observations, obs_dim\n",
    "from src.analysis.specialization import compute_weight_divergence\n",
    "\n",
    "# --- Constants ---\n",
    "NUM_SEEDS = 3\n",
    "SEED_IDS = [0, 1, 2]\n",
    "NUM_ENVS = 32\n",
    "NUM_STEPS = 128\n",
    "MAX_AGENTS = 64\n",
    "SPI = NUM_ENVS * NUM_STEPS * MAX_AGENTS  # 262,144 Steps Per Iteration\n",
    "\n",
    "# Variable iterations per training mode\n",
    "ITERS_GRADIENT = 5       # ~1.3M agent-steps, ~4 min/config\n",
    "ITERS_FREEZE_EVOLVE = 15 # ~3.9M agent-steps, ~8.5 min/config\n",
    "ITERS_EVOLVE = 10        # ~2.6M agent-steps, ~6 min/config\n",
    "\n",
    "ITERS_MAP = {\n",
    "    TrainingMode.GRADIENT: ITERS_GRADIENT,\n",
    "    TrainingMode.FREEZE_EVOLVE: ITERS_FREEZE_EVOLVE,\n",
    "    TrainingMode.EVOLVE: ITERS_EVOLVE,\n",
    "}\n",
    "\n",
    "print(f\"Steps Per Iteration (SPI): {SPI:,}\")\n",
    "print(f\"GRADIENT: {ITERS_GRADIENT} iters = {ITERS_GRADIENT * SPI:,} agent-steps\")\n",
    "print(f\"FREEZE_EVOLVE: {ITERS_FREEZE_EVOLVE} iters = {ITERS_FREEZE_EVOLVE * SPI:,} agent-steps\")\n",
    "print(f\"EVOLVE: {ITERS_EVOLVE} iters = {ITERS_EVOLVE * SPI:,} agent-steps\")\n",
    "\n",
    "# Dependency check\n",
    "assert hasattr(TrainingMode, 'FREEZE_EVOLVE'), \"TrainingMode.FREEZE_EVOLVE not found!\"\n",
    "assert callable(sample_actions_per_agent), \"sample_actions_per_agent not importable!\"\n",
    "print(\"\\nDependency check PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass SweepConfig:\n    \"\"\"A named config for the sweep.\"\"\"\n    name: str\n    group: str\n    config: Config\n\n\ndef make_base_config() -> Config:\n    \"\"\"Proven 64-agent hidden food base config.\"\"\"\n    cfg = Config()\n    # Grid and agents\n    cfg.env.grid_size = 32\n    cfg.env.num_agents = 16\n    cfg.env.num_food = 40\n    # Evolution\n    cfg.evolution.enabled = True\n    cfg.evolution.max_agents = MAX_AGENTS\n    cfg.evolution.starting_energy = 200\n    cfg.evolution.food_energy = 100\n    cfg.evolution.reproduce_threshold = 120\n    cfg.evolution.reproduce_cost = 40\n    cfg.evolution.mutation_std = 0.01\n    # Hidden food (value_multiplier defaults to 5.0)\n    cfg.env.hidden_food.enabled = True\n    cfg.env.hidden_food.num_hidden = 3\n    cfg.env.hidden_food.required_agents = 3\n    cfg.env.hidden_food.reveal_distance = 3\n    # Field\n    cfg.field.diffusion_rate = 0.1\n    cfg.field.decay_rate = 0.05\n    cfg.field.write_strength = 1.0\n    # Training\n    cfg.train.total_steps = 10_000_000\n    cfg.train.num_envs = NUM_ENVS\n    cfg.train.num_steps = NUM_STEPS\n    cfg.log.wandb = False\n    cfg.log.save_interval = 0\n    return cfg\n\n\ndef build_sweep_configs() -> list[SweepConfig]:\n    \"\"\"Build all 34 sweep configurations.\"\"\"\n    configs = []\n\n    # ---- Group A1: diversity_bonus sweep (5 configs) ----\n    for db in [0.0, 0.05, 0.1, 0.2, 0.5]:\n        cfg = make_base_config()\n        cfg.train.training_mode = TrainingMode.GRADIENT\n        cfg.specialization.diversity_bonus = db\n        configs.append(SweepConfig(\n            name=f\"A1_db{db}\", group=\"A1_diversity_bonus\", config=cfg,\n        ))\n\n    # ---- Group A3: mutation_std sweep (5 configs) ----\n    for ms in [0.005, 0.01, 0.02, 0.05, 0.1]:\n        cfg = make_base_config()\n        cfg.train.training_mode = TrainingMode.GRADIENT\n        cfg.evolution.mutation_std = ms\n        configs.append(SweepConfig(\n            name=f\"A3_ms{ms}\", group=\"A3_mutation_std\", config=cfg,\n        ))\n\n    # ---- Group B: diversity x niche grid (6 configs) ----\n    for db in [0.1, 0.2, 0.5]:\n        for np_ in [0.05, 0.1]:\n            cfg = make_base_config()\n            cfg.train.training_mode = TrainingMode.GRADIENT\n            cfg.specialization.diversity_bonus = db\n            cfg.specialization.niche_pressure = np_\n            configs.append(SweepConfig(\n                name=f\"B_db{db}_np{np_}\", group=\"B_div_x_niche\", config=cfg,\n            ))\n\n    # ---- Group G1: baseline field ON (1 config) ----\n    cfg = make_base_config()\n    cfg.train.training_mode = TrainingMode.GRADIENT\n    configs.append(SweepConfig(\n        name=\"G1_baseline_field_on\", group=\"G_baseline\", config=cfg,\n    ))\n\n    # ---- Group G2: baseline field OFF (1 config) ----\n    cfg = make_base_config()\n    cfg.train.training_mode = TrainingMode.GRADIENT\n    cfg.field.decay_rate = 1.0\n    cfg.field.diffusion_rate = 0.0\n    cfg.field.write_strength = 0.0\n    configs.append(SweepConfig(\n        name=\"G2_baseline_field_off\", group=\"G_baseline\", config=cfg,\n    ))\n\n    # ---- Group D1: FE ratio sweep (5 configs) ----\n    fe_ratios = [\n        (\"95_5\",  14, 1),   # ~14:1 ratio\n        (\"90_10\",  9, 1),   # ~9:1 ratio\n        (\"80_20\",  4, 1),   # 4:1 ratio\n        (\"60_40\",  3, 2),   # 3:2 ratio\n        (\"50_50\",  1, 1),   # 1:1 ratio\n    ]\n    for label, grad_iters, evolve_iters in fe_ratios:\n        cfg = make_base_config()\n        cfg.train.training_mode = TrainingMode.FREEZE_EVOLVE\n        cfg.freeze_evolve.gradient_steps = grad_iters * SPI\n        cfg.freeze_evolve.evolve_steps = evolve_iters * SPI\n        cfg.freeze_evolve.evolve_mutation_boost = 5.0\n        configs.append(SweepConfig(\n            name=f\"D1_fe_ratio_{label}\", group=\"D1_FE_ratio\", config=cfg,\n        ))\n\n    # ---- Group D2: FE mutation boost sweep (4 configs) ----\n    for boost in [2.0, 5.0, 10.0, 20.0]:\n        cfg = make_base_config()\n        cfg.train.training_mode = TrainingMode.FREEZE_EVOLVE\n        cfg.freeze_evolve.gradient_steps = 4 * SPI   # 4:1 ratio\n        cfg.freeze_evolve.evolve_steps = 1 * SPI\n        cfg.freeze_evolve.evolve_mutation_boost = boost\n        configs.append(SweepConfig(\n            name=f\"D2_fe_boost{int(boost)}\", group=\"D2_FE_boost\", config=cfg,\n        ))\n\n    # ---- Group E: FE + diversity rewards (4 configs) ----\n    fe_div_combos = [\n        (\"4_1_db02_np01\", 4, 1, 0.2, 0.1),\n        (\"3_2_db02_np01\", 3, 2, 0.2, 0.1),\n        (\"4_1_db01_np005\", 4, 1, 0.1, 0.05),\n        (\"3_2_db01_np005\", 3, 2, 0.1, 0.05),\n    ]\n    for label, grad_i, evolve_i, db, np_ in fe_div_combos:\n        cfg = make_base_config()\n        cfg.train.training_mode = TrainingMode.FREEZE_EVOLVE\n        cfg.freeze_evolve.gradient_steps = grad_i * SPI\n        cfg.freeze_evolve.evolve_steps = evolve_i * SPI\n        cfg.freeze_evolve.evolve_mutation_boost = 5.0\n        cfg.specialization.diversity_bonus = db\n        cfg.specialization.niche_pressure = np_\n        configs.append(SweepConfig(\n            name=f\"E_fe_div_{label}\", group=\"E_FE_diversity\", config=cfg,\n        ))\n\n    # ---- Group F: pure evolve (3 configs) ----\n    for ms in [0.01, 0.05, 0.1]:\n        cfg = make_base_config()\n        cfg.train.training_mode = TrainingMode.EVOLVE\n        cfg.evolution.mutation_std = ms\n        cfg.freeze_evolve.evolve_mutation_boost = 5.0\n        configs.append(SweepConfig(\n            name=f\"F_evolve_ms{ms}\", group=\"F_pure_evolve\", config=cfg,\n        ))\n\n    return configs\n\n\n# Build and verify\nall_configs = build_sweep_configs()\nprint(f\"Total configs: {len(all_configs)}\")\nassert len(all_configs) == 34, f\"Expected 34 configs, got {len(all_configs)}\"\n\n# Sort: GRADIENT first, then FREEZE_EVOLVE, then EVOLVE\nmode_order = {TrainingMode.GRADIENT: 0, TrainingMode.FREEZE_EVOLVE: 1, TrainingMode.EVOLVE: 2}\nall_configs.sort(key=lambda sc: mode_order[sc.config.train.training_mode])\n\n# Print summary\nfrom collections import Counter\ngroup_counts = Counter(sc.group for sc in all_configs)\nmode_counts = Counter(sc.config.train.training_mode.value for sc in all_configs)\nprint(f\"\\nBy group: {dict(group_counts)}\")\nprint(f\"By mode: {dict(mode_counts)}\")\n\n# Time estimate\nt_grad = mode_counts.get('gradient', 0) * 4\nt_fe = mode_counts.get('freeze_evolve', 0) * 8.5\nt_evolve = mode_counts.get('evolve', 0) * 6\nprint(f\"\\nEstimated runtime: {t_grad:.0f} + {t_fe:.0f} + {t_evolve:.0f} = {t_grad + t_fe + t_evolve:.0f} min (~{(t_grad + t_fe + t_evolve) / 60:.1f} hours)\")\n\n# Verify FREEZE_EVOLVE configs have steps as multiples of SPI\n# (EVOLVE mode doesn't use freeze_evolve phase switching, so skip those)\nfor sc in all_configs:\n    if sc.config.train.training_mode == TrainingMode.FREEZE_EVOLVE:\n        gs = sc.config.freeze_evolve.gradient_steps\n        es = sc.config.freeze_evolve.evolve_steps\n        assert gs % SPI == 0, f\"{sc.name}: gradient_steps {gs} not multiple of SPI\"\n        assert es % SPI == 0, f\"{sc.name}: evolve_steps {es} not multiple of SPI\"\n\nprint(\"\\nAll FREEZE_EVOLVE configs verified: gradient_steps and evolve_steps are multiples of SPI\")\n\n# List all configs\nprint(f\"\\n{'Name':<30} {'Group':<20} {'Mode':<15} {'Iters':>5}\")\nprint(\"-\" * 75)\nfor sc in all_configs:\n    iters = ITERS_MAP[sc.config.train.training_mode]\n    print(f\"{sc.name:<30} {sc.group:<20} {sc.config.train.training_mode.value:<15} {iters:>5}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Eval Functions: shared-params AND per-agent\n# =============================================================================\n\ndef run_hidden_food_eval_shared(network, params, config, key, num_eval_steps=200):\n    \"\"\"Evaluate using SHARED params (GRADIENT mode).\"\"\"\n    env_state = reset(key, config)\n\n    total_reward = 0.0\n    regular_food = 0.0\n    hf_collected = 0.0\n\n    for _ in range(num_eval_steps):\n        obs = get_observations(env_state, config)  # (max_agents, obs_dim)\n        # get_deterministic_actions expects (num_envs, num_agents, obs_dim)\n        actions = get_deterministic_actions(network, params, obs[None, :, :])\n        actions = actions[0]  # (max_agents,)\n\n        env_state, rewards, dones, info = step(env_state, actions, config)\n        # Mask rewards by alive agents to avoid dead-slot noise\n        alive_rewards = rewards * env_state.agent_alive.astype(jnp.float32)\n        total_reward += float(jnp.sum(alive_rewards))\n        regular_food += float(info['food_collected_this_step'])\n        hf_collected += float(info['hidden_food_collected_this_step'])\n\n    final_population = int(jnp.sum(env_state.agent_alive))\n    return {\n        'total_reward': total_reward,\n        'regular_food_collected': regular_food,\n        'hidden_food_collected': hf_collected,\n        'final_population': final_population,\n    }\n\n\ndef run_hidden_food_eval_per_agent(network, per_agent_params, config, key, num_eval_steps=200):\n    \"\"\"Evaluate using PER-AGENT params (EVOLVE/FE mode).\n\n    per_agent_params: leaves shape (max_agents, ...) from a single env.\n    \"\"\"\n    env_state = reset(key, config)\n\n    # vmap network.apply with in_axes=(0, 0) for per-agent params\n    batched_apply = jax.vmap(network.apply, in_axes=(0, 0))\n\n    total_reward = 0.0\n    regular_food = 0.0\n    hf_collected = 0.0\n\n    for _ in range(num_eval_steps):\n        obs = get_observations(env_state, config)  # (max_agents, obs_dim)\n        out = batched_apply(per_agent_params, obs)  # per_agent: (A, ...), obs: (A, D)\n        logits = out[0]  # (max_agents, num_actions)\n        actions = jnp.argmax(logits, axis=-1)  # (max_agents,) deterministic\n\n        env_state, rewards, dones, info = step(env_state, actions, config)\n        # Mask rewards by alive agents to avoid dead-slot noise\n        alive_rewards = rewards * env_state.agent_alive.astype(jnp.float32)\n        total_reward += float(jnp.sum(alive_rewards))\n        regular_food += float(info['food_collected_this_step'])\n        hf_collected += float(info['hidden_food_collected_this_step'])\n\n    final_population = int(jnp.sum(env_state.agent_alive))\n    return {\n        'total_reward': total_reward,\n        'regular_food_collected': regular_food,\n        'hidden_food_collected': hf_collected,\n        'final_population': final_population,\n    }\n\n\ndef extract_eval_metrics(trainer, config):\n    \"\"\"Extract eval metrics from a trained ParallelTrainer.\n\n    Returns list of dicts, one per seed.\n    \"\"\"\n    ps = trainer._parallel_state\n    if ps is None:\n        return [{'error': 'no state'}] * trainer.num_seeds\n\n    network = ActorCritic(\n        hidden_dims=tuple(config.agent.hidden_dims),\n        num_actions=6,\n    )\n\n    results = []\n    for i in range(trainer.num_seeds):\n        try:\n            # Extract per-agent params from env 0 for this seed\n            seed_agent_params_env0 = jax.tree.map(\n                lambda x: x[i, 0], ps.env_state.agent_params\n            )  # leaves: (max_agents, ...)\n\n            alive_mask = np.array(ps.env_state.agent_alive[i, 0])  # (max_agents,)\n\n            # Weight divergence\n            div_result = compute_weight_divergence(seed_agent_params_env0, alive_mask)\n\n            # Eval\n            eval_key = jax.random.PRNGKey(42 + i)\n            if config.train.training_mode == TrainingMode.GRADIENT:\n                seed_params = jax.tree.map(lambda x: x[i], ps.params)\n                eval_result = run_hidden_food_eval_shared(\n                    network, seed_params, config, eval_key\n                )\n            else:\n                eval_result = run_hidden_food_eval_per_agent(\n                    network, seed_agent_params_env0, config, eval_key\n                )\n\n            results.append({\n                'mean_divergence': float(div_result['mean_divergence']),\n                'max_divergence': float(div_result['max_divergence']),\n                'alive_count': int(np.sum(alive_mask)),\n                **eval_result,\n            })\n        except Exception as e:\n            results.append({'error': str(e)})\n\n    return results\n\nprint(\"Eval functions defined.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SMOKE TEST: 1 config, 1 iteration - verify full pipeline\n",
    "# =============================================================================\n",
    "print(\"SMOKE TEST: Running G1 baseline for 1 iteration...\")\n",
    "print(\"This verifies: train -> eval -> metrics before the full sweep.\\n\")\n",
    "\n",
    "smoke_cfg = make_base_config()\n",
    "smoke_cfg.train.training_mode = TrainingMode.GRADIENT\n",
    "\n",
    "trainer = ParallelTrainer(\n",
    "    config=smoke_cfg, num_seeds=1, seed_ids=[99],\n",
    "    checkpoint_dir='/tmp/sweep_smoke', master_seed=0,\n",
    ")\n",
    "t0 = time.time()\n",
    "metrics = trainer.train(\n",
    "    num_iterations=1, checkpoint_interval_minutes=999,\n",
    "    resume=False, print_interval=1,\n",
    ")\n",
    "print(f\"\\nTraining done in {time.time() - t0:.1f}s\")\n",
    "\n",
    "# Test eval extraction\n",
    "eval_results = extract_eval_metrics(trainer, smoke_cfg)\n",
    "print(f\"Eval result: {eval_results[0]}\")\n",
    "\n",
    "# Verify\n",
    "assert eval_results[0].get('mean_divergence') is not None, \"Divergence missing!\"\n",
    "assert eval_results[0].get('hidden_food_collected') is not None, \"HF collected missing!\"\n",
    "assert eval_results[0].get('final_population', 0) > 0, \"Population crashed!\"\n",
    "\n",
    "del trainer\n",
    "gc.collect()\n",
    "jax.clear_caches()\n",
    "\n",
    "print(\"\\nSMOKE TEST PASSED - pipeline verified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# MAIN SWEEP LOOP\n# =============================================================================\n\n# Load existing results if resuming\nresults_path = os.path.join(SWEEP_DIR, 'sweep_results.pkl')\nif os.path.exists(results_path):\n    with open(results_path, 'rb') as f:\n        results = pickle.load(f)\n    print(f\"Loaded {len(results)} existing results\")\nelse:\n    results = {}\n\n\ndef save_results(results_dict):\n    \"\"\"Save results dict to Drive.\"\"\"\n    tmp_path = results_path + '.tmp'\n    with open(tmp_path, 'wb') as f:\n        pickle.dump(results_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n    os.replace(tmp_path, results_path)\n\n\nsweep_start = time.time()\ntotal_configs = len(all_configs)\ncompleted = sum(1 for r in results.values() if r.get('success'))\n\nprint(f\"Starting sweep: {total_configs} configs, {completed} already complete\")\nprint(\"=\" * 80)\n\nfor idx, sc in enumerate(all_configs):\n    # Skip if already done\n    if sc.name in results and results[sc.name].get('success'):\n        print(f\"[{idx+1}/{total_configs}] {sc.name} - SKIPPED (already complete)\")\n        continue\n\n    iters = ITERS_MAP[sc.config.train.training_mode]\n    mode = sc.config.train.training_mode.value\n    print(f\"\\n[{idx+1}/{total_configs}] {sc.name} ({sc.group}) | {mode} | {iters} iters\")\n\n    try:\n        t0 = time.time()\n        trainer = ParallelTrainer(\n            config=sc.config, num_seeds=NUM_SEEDS, seed_ids=SEED_IDS,\n            checkpoint_dir=f'/tmp/sweep_{sc.name}', master_seed=42 + idx,\n        )\n        train_metrics = trainer.train(\n            num_iterations=iters,\n            checkpoint_interval_minutes=999,\n            resume=False, print_interval=5,\n        )\n\n        # Extract eval metrics\n        eval_metrics = extract_eval_metrics(trainer, sc.config)\n        train_time = time.time() - t0\n\n        # Aggregate across seeds\n        valid_evals = [e for e in eval_metrics if 'error' not in e]\n        if valid_evals:\n            agg = {}\n            for key in valid_evals[0]:\n                vals = [e[key] for e in valid_evals]\n                agg[f\"{key}_mean\"] = float(np.mean(vals))\n                agg[f\"{key}_std\"] = float(np.std(vals))\n        else:\n            agg = {}\n\n        results[sc.name] = {\n            'success': True,\n            'group': sc.group,\n            'mode': mode,\n            'iters': iters,\n            'train_time': train_time,\n            'train_metrics': train_metrics,\n            'eval_per_seed': eval_metrics,\n            'eval_agg': agg,\n        }\n\n        hf_val = agg.get('hidden_food_collected_mean', 0.0)\n        div_val = agg.get('mean_divergence_mean', 0.0)\n        pop_val = agg.get('final_population_mean', 0.0)\n        print(f\"  Done in {train_time:.1f}s | \"\n              f\"HF: {hf_val:.1f} | Div: {div_val:.4f} | Pop: {pop_val:.0f}\")\n\n    except Exception as e:\n        print(f\"  FAILED: {e}\")\n        results[sc.name] = {\n            'success': False,\n            'group': sc.group,\n            'mode': mode,\n            'error': traceback.format_exc(),\n        }\n\n    finally:\n        try:\n            del trainer\n        except NameError:\n            pass\n        gc.collect()\n        jax.clear_caches()\n\n    # Save after every config\n    save_results(results)\n\n    # Progress\n    completed = sum(1 for r in results.values() if r.get('success'))\n    elapsed = time.time() - sweep_start\n    if completed > 0:\n        avg_time = elapsed / completed\n        remaining = avg_time * (total_configs - completed)\n        eta = datetime.now() + timedelta(seconds=remaining)\n        print(f\"  Progress: {completed}/{total_configs} | \"\n              f\"Elapsed: {elapsed/60:.0f}min | ETA: {eta.strftime('%H:%M')}\")\n\ntotal_elapsed = time.time() - sweep_start\nprint(\"\\n\" + \"=\" * 80)\nprint(f\"SWEEP COMPLETE! {completed}/{total_configs} successful in {total_elapsed/60:.0f} min\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LEADERBOARD\n",
    "# =============================================================================\n",
    "\n",
    "# Reload results (in case of cell re-run)\n",
    "if os.path.exists(results_path):\n",
    "    with open(results_path, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "\n",
    "# Build leaderboard rows\n",
    "rows = []\n",
    "for name, r in results.items():\n",
    "    if not r.get('success'):\n",
    "        rows.append({\n",
    "            'name': name, 'group': r.get('group', '?'),\n",
    "            'hf_collected': 0.0, 'divergence': 0.0,\n",
    "            'pop_stability': 0.0, 'failed': True,\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    agg = r.get('eval_agg', {})\n",
    "    hf = agg.get('hidden_food_collected_mean', 0.0)\n",
    "    div = agg.get('mean_divergence_mean', 0.0)\n",
    "    pop = agg.get('final_population_mean', 0.0)\n",
    "    pop_stability = pop / MAX_AGENTS  # 0-1 scale\n",
    "\n",
    "    rows.append({\n",
    "        'name': name, 'group': r.get('group', '?'),\n",
    "        'hf_collected': hf, 'divergence': div,\n",
    "        'pop_stability': pop_stability, 'final_pop': pop,\n",
    "        'failed': False, 'crashed': pop == 0,\n",
    "    })\n",
    "\n",
    "# Min-max normalization across successful, non-crashed configs\n",
    "valid = [r for r in rows if not r['failed'] and not r.get('crashed', False)]\n",
    "\n",
    "if valid:\n",
    "    hf_vals = [r['hf_collected'] for r in valid]\n",
    "    div_vals = [r['divergence'] for r in valid]\n",
    "\n",
    "    hf_min, hf_max = min(hf_vals), max(hf_vals)\n",
    "    div_min, div_max = min(div_vals), max(div_vals)\n",
    "\n",
    "    def norm(v, vmin, vmax):\n",
    "        return (v - vmin) / (vmax - vmin) if vmax > vmin else 0.5\n",
    "\n",
    "    for r in rows:\n",
    "        if r['failed'] or r.get('crashed', False):\n",
    "            r['score'] = 0.0\n",
    "        else:\n",
    "            hf_norm = norm(r['hf_collected'], hf_min, hf_max)\n",
    "            div_norm = norm(r['divergence'], div_min, div_max)\n",
    "            r['score'] = 0.5 * hf_norm + 0.25 * div_norm + 0.25 * r['pop_stability']\n",
    "\n",
    "    # Sort by score descending\n",
    "    rows.sort(key=lambda r: r['score'], reverse=True)\n",
    "\n",
    "    # Print top 20\n",
    "    print(f\"{'Rank':<5} {'Name':<30} {'Group':<20} {'HF_Col':>8} {'Diverg':>8} {'Pop':>6} {'Score':>7}\")\n",
    "    print(\"-\" * 90)\n",
    "    for i, r in enumerate(rows[:20]):\n",
    "        status = \"\"\n",
    "        if r['failed']:\n",
    "            status = \" FAILED\"\n",
    "        elif r.get('crashed', False):\n",
    "            status = \" CRASHED\"\n",
    "\n",
    "        print(f\"{i+1:<5} {r['name']:<30} {r['group']:<20} \"\n",
    "              f\"{r['hf_collected']:>8.1f} {r['divergence']:>8.4f} \"\n",
    "              f\"{r.get('final_pop', 0):>6.0f} {r['score']:>7.3f}{status}\")\n",
    "\n",
    "    n_failed = sum(1 for r in rows if r['failed'])\n",
    "    n_crashed = sum(1 for r in rows if r.get('crashed', False) and not r['failed'])\n",
    "    print(f\"\\nTotal: {len(rows)} configs | {len(valid)} valid | {n_failed} failed | {n_crashed} crashed\")\n",
    "else:\n",
    "    print(\"No valid results to rank.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# ANALYSIS PLOTS (4 figures)\n# =============================================================================\n\n# Only plot valid configs\nplot_rows = [r for r in rows if not r['failed'] and not r.get('crashed', False)]\n\nif not plot_rows:\n    print(\"No valid data to plot.\")\nelse:\n    # Group colors\n    unique_groups = sorted(set(r['group'] for r in plot_rows))\n    try:\n        cmap = plt.colormaps['tab10'].resampled(max(len(unique_groups), 1))\n    except AttributeError:\n        cmap = plt.cm.get_cmap('tab10', max(len(unique_groups), 1))\n    group_colors = {g: cmap(i) for i, g in enumerate(unique_groups)}\n\n    # --- Figure 1: Scatter - Divergence vs HF Collected ---\n    fig1, ax1 = plt.subplots(figsize=(10, 7))\n    for g in unique_groups:\n        g_rows = [r for r in plot_rows if r['group'] == g]\n        ax1.scatter(\n            [r['divergence'] for r in g_rows],\n            [r['hf_collected'] for r in g_rows],\n            c=[group_colors[g]], label=g, s=80, alpha=0.8, edgecolors='black', linewidths=0.5,\n        )\n    # Annotate top 5\n    top5 = sorted(plot_rows, key=lambda r: r['score'], reverse=True)[:5]\n    for r in top5:\n        ax1.annotate(r['name'], (r['divergence'], r['hf_collected']),\n                     fontsize=7, ha='left', va='bottom',\n                     xytext=(5, 5), textcoords='offset points')\n    ax1.set_xlabel('Weight Divergence (mean cosine distance)')\n    ax1.set_ylabel('Hidden Food Collected')\n    ax1.set_title('Weight Divergence vs Hidden Food Collected')\n    ax1.legend(fontsize=8, bbox_to_anchor=(1.02, 1), loc='upper left')\n    fig1.tight_layout()\n    fig1.savefig(os.path.join(SWEEP_DIR, 'scatter_div_vs_hf.png'), dpi=150)\n    plt.show()\n\n    # --- Figure 2: Top 10 bars (3 panels) ---\n    top10 = sorted(plot_rows, key=lambda r: r['score'], reverse=True)[:10]\n    names = [r['name'] for r in top10]\n\n    fig2, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n    axes[0].barh(names[::-1], [r['hf_collected'] for r in top10][::-1], color='steelblue')\n    axes[0].set_title('Hidden Food Collected')\n    axes[0].set_xlabel('Count')\n\n    axes[1].barh(names[::-1], [r['divergence'] for r in top10][::-1], color='coral')\n    axes[1].set_title('Weight Divergence')\n    axes[1].set_xlabel('Mean cosine distance')\n\n    axes[2].barh(names[::-1], [r['score'] for r in top10][::-1], color='forestgreen')\n    axes[2].set_title('Composite Score')\n    axes[2].set_xlabel('Score')\n\n    fig2.suptitle('Top 10 Configurations', fontsize=14)\n    fig2.tight_layout()\n    fig2.savefig(os.path.join(SWEEP_DIR, 'top10_bars.png'), dpi=150)\n    plt.show()\n\n    # --- Figure 3: Heatmap - Group B grid (div_bonus x niche_pressure) ---\n    b_rows = [r for r in plot_rows if r['group'] == 'B_div_x_niche']\n    if b_rows:\n        db_vals = sorted(set(float(r['name'].split('_db')[1].split('_')[0]) for r in b_rows))\n        np_vals = sorted(set(float(r['name'].split('_np')[1]) for r in b_rows))\n\n        grid = np.zeros((len(np_vals), len(db_vals)))\n        for r in b_rows:\n            db = float(r['name'].split('_db')[1].split('_')[0])\n            np_v = float(r['name'].split('_np')[1])\n            i = np_vals.index(np_v)\n            j = db_vals.index(db)\n            grid[i, j] = r['hf_collected']\n\n        fig3, ax3 = plt.subplots(figsize=(8, 5))\n        im = ax3.imshow(grid, aspect='auto', cmap='YlOrRd')\n        ax3.set_xticks(range(len(db_vals)))\n        ax3.set_xticklabels([str(v) for v in db_vals])\n        ax3.set_yticks(range(len(np_vals)))\n        ax3.set_yticklabels([str(v) for v in np_vals])\n        ax3.set_xlabel('diversity_bonus')\n        ax3.set_ylabel('niche_pressure')\n        ax3.set_title('Group B: Hidden Food Collected (div x niche grid)')\n        for i in range(len(np_vals)):\n            for j in range(len(db_vals)):\n                ax3.text(j, i, f\"{grid[i, j]:.0f}\", ha='center', va='center',\n                         fontsize=12, fontweight='bold')\n        fig3.colorbar(im, ax=ax3)\n        fig3.tight_layout()\n        fig3.savefig(os.path.join(SWEEP_DIR, 'heatmap_div_x_niche.png'), dpi=150)\n        plt.show()\n    else:\n        print(\"No Group B data for heatmap.\")\n\n    # --- Figure 4: Box plots per group ---\n    fig4, ax4 = plt.subplots(figsize=(12, 6))\n    group_data = {}\n    for r in plot_rows:\n        group_data.setdefault(r['group'], []).append(r['hf_collected'])\n\n    groups_sorted = sorted(group_data.keys())\n    data_for_box = [group_data[g] for g in groups_sorted]\n\n    bp = ax4.boxplot(data_for_box, labels=groups_sorted, patch_artist=True)\n    for patch, g in zip(bp['boxes'], groups_sorted):\n        patch.set_facecolor(group_colors.get(g, 'lightblue'))\n\n    ax4.set_ylabel('Hidden Food Collected')\n    ax4.set_title('Hidden Food Collected by Group')\n    plt.xticks(rotation=45, ha='right')\n    fig4.tight_layout()\n    fig4.savefig(os.path.join(SWEEP_DIR, 'boxplot_per_group.png'), dpi=150)\n    plt.show()\n\n    print(f\"\\nAll figures saved to {SWEEP_DIR}/\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE RESULTS + RECOMMENDATIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Save final results pickle (already saved incrementally, but save final version)\n",
    "save_results(results)\n",
    "print(f\"Results saved to {results_path}\")\n",
    "\n",
    "# Save JSON summary (human-readable)\n",
    "summary = {}\n",
    "for name, r in results.items():\n",
    "    if r.get('success'):\n",
    "        summary[name] = {\n",
    "            'group': r['group'],\n",
    "            'mode': r['mode'],\n",
    "            'train_time_s': r.get('train_time', 0),\n",
    "            **r.get('eval_agg', {}),\n",
    "        }\n",
    "    else:\n",
    "        summary[name] = {\n",
    "            'group': r.get('group', '?'),\n",
    "            'error': r.get('error', 'unknown')[:200],\n",
    "        }\n",
    "\n",
    "json_path = os.path.join(SWEEP_DIR, 'sweep_summary.json')\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"JSON summary saved to {json_path}\")\n",
    "\n",
    "# Print top 5 with full config values\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOP 5 CONFIGURATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "top5_names = [r['name'] for r in rows if not r['failed']][:5]\n",
    "for rank, name in enumerate(top5_names, 1):\n",
    "    r = results[name]\n",
    "    agg = r.get('eval_agg', {})\n",
    "\n",
    "    # Find the matching SweepConfig\n",
    "    sc = next((s for s in all_configs if s.name == name), None)\n",
    "    if sc is None:\n",
    "        continue\n",
    "\n",
    "    cfg = sc.config\n",
    "    print(f\"\\n--- #{rank}: {name} (score: {rows[rank-1]['score']:.3f}) ---\")\n",
    "    print(f\"  Group: {sc.group}\")\n",
    "    print(f\"  Mode: {cfg.train.training_mode.value}\")\n",
    "    print(f\"  HF collected: {agg.get('hidden_food_collected_mean', 0):.1f} +/- {agg.get('hidden_food_collected_std', 0):.1f}\")\n",
    "    print(f\"  Divergence: {agg.get('mean_divergence_mean', 0):.4f}\")\n",
    "    print(f\"  Population: {agg.get('final_population_mean', 0):.0f}\")\n",
    "    print(f\"  Config params:\")\n",
    "    print(f\"    diversity_bonus = {cfg.specialization.diversity_bonus}\")\n",
    "    print(f\"    niche_pressure = {cfg.specialization.niche_pressure}\")\n",
    "    print(f\"    mutation_std = {cfg.evolution.mutation_std}\")\n",
    "    if cfg.train.training_mode in (TrainingMode.FREEZE_EVOLVE, TrainingMode.EVOLVE):\n",
    "        print(f\"    gradient_steps = {cfg.freeze_evolve.gradient_steps}\")\n",
    "        print(f\"    evolve_steps = {cfg.freeze_evolve.evolve_steps}\")\n",
    "        print(f\"    evolve_mutation_boost = {cfg.freeze_evolve.evolve_mutation_boost}\")\n",
    "    print(f\"    field: diffusion={cfg.field.diffusion_rate}, decay={cfg.field.decay_rate}, write={cfg.field.write_strength}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Copy-paste ready config for specialization training notebook:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if top5_names:\n",
    "    best = next(s for s in all_configs if s.name == top5_names[0])\n",
    "    bc = best.config\n",
    "    print(f\"\"\"\\n# Best config from sweep: {best.name}\n",
    "cfg.train.training_mode = TrainingMode.{bc.train.training_mode.name}\n",
    "cfg.specialization.diversity_bonus = {bc.specialization.diversity_bonus}\n",
    "cfg.specialization.niche_pressure = {bc.specialization.niche_pressure}\n",
    "cfg.evolution.mutation_std = {bc.evolution.mutation_std}\"\"\")\n",
    "    if bc.train.training_mode in (TrainingMode.FREEZE_EVOLVE, TrainingMode.EVOLVE):\n",
    "        print(f\"cfg.freeze_evolve.gradient_steps = {bc.freeze_evolve.gradient_steps}\")\n",
    "        print(f\"cfg.freeze_evolve.evolve_steps = {bc.freeze_evolve.evolve_steps}\")\n",
    "        print(f\"cfg.freeze_evolve.evolve_mutation_boost = {bc.freeze_evolve.evolve_mutation_boost}\")\n",
    "\n",
    "print(f\"\\nAll results saved to {SWEEP_DIR}/\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}