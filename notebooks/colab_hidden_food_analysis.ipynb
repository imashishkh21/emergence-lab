{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Food Coordination Analysis — Field ON vs Field OFF\n",
    "\n",
    "**Hypothesis**: Field ON agents can use the shared field to signal hidden food locations,\n",
    "enabling coordination. Field OFF agents have NO signaling mechanism.\n",
    "\n",
    "## Key Metrics\n",
    "- Hidden food revealed: how many times K agents cluster to reveal hidden food\n",
    "- Hidden food collected: how many hidden food items are actually eaten\n",
    "- Regular food collected: baseline foraging performance\n",
    "- Reward breakdown: regular food energy vs hidden food energy\n",
    "\n",
    "## Setup\n",
    "1. Runtime > Change runtime type > **TPU v6e** + **High-RAM**\n",
    "2. Run all cells (Ctrl+F9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "REPO_DIR = '/content/emergence-lab'\n",
    "GITHUB_USERNAME = \"imashishkh21\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone https://github.com/{GITHUB_USERNAME}/emergence-lab.git {REPO_DIR}\n",
    "else:\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "!pip install -e \".[dev,phase5]\" -q\n",
    "\n",
    "import jax\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Devices: {jax.devices()}\")\n",
    "print(f\"Device count: {jax.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport json, pickle, gc, os, copy\nimport glob as glob_mod\nfrom pathlib import Path\nfrom collections import defaultdict\nfrom datetime import datetime\n\nimport jax\nimport jax.numpy as jnp\nfrom scipy import stats as scipy_stats\n\nfrom src.configs import Config\nfrom src.agents.network import ActorCritic\nfrom src.agents.policy import get_deterministic_actions\nfrom src.environment.env import reset, step\nfrom src.environment.obs import get_observations\nfrom src.analysis.specialization import compute_weight_divergence\nfrom src.analysis.ablation import _run_episode_full\nfrom src.training.checkpointing import load_checkpoint\nfrom src.analysis.statistics import (\n    compute_iqm, compare_methods, welch_t_test, mann_whitney_test,\n    probability_of_improvement,\n)\nfrom src.analysis.paper_figures import (\n    setup_publication_style, plot_performance_profiles, save_figure,\n)\n\n# Override Agg backend from paper_figures import\n%matplotlib inline\n\nFIELD_ON_DIR = '/content/drive/MyDrive/emergence-lab/hidden_food_field_on/'\nFIELD_OFF_DIR = '/content/drive/MyDrive/emergence-lab/hidden_food_field_off/'\nOUTPUT_DIR = '/content/drive/MyDrive/emergence-lab/hidden_food_analysis_results/'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(\"Imports loaded.\")\nprint(f\"Field ON dir:  {FIELD_ON_DIR} (exists: {os.path.exists(FIELD_ON_DIR)})\")\nprint(f\"Field OFF dir: {FIELD_OFF_DIR} (exists: {os.path.exists(FIELD_OFF_DIR)})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEST STEP - Verify everything works before running full analysis\n",
    "# =============================================================================\n",
    "import time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TEST MODE: Running quick verification...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "errors = []\n",
    "\n",
    "# Test 1: Stats functions work\n",
    "print(\"\\n[1/7] Testing statistics functions...\")\n",
    "try:\n",
    "    test_a = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "    test_b = np.array([2.0, 3.0, 4.0, 5.0, 6.0])\n",
    "    iqm_test = compute_iqm(test_a, n_bootstrap=100, seed=0)\n",
    "    welch_test = welch_t_test(test_a, test_b)\n",
    "    mw_test = mann_whitney_test(test_a, test_b)\n",
    "    poi_test = probability_of_improvement(test_a, test_b, n_bootstrap=100, seed=0)\n",
    "    comp_test = compare_methods({\"A\": test_a, \"B\": test_b}, n_bootstrap=100, seed=0)\n",
    "    assert hasattr(iqm_test, 'iqm'), \"IQM result missing .iqm\"\n",
    "    assert hasattr(welch_test, 'p_value'), \"Welch result missing .p_value\"\n",
    "    assert 'prob_x_better' in poi_test, \"POI missing prob_x_better\"\n",
    "    assert hasattr(comp_test, 'summary'), \"Compare result missing .summary\"\n",
    "    print(\"   PASS: All statistics functions work correctly\")\n",
    "except Exception as e:\n",
    "    errors.append(f\"Stats functions: {e}\")\n",
    "    print(f\"   FAIL: {e}\")\n",
    "\n",
    "# Test 2: Plot functions work\n",
    "print(\"\\n[2/7] Testing plot functions...\")\n",
    "try:\n",
    "    setup_publication_style()\n",
    "    fig_test, ax_test = plt.subplots()\n",
    "    ax_test.plot([1, 2, 3])\n",
    "    plt.close(fig_test)\n",
    "    print(\"   PASS: Matplotlib + publication style working\")\n",
    "except Exception as e:\n",
    "    errors.append(f\"Plot functions: {e}\")\n",
    "    print(f\"   FAIL: {e}\")\n",
    "\n",
    "# Test 3: Field ON Drive directory\n",
    "print(\"\\n[3/7] Testing Field ON Drive access...\")\n",
    "try:\n",
    "    assert os.path.exists(FIELD_ON_DIR), f\"Directory not found: {FIELD_ON_DIR}\"\n",
    "    on_batches = [d for d in os.listdir(FIELD_ON_DIR) if d.startswith('batch_')]\n",
    "    assert len(on_batches) > 0, \"No batch directories found for Field ON\"\n",
    "    on_ckpts = 0\n",
    "    for bd in on_batches:\n",
    "        bp = os.path.join(FIELD_ON_DIR, bd)\n",
    "        for sd in os.listdir(bp):\n",
    "            sp = os.path.join(bp, sd)\n",
    "            if os.path.isdir(sp):\n",
    "                pkls = glob_mod.glob(os.path.join(sp, 'step_*.pkl'))\n",
    "                on_ckpts += len(pkls) > 0\n",
    "    print(f\"   PASS: Field ON -- {len(on_batches)} batches, {on_ckpts} seed checkpoints\")\n",
    "except Exception as e:\n",
    "    errors.append(f\"Field ON Drive: {e}\")\n",
    "    print(f\"   FAIL: {e}\")\n",
    "\n",
    "# Test 4: Field OFF Drive directory\n",
    "print(\"\\n[4/7] Testing Field OFF Drive access...\")\n",
    "try:\n",
    "    assert os.path.exists(FIELD_OFF_DIR), f\"Directory not found: {FIELD_OFF_DIR}\"\n",
    "    off_batches = [d for d in os.listdir(FIELD_OFF_DIR) if d.startswith('batch_')]\n",
    "    assert len(off_batches) > 0, \"No batch directories found for Field OFF\"\n",
    "    off_ckpts = 0\n",
    "    for bd in off_batches:\n",
    "        bp = os.path.join(FIELD_OFF_DIR, bd)\n",
    "        for sd in os.listdir(bp):\n",
    "            sp = os.path.join(bp, sd)\n",
    "            if os.path.isdir(sp):\n",
    "                pkls = glob_mod.glob(os.path.join(sp, 'step_*.pkl'))\n",
    "                off_ckpts += len(pkls) > 0\n",
    "    print(f\"   PASS: Field OFF -- {len(off_batches)} batches, {off_ckpts} seed checkpoints\")\n",
    "except Exception as e:\n",
    "    errors.append(f\"Field OFF Drive: {e}\")\n",
    "    print(f\"   FAIL: {e}\")\n",
    "\n",
    "# Test 5: Checkpoint loading - verify hidden food enabled\n",
    "print(\"\\n[5/7] Testing checkpoint loading (one per condition)...\")\n",
    "try:\n",
    "    for cond_name, cond_dir in [(\"Field ON\", FIELD_ON_DIR), (\"Field OFF\", FIELD_OFF_DIR)]:\n",
    "        test_ckpt = None\n",
    "        batch_dirs = sorted([d for d in os.listdir(cond_dir) if d.startswith('batch_')])\n",
    "        for bd in batch_dirs:\n",
    "            bp = os.path.join(cond_dir, bd)\n",
    "            for sd in sorted(os.listdir(bp)):\n",
    "                sp = os.path.join(bp, sd)\n",
    "                if os.path.isdir(sp):\n",
    "                    pkls = glob_mod.glob(os.path.join(sp, 'step_*.pkl'))\n",
    "                    if pkls:\n",
    "                        test_ckpt = sorted(pkls)[-1]\n",
    "                        break\n",
    "            if test_ckpt:\n",
    "                break\n",
    "        assert test_ckpt is not None, f\"No checkpoint found for {cond_name}\"\n",
    "        ckpt = load_checkpoint(test_ckpt)\n",
    "        config = ckpt['config']\n",
    "        assert hasattr(config, 'env'), f\"{cond_name} config not a dataclass\"\n",
    "        assert config.env.hidden_food.enabled == True, f\"{cond_name}: hidden_food not enabled!\"\n",
    "        print(f\"   {cond_name}: seed={ckpt.get('seed_id', -1)}, grid={config.env.grid_size}, \"\n",
    "              f\"hidden_food={config.env.hidden_food.enabled}, \"\n",
    "              f\"decay={config.field.decay_rate}, diffusion={config.field.diffusion_rate}\")\n",
    "    print(\"   PASS: Both conditions' checkpoints load correctly with hidden food enabled\")\n",
    "except Exception as e:\n",
    "    errors.append(f\"Checkpoint loading: {e}\")\n",
    "    print(f\"   FAIL: {e}\")\n",
    "\n",
    "# Test 6: Standard eval episode\n",
    "print(\"\\n[6/7] Testing standard eval episode...\")\n",
    "try:\n",
    "    agent_params = jax.tree_util.tree_map(lambda x: x[0], ckpt['agent_params'])\n",
    "    network = ActorCritic(hidden_dims=tuple(config.agent.hidden_dims), num_actions=6)\n",
    "    t0 = time.time()\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    stats = _run_episode_full(\n",
    "        network=network, params=ckpt['params'], config=config,\n",
    "        key=key, condition=\"normal\", evolution=True,\n",
    "    )\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"   Eval: reward={stats.total_reward:.1f}, pop={stats.final_population} ({elapsed:.1f}s)\")\n",
    "    print(\"   PASS: Standard eval episode works\")\n",
    "except Exception as e:\n",
    "    errors.append(f\"Standard eval: {e}\")\n",
    "    print(f\"   FAIL: {e}\")\n",
    "\n",
    "# Test 7: Hidden food custom eval\n",
    "print(\"\\n[7/7] Testing hidden food custom eval...\")\n",
    "try:\n",
    "    key = jax.random.PRNGKey(99)\n",
    "    state = reset(key, config)\n",
    "    obs = get_observations(state, config)\n",
    "    obs_batched = obs[None, :, :]  # (1, max_agents, obs_dim)\n",
    "    actions = get_deterministic_actions(network, ckpt['params'], obs_batched)\n",
    "    actions = actions[0]  # (max_agents,)\n",
    "    pre_revealed = state.hidden_food_revealed\n",
    "    state2, rewards, done, info = step(state, actions, config)\n",
    "    assert 'hidden_food_collected_this_step' in info\n",
    "    assert 'food_collected_this_step' in info\n",
    "    newly_revealed = (~pre_revealed) & state2.hidden_food_revealed\n",
    "    print(f\"   Custom eval step: regular_food={info['food_collected_this_step']}, \"\n",
    "          f\"hf_collected={info['hidden_food_collected_this_step']}, \"\n",
    "          f\"newly_revealed={int(jnp.sum(newly_revealed))}\")\n",
    "    print(\"   PASS: Hidden food custom eval mechanics work\")\n",
    "except Exception as e:\n",
    "    errors.append(f\"Hidden food eval: {e}\")\n",
    "    print(f\"   FAIL: {e}\")\n",
    "\n",
    "# Summary\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "if errors:\n",
    "    print(f\"TEST FAILED! {len(errors)} error(s):\")\n",
    "    for err in errors:\n",
    "        print(f\"  - {err}\")\n",
    "    print(\"\\nDO NOT proceed until all tests pass.\")\n",
    "    print(\"=\"*70)\n",
    "    raise RuntimeError(f\"Test failed with {len(errors)} error(s)\")\n",
    "else:\n",
    "    print(\"ALL 7 TESTS PASSED!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Proceed to run the full analysis below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Eval: Hidden Food Metrics\n",
    "\n",
    "This function tracks per-step hidden food statistics that aren't captured\n",
    "by the standard `_run_episode_full()`.\n",
    "\n",
    "**Warning**: ~5-10 minutes per seed on TPU (500 Python-level JAX calls per episode,\n",
    "not JIT-compiled). With 60 seeds at 1 episode each = ~5-10 hours total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hidden_food_eval(network, params, config, key, num_episodes=1):\n",
    "    \"\"\"Run eval episodes tracking hidden food reveal and collection.\n",
    "\n",
    "    Args:\n",
    "        network: ActorCritic module.\n",
    "        params: Network parameters.\n",
    "        config: Config with hidden_food enabled.\n",
    "        key: PRNG key.\n",
    "        num_episodes: Episodes to run. Default 1 (set to 5 for more precision,\n",
    "            but each episode takes ~5-10 min on TPU).\n",
    "\n",
    "    Returns:\n",
    "        dict with aggregated metrics and per_episode list.\n",
    "\n",
    "    Note on reveal counting:\n",
    "        Reveals are counted by diffing state.hidden_food_revealed before/after\n",
    "        each step. If food is revealed AND collected in the same step, the diff\n",
    "        misses it (revealed resets to False). So:\n",
    "        - hidden_food_collected is EXACT (from info dict)\n",
    "        - hidden_food_revealed is a LOWER BOUND\n",
    "        - total_reveal_events = revealed + collected (upper bound,\n",
    "          since some collected food may have been revealed in a prior step)\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        key, ep_key = jax.random.split(key)\n",
    "        state = reset(ep_key, config)\n",
    "\n",
    "        ep_reward = 0.0\n",
    "        ep_regular_food = 0.0\n",
    "        ep_hf_revealed = 0\n",
    "        ep_hf_collected = 0.0\n",
    "        ep_births = 0\n",
    "        ep_deaths = 0\n",
    "\n",
    "        for t in range(config.env.max_steps):\n",
    "            obs = get_observations(state, config)\n",
    "            obs_batched = obs[None, :, :]  # (1, max_agents, obs_dim)\n",
    "            actions = get_deterministic_actions(network, params, obs_batched)\n",
    "            actions = actions[0]  # (max_agents,)\n",
    "\n",
    "            # Track pre-step hidden food revealed state\n",
    "            pre_revealed = state.hidden_food_revealed\n",
    "\n",
    "            state, rewards, done, info = step(state, actions, config)\n",
    "\n",
    "            ep_reward += float(jnp.sum(rewards))\n",
    "            ep_regular_food += float(info['food_collected_this_step'])\n",
    "            ep_hf_collected += float(info['hidden_food_collected_this_step'])\n",
    "            ep_births += int(info['births_this_step'])\n",
    "            ep_deaths += int(info['deaths_this_step'])\n",
    "\n",
    "            # Count newly revealed hidden food (lower bound - see docstring)\n",
    "            if pre_revealed is not None and state.hidden_food_revealed is not None:\n",
    "                newly_revealed = (~pre_revealed) & state.hidden_food_revealed\n",
    "                ep_hf_revealed += int(jnp.sum(newly_revealed))\n",
    "\n",
    "            if bool(done):\n",
    "                break\n",
    "\n",
    "        final_pop = int(jnp.sum(state.agent_alive.astype(jnp.int32)))\n",
    "\n",
    "        # Energy breakdown\n",
    "        food_energy = config.evolution.food_energy\n",
    "        hf_multiplier = config.env.hidden_food.hidden_food_value_multiplier\n",
    "        regular_energy = ep_regular_food * food_energy\n",
    "        hidden_energy = ep_hf_collected * food_energy * hf_multiplier\n",
    "\n",
    "        all_results.append({\n",
    "            'total_reward': ep_reward,\n",
    "            'regular_food_collected': ep_regular_food,\n",
    "            'hidden_food_revealed': ep_hf_revealed,\n",
    "            'hidden_food_collected': ep_hf_collected,\n",
    "            'regular_food_energy': regular_energy,\n",
    "            'hidden_food_energy': hidden_energy,\n",
    "            'final_population': final_pop,\n",
    "            'total_births': ep_births,\n",
    "            'total_deaths': ep_deaths,\n",
    "        })\n",
    "\n",
    "    # Aggregate\n",
    "    agg = {k: np.mean([r[k] for r in all_results]) for k in all_results[0]}\n",
    "    agg['per_episode'] = all_results\n",
    "    return agg\n",
    "\n",
    "\n",
    "# Quick smoke test\n",
    "print(\"Hidden food eval function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Load Training Data\n",
    "\n",
    "Discover checkpoints and load training summaries from Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== DISCOVER CHECKPOINTS + LOAD TRAINING SUMMARIES ==========\n",
    "\n",
    "def discover_checkpoints(drive_dir, condition_name):\n",
    "    \"\"\"Discover all checkpoint paths in a Drive directory.\"\"\"\n",
    "    paths = []\n",
    "    for batch_idx in range(10):\n",
    "        batch_dir = os.path.join(drive_dir, f'batch_{batch_idx}')\n",
    "        if not os.path.exists(batch_dir):\n",
    "            continue\n",
    "        for seed_dir in sorted(os.listdir(batch_dir)):\n",
    "            seed_path = os.path.join(batch_dir, seed_dir)\n",
    "            if not os.path.isdir(seed_path):\n",
    "                continue\n",
    "            pkl_files = glob_mod.glob(os.path.join(seed_path, 'step_*.pkl'))\n",
    "            if pkl_files:\n",
    "                paths.append(sorted(pkl_files)[-1])\n",
    "    print(f\"  {condition_name}: Found {len(paths)} checkpoints\")\n",
    "    return paths\n",
    "\n",
    "\n",
    "def load_training_summary(drive_dir, condition_name):\n",
    "    \"\"\"Load training_summary.pkl and extract per-seed rewards + populations.\"\"\"\n",
    "    summary_path = os.path.join(drive_dir, 'training_summary.pkl')\n",
    "    if not os.path.exists(summary_path):\n",
    "        print(f\"  {condition_name}: No training_summary.pkl found\")\n",
    "        return None, None\n",
    "    with open(summary_path, 'rb') as f:\n",
    "        summary = pickle.load(f)\n",
    "    rewards = []\n",
    "    populations = []\n",
    "    skipped = 0\n",
    "    for batch in summary['all_results']:\n",
    "        if not batch.get('success', True):\n",
    "            skipped += 1\n",
    "            continue\n",
    "        if 'metrics' in batch and 'mean_reward' in batch['metrics']:\n",
    "            rewards.extend(batch['metrics']['mean_reward'])\n",
    "        if 'metrics' in batch and 'population_size' in batch['metrics']:\n",
    "            populations.extend(batch['metrics']['population_size'])\n",
    "    if skipped > 0:\n",
    "        print(f\"  {condition_name}: WARNING - skipped {skipped} failed batches\")\n",
    "    if rewards:\n",
    "        rewards = np.array(rewards)\n",
    "        populations = np.array(populations, dtype=int) if populations else None\n",
    "        print(f\"  {condition_name}: Loaded {len(rewards)} rewards from training_summary.pkl\")\n",
    "        return rewards, populations\n",
    "    return None, None\n",
    "\n",
    "\n",
    "print(\"Discovering data on Drive...\")\n",
    "field_on_ckpt_paths = discover_checkpoints(FIELD_ON_DIR, \"Field ON\")\n",
    "field_off_ckpt_paths = discover_checkpoints(FIELD_OFF_DIR, \"Field OFF\")\n",
    "\n",
    "# Load training summaries\n",
    "on_rewards_drive, on_pops_drive = load_training_summary(FIELD_ON_DIR, \"Field ON\")\n",
    "off_rewards_drive, off_pops_drive = load_training_summary(FIELD_OFF_DIR, \"Field OFF\")\n",
    "\n",
    "field_on_rewards = on_rewards_drive\n",
    "field_on_populations = on_pops_drive\n",
    "field_off_rewards = off_rewards_drive\n",
    "field_off_populations = off_pops_drive\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING DATA SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Checkpoints: {len(field_on_ckpt_paths)} Field ON, {len(field_off_ckpt_paths)} Field OFF\")\n",
    "if field_on_rewards is not None:\n",
    "    print(f\"Field ON:  {len(field_on_rewards)} seeds, mean={field_on_rewards.mean():.3f} +/- {field_on_rewards.std(ddof=1):.3f}\")\n",
    "else:\n",
    "    print(\"Field ON:  No training_summary.pkl (will use eval data only)\")\n",
    "if field_off_rewards is not None:\n",
    "    print(f\"Field OFF: {len(field_off_rewards)} seeds, mean={field_off_rewards.mean():.3f} +/- {field_off_rewards.std(ddof=1):.3f}\")\n",
    "else:\n",
    "    print(\"Field OFF: No training_summary.pkl (will use eval data only)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== DESCRIPTIVE STATISTICS (Training Reward) ==========\n",
    "print(\"=\"*60)\n",
    "print(\"DESCRIPTIVE STATISTICS (Training Reward)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, rewards in [(\"Field ON\", field_on_rewards), (\"Field OFF\", field_off_rewards)]:\n",
    "    if rewards is None:\n",
    "        print(f\"\\n{name}: No training data available\")\n",
    "        continue\n",
    "    iqm = compute_iqm(rewards, n_bootstrap=10000, seed=42)\n",
    "    print(f\"\\n{name} (n={len(rewards)}):\")\n",
    "    print(f\"  Mean:   {rewards.mean():.4f} +/- {rewards.std(ddof=1):.4f}\")\n",
    "    print(f\"  Median: {np.median(rewards):.4f}\")\n",
    "    print(f\"  IQM:    {iqm.iqm:.4f} [{iqm.ci_lower:.4f}, {iqm.ci_upper:.4f}]\")\n",
    "    print(f\"  Min:    {rewards.min():.4f}\")\n",
    "    print(f\"  Max:    {rewards.max():.4f}\")\n",
    "    print(f\"  CoV:    {rewards.std(ddof=1)/rewards.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== HYPOTHESIS TESTS (Training Reward) ==========\n",
    "print(\"=\"*60)\n",
    "print(\"HYPOTHESIS TESTS (Training Reward)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if field_on_rewards is not None and field_off_rewards is not None:\n",
    "    welch = welch_t_test(field_on_rewards, field_off_rewards)\n",
    "    mw = mann_whitney_test(field_on_rewards, field_off_rewards)\n",
    "    poi = probability_of_improvement(field_on_rewards, field_off_rewards, n_bootstrap=5000, seed=42)\n",
    "\n",
    "    print(f\"\\n1. Welch's t-test:\")\n",
    "    print(f\"   t = {welch.statistic:.4f}, p = {welch.p_value:.6f}\")\n",
    "    d = abs(welch.effect_size)\n",
    "    d_str = \"negligible\" if d < 0.2 else \"small\" if d < 0.5 else \"MEDIUM\" if d < 0.8 else \"LARGE\"\n",
    "    print(f\"   Cohen's d = {welch.effect_size:.4f} ({d_str})\")\n",
    "\n",
    "    print(f\"\\n2. Mann-Whitney U test:\")\n",
    "    print(f\"   U = {mw.statistic:.1f}, p = {mw.p_value:.6f}\")\n",
    "    print(f\"   Rank-biserial r = {mw.effect_size:.4f}\")\n",
    "\n",
    "    print(f\"\\n3. Probability of Improvement:\")\n",
    "    print(f\"   P(Field ON > Field OFF) = {poi['prob_x_better']:.4f}\")\n",
    "    print(f\"   P(Field OFF > Field ON) = {poi['prob_y_better']:.4f}\")\n",
    "\n",
    "    print(f\"\\n4. Direction:\")\n",
    "    print(f\"   Field ON mean:  {field_on_rewards.mean():.4f}\")\n",
    "    print(f\"   Field OFF mean: {field_off_rewards.mean():.4f}\")\n",
    "    print(f\"   Gap: {field_on_rewards.mean() - field_off_rewards.mean():+.4f}\")\n",
    "else:\n",
    "    print(\"\\nSkipping: training data not available for both conditions.\")\n",
    "    welch = mw = poi = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== FULL METHOD COMPARISON (rliable) ==========\n",
    "print(\"=\"*60)\n",
    "print(\"FULL METHOD COMPARISON (rliable-style)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if field_on_rewards is not None and field_off_rewards is not None:\n",
    "    comparison = compare_methods(\n",
    "        {\"Field ON\": field_on_rewards, \"Field OFF\": field_off_rewards},\n",
    "        n_bootstrap=10000, seed=42,\n",
    "    )\n",
    "    print(comparison.summary)\n",
    "else:\n",
    "    print(\"Skipping: training data not available for both conditions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Comparison Plots (Training Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== COMPARISON PLOTS (Training Data) ==========\n",
    "if field_on_rewards is not None and field_off_rewards is not None:\n",
    "    setup_publication_style()\n",
    "    colors = ['#009988', '#BBBBBB']\n",
    "\n",
    "    iqm_on = compute_iqm(field_on_rewards, n_bootstrap=10000, seed=42)\n",
    "    iqm_off = compute_iqm(field_off_rewards, n_bootstrap=10000, seed=42)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # 1. Bar chart: IQM with CI\n",
    "    ax = axes[0]\n",
    "    methods = ['Field ON\\n(Stigmergy)', 'Field OFF\\n(No Field)']\n",
    "    iqm_vals = [iqm_on.iqm, iqm_off.iqm]\n",
    "    iqm_lo = [iqm_on.iqm - iqm_on.ci_lower, iqm_off.iqm - iqm_off.ci_lower]\n",
    "    iqm_hi = [iqm_on.ci_upper - iqm_on.iqm, iqm_off.ci_upper - iqm_off.iqm]\n",
    "    bars = ax.bar(methods, iqm_vals, yerr=[iqm_lo, iqm_hi], color=colors,\n",
    "                  edgecolor='black', linewidth=0.5, capsize=8)\n",
    "    for bar, val in zip(bars, iqm_vals):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(iqm_hi) + 0.05,\n",
    "                f'{val:.3f}', ha='center', fontsize=10)\n",
    "    ax.set_ylabel('IQM Reward')\n",
    "    ax.set_title('(a) IQM + 95% CI')\n",
    "\n",
    "    # 2. Violin + swarm\n",
    "    ax = axes[1]\n",
    "    parts = ax.violinplot([field_on_rewards, field_off_rewards], positions=[1, 2],\n",
    "                           showmeans=True, showmedians=True, showextrema=False)\n",
    "    for i, body in enumerate(parts['bodies']):\n",
    "        body.set_facecolor(colors[i])\n",
    "        body.set_alpha(0.4)\n",
    "    rng = np.random.default_rng(42)\n",
    "    for i, (data, pos) in enumerate(zip([field_on_rewards, field_off_rewards], [1, 2])):\n",
    "        jitter = rng.normal(0, 0.05, size=len(data))\n",
    "        ax.scatter(np.full_like(data, pos) + jitter, data, alpha=0.6, s=25,\n",
    "                   color=colors[i], edgecolor='black', linewidth=0.3, zorder=3)\n",
    "    ax.set_xticks([1, 2])\n",
    "    ax.set_xticklabels(['Field ON', 'Field OFF'])\n",
    "    ax.set_ylabel('Mean Reward')\n",
    "    ax.set_title('(b) Distribution')\n",
    "\n",
    "    # 3. Population comparison\n",
    "    ax = axes[2]\n",
    "    if field_on_populations is not None:\n",
    "        ax.hist(field_on_populations, bins=15, alpha=0.6, color=colors[0],\n",
    "                label='Field ON', edgecolor='black')\n",
    "    if field_off_populations is not None:\n",
    "        ax.hist(field_off_populations, bins=15, alpha=0.6, color=colors[1],\n",
    "                label='Field OFF', edgecolor='black')\n",
    "    ax.set_xlabel('Final Population (training)')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('(c) Population Distribution')\n",
    "    ax.axvline(x=64, color='red', linestyle='--', alpha=0.5, label='Max capacity')\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "    plt.suptitle('Hidden Food: Field ON vs Field OFF (Training Data)', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    save_figure(fig, os.path.join(OUTPUT_DIR, 'training_comparison'))\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping training plots: data not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== PERFORMANCE PROFILES ==========\n",
    "if field_on_rewards is not None and field_off_rewards is not None:\n",
    "    setup_publication_style()\n",
    "    fig = plot_performance_profiles(\n",
    "        {\"Field ON (Stigmergy)\": field_on_rewards, \"Field OFF (No Field)\": field_off_rewards},\n",
    "        output_path=os.path.join(OUTPUT_DIR, 'performance_profiles'),\n",
    "        tau_range=(0, 1.05),\n",
    "    )\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping performance profiles: training data not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Checkpoint Analysis — Hidden Food Eval\n",
    "\n",
    "Load ALL 60 checkpoints. For each seed, run hidden food eval + compute weight divergence\n",
    "in a SINGLE pass (don't load checkpoints twice).\n",
    "\n",
    "**Warning**: This takes ~5-10 minutes per seed on TPU. Total: ~5-10 hours for 60 seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== LOAD + EVAL ALL CHECKPOINTS (BOTH CONDITIONS) ==========\n\nNUM_EVAL_EPISODES = 1  # Set to 5 for higher precision (but ~30min/seed)\n\n\ndef load_seed_data(ckpt_path):\n    \"\"\"Load a checkpoint and extract seed data for eval + analysis.\"\"\"\n    ckpt = load_checkpoint(ckpt_path)\n    config = ckpt['config']\n    agent_params = jax.tree_util.tree_map(lambda x: x[0], ckpt['agent_params'])\n    network = ActorCritic(hidden_dims=tuple(config.agent.hidden_dims), num_actions=6)\n    result = {\n        'params': ckpt['params'],\n        'agent_params': agent_params,\n        'config': config,\n        'network': network,\n        'seed_id': ckpt.get('seed_id', -1),\n    }\n    del ckpt  # Free full checkpoint (includes opt_state) to reduce memory pressure\n    return result\n\n\nhf_eval_on = []\nhf_eval_off = []\ndivergence_on = []\ndivergence_off = []\n\nfor cond_name, ckpt_paths, eval_list, div_list in [\n    (\"Field ON\", field_on_ckpt_paths, hf_eval_on, divergence_on),\n    (\"Field OFF\", field_off_ckpt_paths, hf_eval_off, divergence_off),\n]:\n    print(f\"\\n{'='*60}\")\n    print(f\"ANALYZING {cond_name} ({len(ckpt_paths)} seeds, {NUM_EVAL_EPISODES} ep/seed)\")\n    print(f\"{'='*60}\")\n\n    for i, ckpt_path in enumerate(ckpt_paths):\n        seed_data = load_seed_data(ckpt_path)\n        key = jax.random.PRNGKey(42 + i)\n\n        # Hidden food eval\n        hf_result = run_hidden_food_eval(\n            seed_data['network'], seed_data['params'],\n            seed_data['config'], key, num_episodes=NUM_EVAL_EPISODES,\n        )\n        hf_result['seed_id'] = seed_data['seed_id']\n        eval_list.append(hf_result)\n\n        # Weight divergence\n        div = compute_weight_divergence(seed_data['agent_params'])\n        div_list.append({\n            'seed_id': seed_data['seed_id'],\n            'mean_divergence': float(div['mean_divergence']),\n            'max_divergence': float(div['max_divergence']),\n        })\n\n        del seed_data\n        gc.collect()\n\n        if (i + 1) % 5 == 0 or i == 0:\n            print(f\"  [{i+1}/{len(ckpt_paths)}] seed {eval_list[-1]['seed_id']}: \"\n                  f\"reg_food={hf_result['regular_food_collected']:.0f}, \"\n                  f\"hf_revealed={hf_result['hidden_food_revealed']:.1f}, \"\n                  f\"hf_collected={hf_result['hidden_food_collected']:.1f}, \"\n                  f\"div={div['mean_divergence']:.4f}\")\n\n# ========== EXTRACT NUMPY ARRAYS ==========\non_hf_revealed = np.array([r['hidden_food_revealed'] for r in hf_eval_on])\noff_hf_revealed = np.array([r['hidden_food_revealed'] for r in hf_eval_off])\non_hf_collected = np.array([r['hidden_food_collected'] for r in hf_eval_on])\noff_hf_collected = np.array([r['hidden_food_collected'] for r in hf_eval_off])\non_regular_food = np.array([r['regular_food_collected'] for r in hf_eval_on])\noff_regular_food = np.array([r['regular_food_collected'] for r in hf_eval_off])\non_regular_energy = np.array([r['regular_food_energy'] for r in hf_eval_on])\noff_regular_energy = np.array([r['regular_food_energy'] for r in hf_eval_off])\non_hidden_energy = np.array([r['hidden_food_energy'] for r in hf_eval_on])\noff_hidden_energy = np.array([r['hidden_food_energy'] for r in hf_eval_off])\non_eval_rewards = np.array([r['total_reward'] for r in hf_eval_on])\noff_eval_rewards = np.array([r['total_reward'] for r in hf_eval_off])\non_eval_pops = np.array([r['final_population'] for r in hf_eval_on])\noff_eval_pops = np.array([r['final_population'] for r in hf_eval_off])\n\non_mean_divs = np.array([r['mean_divergence'] for r in divergence_on])\noff_mean_divs = np.array([r['mean_divergence'] for r in divergence_off])\n\n# ========== SUMMARY TABLE ==========\nprint(f\"\\n{'='*60}\")\nprint(\"HIDDEN FOOD EVAL SUMMARY\")\nprint(f\"{'='*60}\")\nfor name, hf_rev, hf_col, reg_food, reg_e, hid_e, ev_rew, ev_pop, divs in [\n    (\"Field ON\", on_hf_revealed, on_hf_collected, on_regular_food,\n     on_regular_energy, on_hidden_energy, on_eval_rewards, on_eval_pops, on_mean_divs),\n    (\"Field OFF\", off_hf_revealed, off_hf_collected, off_regular_food,\n     off_regular_energy, off_hidden_energy, off_eval_rewards, off_eval_pops, off_mean_divs),\n]:\n    print(f\"\\n{name} ({len(hf_rev)} seeds):\")\n    print(f\"  Hidden food revealed:  {hf_rev.mean():.2f} +/- {hf_rev.std(ddof=1):.2f}\")\n    print(f\"  Hidden food collected: {hf_col.mean():.2f} +/- {hf_col.std(ddof=1):.2f}\")\n    print(f\"  Regular food:          {reg_food.mean():.1f} +/- {reg_food.std(ddof=1):.1f}\")\n    print(f\"  Regular energy:        {reg_e.mean():.0f}\")\n    print(f\"  Hidden energy:         {hid_e.mean():.0f}\")\n    print(f\"  Total eval reward:     {ev_rew.mean():.1f} +/- {ev_rew.std(ddof=1):.1f}\")\n    print(f\"  Final population:      {ev_pop.mean():.1f} +/- {ev_pop.std(ddof=1):.1f}\")\n    print(f\"  Weight divergence:     {divs.mean():.4f} +/- {divs.std(ddof=1):.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== HIDDEN FOOD STATISTICAL TESTS ==========\n",
    "print(\"=\"*60)\n",
    "print(\"HIDDEN FOOD COORDINATION TESTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for metric_name, on_vals, off_vals in [\n",
    "    (\"Hidden Food Revealed\", on_hf_revealed, off_hf_revealed),\n",
    "    (\"Hidden Food Collected\", on_hf_collected, off_hf_collected),\n",
    "    (\"Regular Food Collected\", on_regular_food, off_regular_food),\n",
    "    (\"Hidden Food Energy\", on_hidden_energy, off_hidden_energy),\n",
    "]:\n",
    "    print(f\"\\n--- {metric_name} ---\")\n",
    "    print(f\"  Field ON:  {on_vals.mean():.2f} +/- {on_vals.std(ddof=1):.2f}\")\n",
    "    print(f\"  Field OFF: {off_vals.mean():.2f} +/- {off_vals.std(ddof=1):.2f}\")\n",
    "\n",
    "    # Handle all-zero case\n",
    "    if on_vals.sum() == 0 and off_vals.sum() == 0:\n",
    "        print(\"  Both conditions show zero -- no coordination observed.\")\n",
    "        continue\n",
    "    if off_vals.std() == 0 and on_vals.std() == 0:\n",
    "        print(\"  No variance in either condition -- test not applicable.\")\n",
    "        continue\n",
    "\n",
    "    w = welch_t_test(on_vals, off_vals)\n",
    "    mw_hf = mann_whitney_test(on_vals, off_vals)\n",
    "    print(f\"  Welch: t={w.statistic:.3f}, p={w.p_value:.6f}, d={w.effect_size:.3f}\")\n",
    "    print(f\"  Mann-Whitney: U={mw_hf.statistic:.1f}, p={mw_hf.p_value:.6f}\")\n",
    "\n",
    "    if off_vals.sum() == 0:\n",
    "        print(\"  NOTE: Field OFF shows ZERO -- agents cannot coordinate without field.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== HIDDEN FOOD PLOTS (THE MONEY SHOTS) ==========\n",
    "setup_publication_style()\n",
    "colors = ['#009988', '#BBBBBB']\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(25, 5))\n",
    "\n",
    "# Helper for bar chart with zero handling\n",
    "def bar_with_zero_handling(ax, on_vals, off_vals, title, ylabel):\n",
    "    means = [on_vals.mean(), off_vals.mean()]\n",
    "    stds = [on_vals.std(ddof=1) if len(on_vals) > 1 else 0,\n",
    "            off_vals.std(ddof=1) if len(off_vals) > 1 else 0]\n",
    "    bars = ax.bar(['Field ON', 'Field OFF'], means, yerr=stds, color=colors,\n",
    "                  edgecolor='black', linewidth=0.5, capsize=8)\n",
    "    for bar, val in zip(bars, means):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(stds) * 0.1 + 0.01,\n",
    "                f'{val:.2f}', ha='center', fontsize=10)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    # Annotate if Field OFF is all zero\n",
    "    if off_vals.sum() == 0 and on_vals.sum() > 0:\n",
    "        ax.text(0.5, 0.1, 'Field OFF: No coordination\\nobserved (confirms hypothesis)',\n",
    "                ha='center', transform=ax.transAxes, fontsize=9, color='red',\n",
    "                style='italic')\n",
    "\n",
    "# 1. Hidden Food Revealed\n",
    "bar_with_zero_handling(axes[0], on_hf_revealed, off_hf_revealed,\n",
    "                       '(a) Hidden Food Revealed', 'Reveal Events')\n",
    "\n",
    "# 2. Hidden Food Collected\n",
    "bar_with_zero_handling(axes[1], on_hf_collected, off_hf_collected,\n",
    "                       '(b) Hidden Food Collected', 'Items Collected')\n",
    "\n",
    "# 3. Regular Food Collected\n",
    "bar_with_zero_handling(axes[2], on_regular_food, off_regular_food,\n",
    "                       '(c) Regular Food Collected', 'Items Collected')\n",
    "\n",
    "# 4. Energy Breakdown (stacked bar)\n",
    "ax = axes[3]\n",
    "x_pos = [0, 1]\n",
    "x_labels = ['Field ON', 'Field OFF']\n",
    "reg_means = [on_regular_energy.mean(), off_regular_energy.mean()]\n",
    "hid_means = [on_hidden_energy.mean(), off_hidden_energy.mean()]\n",
    "ax.bar(x_pos, reg_means, color=['#009988', '#BBBBBB'], edgecolor='black', linewidth=0.5,\n",
    "       label='Regular Food Energy')\n",
    "ax.bar(x_pos, hid_means, bottom=reg_means, color=['#006655', '#888888'], edgecolor='black',\n",
    "       linewidth=0.5, label='Hidden Food Energy')\n",
    "for i, (r, h) in enumerate(zip(reg_means, hid_means)):\n",
    "    ax.text(i, r + h + 50, f'{r+h:.0f}', ha='center', fontsize=10)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(x_labels)\n",
    "ax.set_ylabel('Total Energy')\n",
    "ax.set_title('(d) Energy Breakdown')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# 5. Hidden Food Collected Distribution (violin)\n",
    "ax = axes[4]\n",
    "# Check if we have non-zero data for violin\n",
    "has_on_data = on_hf_collected.sum() > 0\n",
    "has_off_data = off_hf_collected.sum() > 0\n",
    "\n",
    "if has_on_data or has_off_data:\n",
    "    data_to_plot = []\n",
    "    positions = []\n",
    "    plot_colors = []\n",
    "    if has_on_data:\n",
    "        data_to_plot.append(on_hf_collected)\n",
    "        positions.append(1)\n",
    "        plot_colors.append(colors[0])\n",
    "    if has_off_data:\n",
    "        data_to_plot.append(off_hf_collected)\n",
    "        positions.append(2)\n",
    "        plot_colors.append(colors[1])\n",
    "    if len(data_to_plot) > 0:\n",
    "        parts = ax.violinplot(data_to_plot, positions=positions,\n",
    "                               showmeans=True, showmedians=True, showextrema=False)\n",
    "        for i, body in enumerate(parts['bodies']):\n",
    "            body.set_facecolor(plot_colors[i])\n",
    "            body.set_alpha(0.4)\n",
    "    if not has_off_data:\n",
    "        ax.text(2, ax.get_ylim()[1] * 0.5, 'No coordination\\nobserved',\n",
    "                ha='center', fontsize=9, color='red', style='italic')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No hidden food coordination\\nobserved in either condition',\n",
    "            ha='center', va='center', transform=ax.transAxes, fontsize=10, color='red')\n",
    "\n",
    "ax.set_xticks([1, 2])\n",
    "ax.set_xticklabels(['Field ON', 'Field OFF'])\n",
    "ax.set_ylabel('Hidden Food Collected')\n",
    "ax.set_title('(e) HF Collected Distribution')\n",
    "\n",
    "plt.suptitle('Hidden Food Coordination: Field ON vs Field OFF', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "save_figure(fig, os.path.join(OUTPUT_DIR, 'hidden_food_coordination'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== WEIGHT DIVERGENCE + CORRELATION PLOTS ==========\nsetup_publication_style()\ncolors = ['#009988', '#BBBBBB']\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# (a) Weight divergence histogram\nax = axes[0, 0]\nax.hist(on_mean_divs, bins=12, alpha=0.6, color=colors[0], label='Field ON', edgecolor='black')\nax.hist(off_mean_divs, bins=12, alpha=0.6, color=colors[1], label='Field OFF', edgecolor='black')\nax.axvline(on_mean_divs.mean(), color=colors[0], linestyle='--', linewidth=2)\nax.axvline(off_mean_divs.mean(), color=colors[1], linestyle='--', linewidth=2)\nax.set_xlabel('Mean Pairwise Weight Divergence (cosine)')\nax.set_ylabel('Count')\nax.set_title('(a) Weight Divergence Distribution')\nax.legend()\n\n# (b) Eval population histogram\nax = axes[0, 1]\nax.hist(on_eval_pops, bins=15, alpha=0.6, color=colors[0], label='Field ON', edgecolor='black')\nax.hist(off_eval_pops, bins=15, alpha=0.6, color=colors[1], label='Field OFF', edgecolor='black')\nax.axvline(x=64, color='red', linestyle='--', alpha=0.5, label='Max capacity')\nax.set_xlabel('Eval Population')\nax.set_ylabel('Count')\nax.set_title('(b) Eval Population Distribution')\nax.legend()\n\n# (c) Hidden food collected vs weight divergence (scatter)\nax = axes[1, 0]\nax.scatter(on_mean_divs, on_hf_collected, color=colors[0], s=50, alpha=0.7,\n           edgecolor='black', linewidth=0.5, label='Field ON')\nax.scatter(off_mean_divs, off_hf_collected, color=colors[1], s=50, alpha=0.7,\n           edgecolor='black', linewidth=0.5, label='Field OFF')\n# Pearson correlation for Field ON only\nif on_hf_collected.sum() > 0 and on_mean_divs.std() > 0:\n    r, p = scipy_stats.pearsonr(on_mean_divs, on_hf_collected)\n    ax.text(0.05, 0.95, f'Field ON: r={r:.3f}, p={p:.4f}',\n            transform=ax.transAxes, fontsize=9, va='top',\n            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\nax.set_xlabel('Mean Weight Divergence')\nax.set_ylabel('Hidden Food Collected')\nax.set_title('(c) Coordination vs Specialization')\nax.legend()\n\n# (d) Eval reward vs population scatter\nax = axes[1, 1]\nax.scatter(on_eval_pops, on_eval_rewards, color=colors[0], s=50, alpha=0.7,\n           edgecolor='black', linewidth=0.5, label='Field ON')\nax.scatter(off_eval_pops, off_eval_rewards, color=colors[1], s=50, alpha=0.7,\n           edgecolor='black', linewidth=0.5, label='Field OFF')\nax.set_xlabel('Final Population')\nax.set_ylabel('Total Eval Reward')\nax.set_title('(d) Eval: Reward vs Population')\nax.legend()\n\nplt.suptitle('Weight Divergence & Correlations', fontsize=14, y=1.01)\nplt.tight_layout()\nsave_figure(fig, os.path.join(OUTPUT_DIR, 'divergence_correlation'))\nplt.show()\n\n# Divergence statistical comparison\ndiv_welch = welch_t_test(on_mean_divs, off_mean_divs)\ndiv_mw = mann_whitney_test(on_mean_divs, off_mean_divs)\nprint(f\"\\nDivergence comparison:\")\nprint(f\"  Welch t-test: t={div_welch.statistic:.4f}, p={div_welch.p_value:.6f}, d={div_welch.effect_size:.4f}\")\nprint(f\"  Mann-Whitney: U={div_mw.statistic:.1f}, p={div_mw.p_value:.6f}, r={div_mw.effect_size:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Report & Save\n",
    "\n",
    "Generate formatted comparison report and save all results to Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== GENERATE MARKDOWN REPORT ==========\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Compute stats for report\n",
    "div_welch = welch_t_test(on_mean_divs, off_mean_divs)\n",
    "\n",
    "# Hidden food stats\n",
    "hf_rev_welch = None\n",
    "hf_col_welch = None\n",
    "if on_hf_revealed.sum() > 0 or off_hf_revealed.sum() > 0:\n",
    "    if on_hf_revealed.std() > 0 or off_hf_revealed.std() > 0:\n",
    "        hf_rev_welch = welch_t_test(on_hf_revealed, off_hf_revealed)\n",
    "if on_hf_collected.sum() > 0 or off_hf_collected.sum() > 0:\n",
    "    if on_hf_collected.std() > 0 or off_hf_collected.std() > 0:\n",
    "        hf_col_welch = welch_t_test(on_hf_collected, off_hf_collected)\n",
    "\n",
    "# Correlation\n",
    "corr_str = \"N/A (no hidden food collected)\"\n",
    "if on_hf_collected.sum() > 0 and on_mean_divs.std() > 0:\n",
    "    r_corr, p_corr = scipy_stats.pearsonr(on_mean_divs, on_hf_collected)\n",
    "    corr_str = f\"r={r_corr:.3f}, p={p_corr:.4f}\"\n",
    "\n",
    "# Energy advantage\n",
    "energy_advantage = on_hidden_energy.mean() - off_hidden_energy.mean()\n",
    "\n",
    "report = f\"\"\"# Hidden Food Coordination Analysis Report\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Experiment Setup\n",
    "- **Conditions**: Field ON (stigmergy) vs Field OFF (no shared field)\n",
    "- **Seeds per condition**: {len(field_on_ckpt_paths)} Field ON, {len(field_off_ckpt_paths)} Field OFF\n",
    "- **Training steps**: 10M per seed\n",
    "- **Config**: 64-agent, grid=32, num_food=40\n",
    "- **Hidden food**: 3 items, require K=3 agents within distance 3 to reveal, 5x value (500 energy)\n",
    "- **Field ON**: diffusion=0.1, decay=0.05, write_strength=1.0\n",
    "- **Field OFF**: diffusion=0.0, decay=1.0, write_strength=0.0\n",
    "- **Eval episodes per seed**: {NUM_EVAL_EPISODES}\n",
    "\n",
    "## Training Reward Comparison\n",
    "\"\"\"\n",
    "\n",
    "if field_on_rewards is not None and field_off_rewards is not None:\n",
    "    iqm_on = compute_iqm(field_on_rewards, n_bootstrap=10000, seed=42)\n",
    "    iqm_off = compute_iqm(field_off_rewards, n_bootstrap=10000, seed=42)\n",
    "    report += f\"\"\"| Metric | Field ON | Field OFF |\n",
    "|--------|----------|----------|\n",
    "| Mean | {field_on_rewards.mean():.4f} +/- {field_on_rewards.std(ddof=1):.4f} | {field_off_rewards.mean():.4f} +/- {field_off_rewards.std(ddof=1):.4f} |\n",
    "| Median | {np.median(field_on_rewards):.4f} | {np.median(field_off_rewards):.4f} |\n",
    "| IQM | {iqm_on.iqm:.4f} [{iqm_on.ci_lower:.4f}, {iqm_on.ci_upper:.4f}] | {iqm_off.iqm:.4f} [{iqm_off.ci_lower:.4f}, {iqm_off.ci_upper:.4f}] |\n",
    "\n",
    "\"\"\"\n",
    "else:\n",
    "    report += \"Training summaries not available on Drive.\\n\\n\"\n",
    "\n",
    "report += f\"\"\"## Hidden Food Coordination Metrics (THE KEY SECTION)\n",
    "\n",
    "| Metric | Field ON | Field OFF |\n",
    "|--------|----------|----------|\n",
    "| Hidden food revealed | {on_hf_revealed.mean():.2f} +/- {on_hf_revealed.std(ddof=1):.2f} | {off_hf_revealed.mean():.2f} +/- {off_hf_revealed.std(ddof=1):.2f} |\n",
    "| Hidden food collected | {on_hf_collected.mean():.2f} +/- {on_hf_collected.std(ddof=1):.2f} | {off_hf_collected.mean():.2f} +/- {off_hf_collected.std(ddof=1):.2f} |\n",
    "| Regular food collected | {on_regular_food.mean():.1f} +/- {on_regular_food.std(ddof=1):.1f} | {off_regular_food.mean():.1f} +/- {off_regular_food.std(ddof=1):.1f} |\n",
    "| Regular food energy | {on_regular_energy.mean():.0f} | {off_regular_energy.mean():.0f} |\n",
    "| Hidden food energy | {on_hidden_energy.mean():.0f} | {off_hidden_energy.mean():.0f} |\n",
    "| Total energy | {(on_regular_energy + on_hidden_energy).mean():.0f} | {(off_regular_energy + off_hidden_energy).mean():.0f} |\n",
    "\n",
    "### Statistical Tests (Hidden Food)\n",
    "\"\"\"\n",
    "\n",
    "if hf_rev_welch is not None:\n",
    "    report += f\"- **Hidden Food Revealed**: Welch t={hf_rev_welch.statistic:.3f}, p={hf_rev_welch.p_value:.6f}, d={hf_rev_welch.effect_size:.3f}\\n\"\n",
    "else:\n",
    "    report += \"- **Hidden Food Revealed**: Test not applicable (zero variance or all zeros)\\n\"\n",
    "\n",
    "if hf_col_welch is not None:\n",
    "    report += f\"- **Hidden Food Collected**: Welch t={hf_col_welch.statistic:.3f}, p={hf_col_welch.p_value:.6f}, d={hf_col_welch.effect_size:.3f}\\n\"\n",
    "else:\n",
    "    report += \"- **Hidden Food Collected**: Test not applicable (zero variance or all zeros)\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "### Energy Breakdown\n",
    "- Field ON gets **{energy_advantage:.0f} extra energy** from hidden food coordination\n",
    "- This represents a {energy_advantage / max(off_regular_energy.mean(), 1) * 100:.1f}% bonus over Field OFF's regular food energy\n",
    "\n",
    "### Interpretation\n",
    "\"\"\"\n",
    "\n",
    "if off_hf_revealed.sum() == 0 and on_hf_revealed.sum() > 0:\n",
    "    report += \"\"\"**Field OFF shows ZERO hidden food reveals.** This confirms agents CANNOT coordinate\n",
    "without the shared field. The field is necessary and sufficient for the coordination task.\n",
    "\"\"\"\n",
    "elif on_hf_revealed.mean() > off_hf_revealed.mean():\n",
    "    report += \"\"\"Field ON shows higher hidden food coordination than Field OFF.\n",
    "The shared field enables agents to signal hidden food locations.\n",
    "\"\"\"\n",
    "else:\n",
    "    report += \"\"\"Results are inconclusive or unexpected. Further investigation needed.\n",
    "\"\"\"\n",
    "\n",
    "report += f\"\"\"\n",
    "## Eval Episode Metrics\n",
    "\n",
    "| Metric | Field ON | Field OFF |\n",
    "|--------|----------|----------|\n",
    "| Mean total reward | {on_eval_rewards.mean():.1f} +/- {on_eval_rewards.std(ddof=1):.1f} | {off_eval_rewards.mean():.1f} +/- {off_eval_rewards.std(ddof=1):.1f} |\n",
    "| Mean population | {on_eval_pops.mean():.1f} +/- {on_eval_pops.std(ddof=1):.1f} | {off_eval_pops.mean():.1f} +/- {off_eval_pops.std(ddof=1):.1f} |\n",
    "\n",
    "## Weight Divergence Comparison\n",
    "\n",
    "| Metric | Field ON | Field OFF |\n",
    "|--------|----------|----------|\n",
    "| Mean divergence | {on_mean_divs.mean():.4f} +/- {on_mean_divs.std(ddof=1):.4f} | {off_mean_divs.mean():.4f} +/- {off_mean_divs.std(ddof=1):.4f} |\n",
    "| Welch p-value | {div_welch.p_value:.6f} | Cohen's d = {div_welch.effect_size:.4f} |\n",
    "\n",
    "## Correlation: Hidden Food vs Divergence\n",
    "- Field ON Pearson correlation (hidden food collected vs weight divergence): {corr_str}\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "1. **Coordination signal**: {'The shared field enables hidden food coordination (Field ON > Field OFF)' if on_hf_collected.mean() > off_hf_collected.mean() else 'Results inconclusive on coordination advantage'}\n",
    "2. **Energy advantage**: Field ON gains {energy_advantage:.0f} extra energy from hidden food\n",
    "3. **Specialization**: {'Field ON shows higher weight divergence, suggesting the field enables specialization' if on_mean_divs.mean() > off_mean_divs.mean() else 'Weight divergence comparison requires further analysis'}\n",
    "4. **Note on reveal counting**: Hidden food revealed counts are LOWER BOUNDS (same-step reveal+collect events are missed by the diff method). Hidden food collected counts are EXACT.\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(report))\n",
    "print(\"\\nReport generated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== SAVE RESULTS ==========\n",
    "\n",
    "# Prepare results dict\n",
    "results = {\n",
    "    'metadata': {\n",
    "        'generated': datetime.now().isoformat(),\n",
    "        'field_on_seeds': len(field_on_ckpt_paths),\n",
    "        'field_off_seeds': len(field_off_ckpt_paths),\n",
    "        'steps_per_seed': 10_000_000,\n",
    "        'eval_episodes_per_seed': NUM_EVAL_EPISODES,\n",
    "        'hidden_food_config': {\n",
    "            'num_hidden': 3,\n",
    "            'required_agents': 3,\n",
    "            'reveal_distance': 3,\n",
    "            'value_multiplier': 5.0,\n",
    "        },\n",
    "    },\n",
    "    'hidden_food_metrics': {\n",
    "        'field_on': {\n",
    "            'hf_revealed': on_hf_revealed.tolist(),\n",
    "            'hf_collected': on_hf_collected.tolist(),\n",
    "            'regular_food': on_regular_food.tolist(),\n",
    "            'regular_energy': on_regular_energy.tolist(),\n",
    "            'hidden_energy': on_hidden_energy.tolist(),\n",
    "        },\n",
    "        'field_off': {\n",
    "            'hf_revealed': off_hf_revealed.tolist(),\n",
    "            'hf_collected': off_hf_collected.tolist(),\n",
    "            'regular_food': off_regular_food.tolist(),\n",
    "            'regular_energy': off_regular_energy.tolist(),\n",
    "            'hidden_energy': off_hidden_energy.tolist(),\n",
    "        },\n",
    "    },\n",
    "    'eval_metrics': {\n",
    "        'field_on': {\n",
    "            'rewards': on_eval_rewards.tolist(),\n",
    "            'populations': on_eval_pops.tolist(),\n",
    "            'per_seed': [{k: v for k, v in r.items() if k != 'per_episode'}\n",
    "                         for r in hf_eval_on],\n",
    "        },\n",
    "        'field_off': {\n",
    "            'rewards': off_eval_rewards.tolist(),\n",
    "            'populations': off_eval_pops.tolist(),\n",
    "            'per_seed': [{k: v for k, v in r.items() if k != 'per_episode'}\n",
    "                         for r in hf_eval_off],\n",
    "        },\n",
    "    },\n",
    "    'weight_divergence': {\n",
    "        'field_on': on_mean_divs.tolist(),\n",
    "        'field_off': off_mean_divs.tolist(),\n",
    "        'welch_p': float(div_welch.p_value),\n",
    "        'cohens_d': float(div_welch.effect_size),\n",
    "    },\n",
    "    'training_rewards': {\n",
    "        'field_on': field_on_rewards.tolist() if field_on_rewards is not None else None,\n",
    "        'field_off': field_off_rewards.tolist() if field_off_rewards is not None else None,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save JSON\n",
    "json_path = os.path.join(OUTPUT_DIR, 'hidden_food_results.json')\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"JSON saved: {json_path}\")\n",
    "\n",
    "# Save pickle (preserves numpy arrays)\n",
    "pkl_path = os.path.join(OUTPUT_DIR, 'hidden_food_results.pkl')\n",
    "with open(pkl_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        **results,\n",
    "        # Include numpy arrays directly for convenience\n",
    "        'numpy_arrays': {\n",
    "            'on_hf_revealed': on_hf_revealed,\n",
    "            'off_hf_revealed': off_hf_revealed,\n",
    "            'on_hf_collected': on_hf_collected,\n",
    "            'off_hf_collected': off_hf_collected,\n",
    "            'on_regular_food': on_regular_food,\n",
    "            'off_regular_food': off_regular_food,\n",
    "            'on_regular_energy': on_regular_energy,\n",
    "            'off_regular_energy': off_regular_energy,\n",
    "            'on_hidden_energy': on_hidden_energy,\n",
    "            'off_hidden_energy': off_hidden_energy,\n",
    "            'on_eval_rewards': on_eval_rewards,\n",
    "            'off_eval_rewards': off_eval_rewards,\n",
    "            'on_eval_pops': on_eval_pops,\n",
    "            'off_eval_pops': off_eval_pops,\n",
    "            'on_mean_divs': on_mean_divs,\n",
    "            'off_mean_divs': off_mean_divs,\n",
    "        },\n",
    "    }, f)\n",
    "print(f\"Pickle saved: {pkl_path}\")\n",
    "\n",
    "# Save report markdown\n",
    "md_path = os.path.join(OUTPUT_DIR, 'hidden_food_report.md')\n",
    "with open(md_path, 'w') as f:\n",
    "    f.write(report)\n",
    "print(f\"Report saved: {md_path}\")\n",
    "\n",
    "# List all output files\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL OUTPUT FILES:\")\n",
    "for fname in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    fpath = os.path.join(OUTPUT_DIR, fname)\n",
    "    size_mb = os.path.getsize(fpath) / 1024 / 1024\n",
    "    print(f\"  {fname} ({size_mb:.2f} MB)\")\n",
    "print(f\"\\nDone! All results saved to {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}