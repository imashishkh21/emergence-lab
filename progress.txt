# Phase 3 Progress: Specialization Detection

## Session Start
Date: 2026-02-02
Previous Phases:

## Phase 3 Goal
Detect when agents evolve into different "species" with distinct strategies.
Prove that field enables DIVERSITY, not just higher reward.

## Key Insight from Phase 2
- Random field HURTS agents (585 < 600) → they learned to READ field
- Field might enable specialization, not just coordination

---

## Task Log

### US-001: Weight Divergence Metric ✓
**What:** Implemented `compute_weight_divergence()` and `flatten_agent_params()` in `src/analysis/specialization.py`.
**Decisions:**
- Used cosine distance (1 - cosine_similarity) as the divergence metric — standard for comparing high-dimensional weight vectors
- Supports alive_mask to only compare living agents
- Returns dict with mean_divergence, max_divergence, full divergence_matrix, and agent_indices
- Flattens all pytree leaves per-agent into a single 1D vector for comparison
**Files changed:**
- `src/analysis/specialization.py` (new) — `flatten_agent_params`, `compute_weight_divergence`
- `tests/test_specialization.py` (new) — 10 tests covering identical/different agents, symmetry, alive mask, edge cases, multi-leaf params
**Validation:** mypy clean, 184/184 tests pass

---

### US-002: Behavioral Feature Extraction ✓
**What:** Implemented `extract_behavior_features(trajectory)` in `src/analysis/specialization.py` that extracts 7 behavioral features per agent from trajectory data.
**Decisions:**
- Trajectory input is a simple dict of numpy arrays (actions, positions, rewards, alive_mask, energy, optional births/field_values) — keeps it generic and decoupled from US-003's TrajectoryRecorder
- 7 features per agent: movement_entropy, food_collection_rate, distance_per_step, reproduction_rate, mean_energy, exploration_ratio, action_stay_fraction
- Movement entropy normalized to [0,1] using scipy.stats.entropy / log(num_actions)
- Reproduction rate: uses explicit 'births' key if available, otherwise infers from action 5 count
- Dead agents (never alive in trajectory) get all-zero feature vectors
- Only alive steps are used for all metrics (via alive_mask filtering)
- Stay fraction counts both action 0 (stay) and action 5 (reproduce) since both keep agent stationary
- Added helper functions `_movement_entropy()`, `_distance_traveled()`, `_exploration_ratio()` for modularity
**Files changed:**
- `src/analysis/specialization.py` — added `extract_behavior_features`, `_movement_entropy`, `_distance_traveled`, `_exploration_ratio`; added `scipy.stats.entropy` import
- `tests/test_specialization.py` — added `TestBehaviorFeatures` class with 15 tests covering shape, finiteness, dead agents, deterministic/uniform entropy, food rate, distance, reproduction from births/actions, energy, exploration, stay fraction, partial alive, different agents
**Validation:** mypy clean, 199/199 tests pass

---

### US-003: Trajectory Recording ✓
**What:** Implemented `TrajectoryRecorder` class and `record_episode()` function in `src/analysis/trajectory.py` for recording per-agent, per-step data during evaluation episodes.
**Decisions:**
- `TrajectoryRecorder` is a simple append-based collector that stores per-step numpy arrays, then stacks them into a trajectory dict via `get_trajectory()`
- Accepts both JAX arrays and numpy arrays (converts to numpy on record)
- Optional fields: `births` (bool) and `field_values` (float) — only included in output if provided
- `record_episode()` runs a single episode using the same pattern as `video.py`'s `record_episode` (single env, Python loop)
- Supports both stochastic (default) and deterministic action modes via `deterministic` kwarg
- Births detected by comparing `agent_alive` before and after step (new alive = birth)
- Field values recorded as mean across channels at agent positions (using `read_local` with radius=0)
- `alive_mask` captures pre-step alive state (consistent with `extract_behavior_features` expectations)
- Output format directly compatible with `extract_behavior_features()` from US-002
**Files changed:**
- `src/analysis/trajectory.py` (new) — `TrajectoryRecorder` class, `record_episode()` function
- `tests/test_specialization.py` — added `TestTrajectoryRecording` class with 13 tests covering: basic shapes, num_steps property, empty raises, optional births/field_values, data preservation, JAX array acceptance, compatibility with behavior features, record_episode runs/shapes/births+field/deterministic/features pipeline/alive mask
**Validation:** mypy clean (31 files), 212/212 tests pass

---

### US-004: Behavioral Clustering ✓
**What:** Implemented `cluster_agents()` and `find_optimal_clusters()` in `src/analysis/specialization.py` for K-means-based behavioral clustering with silhouette score evaluation.
**Decisions:**
- Features are standardized (StandardScaler: zero mean, unit variance) before clustering so all features contribute equally regardless of scale
- `cluster_agents()` uses sklearn KMeans with n_init=10 for robust convergence
- n_clusters is clamped to the number of unique data points to avoid degenerate K-means
- Silhouette score guard: returns 0.0 when n_samples <= n_labels (sklearn requires n_samples > n_labels)
- `find_optimal_clusters()` tries k=2..min(max_k, n_unique) and picks the k with the highest silhouette score
- Degenerate cases (all identical features, single agent) gracefully return k=1 with silhouette=0.0
- Returns dict with labels, centroids (in standardized space), silhouette score, and n_clusters
**Files changed:**
- `src/analysis/specialization.py` — added `cluster_agents`, `find_optimal_clusters`; added sklearn imports (KMeans, silhouette_score, StandardScaler)
- `tests/test_specialization.py` — added `TestClustering` class with 15 tests covering: required keys, label shapes, centroid shapes, silhouette range, well-separated clusters, identical features, two agents, k clamping, find_optimal keys, correct k detection, silhouette_scores dict, identical data, single agent, reproducibility, full pipeline
**Validation:** mypy clean (31 files), 227/227 tests pass

---

### US-005: Specialization Score ✓
**What:** Implemented `specialization_score()` and `novelty_score()` in `src/analysis/specialization.py`.
**Decisions:**
- `specialization_score()` combines three normalized [0,1] components with configurable weights:
  - Silhouette component (weight 0.5): optimal clustering silhouette score clamped to [0,1] (negative values = poor clustering → 0)
  - Weight divergence component (weight 0.25): mean cosine distance / 2.0 (cosine distance range is [0,2])
  - Behavioral variance component (weight 0.25): tanh(mean variance of standardized features) — maps naturally to [0,1]
- When `agent_params` is None, divergence weight is redistributed equally to silhouette and variance components
- `novelty_score()` implements Lehman & Stanley (2011) k-NN distance metric
  - Uses `scipy.spatial.distance.cdist` for efficient Euclidean distance computation
  - Uses `np.argpartition` for O(n) k-nearest selection vs O(n log n) full sort
  - Gracefully handles edge cases: empty archive, k=0, k > archive_size
**Files changed:**
- `src/analysis/specialization.py` — added `specialization_score`, `novelty_score`; added `scipy.spatial.distance.cdist` import
- `tests/test_specialization.py` — added `TestSpecializationScore` (11 tests) and `TestNoveltyScore` (9 tests) covering: required keys, score range, identical/separated clusters, single agent, with/without params, component ranges, diversity ordering, custom weights, pipeline, output shape, non-negativity, identical-to-archive, novel agent, empty archive, k edge cases, single agent/archive, k=0
**Validation:** mypy clean (31 files), 247/247 tests pass

---

### US-006: Field Usage Analysis ✓
**What:** Implemented `analyze_field_usage()` and `_classify_cluster_role()` in `src/analysis/specialization.py`.
**Decisions:**
- Since all alive agents write to the field automatically (not action-dependent), "write frequency" = fraction of steps an agent is alive
- Per-cluster statistics computed: write_frequency, mean_field_value, field_value_std, movement_rate, spatial_spread, field_action_correlation
- Movement rate: fraction of alive steps where position changed (distinguishes movers from sitters)
- Spatial spread: unique positions / alive steps (how spread out an agent's trajectory is)
- Field-action correlation: Pearson correlation between field_value[t] and whether agent moved at step t+1 — captures whether agents are "guided" by the field
- Role classification heuristic: writers = high movement + low field values, readers = low movement + high field values, balanced = neither extreme
- Gracefully handles missing field_values key (returns 0.0 for field-based stats)
- Dead agents contribute zero to all metrics
**Files changed:**
- `src/analysis/specialization.py` — added `analyze_field_usage`, `_classify_cluster_role`
- `tests/test_specialization.py` — added `TestFieldUsage` class with 15 tests covering: required keys, num_clusters, per-cluster stats, cluster roles, write_frequency range, all-alive frequency, dead agent stats, stationary/moving agents, reader/writer detection, without field_values, single cluster, spatial_spread range, field_action_correlation range, full pipeline
**Validation:** mypy clean (31 files), 262/262 tests pass

---

### US-007: Specialization Tracker ✓
**What:** Implemented `SpecializationTracker` class and `SpecializationEvent` dataclass in `src/analysis/specialization.py`, integrated into `src/training/train.py`.
**Decisions:**
- Follows the `EmergenceTracker` pattern: rolling window z-score detection for phase transitions
- Tracks three metrics per update: `weight_divergence` (mean cosine distance), `max_divergence`, and `num_alive`
- Detects "specialization events" when any metric deviates beyond `z_threshold` (default 3.0) standard deviations from the rolling window mean (default window_size=20)
- Added `specialization_check_interval: int = 20000` to `AnalysisConfig` — set to 2x emergence_check_interval since weight divergence computation requires flattening all agent params
- `get_metrics()` returns dict with `specialization/` prefix for W&B logging
- `get_summary()` returns final/mean/std for each metric plus all detected events
- Integration in train.py: only runs when `evolution.enabled`, uses first env's per-agent params and alive mask, logs to tqdm and W&B
- Prints specialization summary after training (final divergence, mean divergence, events)
- Added `import numpy as np` to train.py (was missing, needed for `np.asarray` of alive mask)
**Files changed:**
- `src/configs.py` — added `specialization_check_interval: int = 20000` to `AnalysisConfig`
- `src/analysis/specialization.py` — added `SpecializationEvent` dataclass, `SpecializationTracker` class with `update()`, `get_metrics()`, `get_summary()`; added `from src.configs import Config` import
- `src/training/train.py` — imported `SpecializationTracker` and `numpy`, instantiated tracker, added specialization check block in training loop, added specialization summary printing
- `tests/test_specialization.py` — added `TestSpecializationTracker` class with 15 tests covering: init, update returns, step count increment, history recording, identical/different agent divergence, alive count tracking, get_metrics keys and finiteness, get_summary keys, sudden divergence event detection, steady divergence no events, JAX alive mask acceptance, event str output, history growth
**Validation:** mypy clean (31 files), 277/277 tests pass

---

### US-008: Lineage-Strategy Correlation ✓
**What:** Implemented `correlate_lineage_strategy()` in `src/analysis/specialization.py` to analyze whether agents from the same lineage (shared ancestry) cluster into the same behavioral strategies.
**Decisions:**
- A "lineage" is defined by the root ancestor (parent_id == -1) — each agent is traced upward through the parent chain to its root
- Computes per-lineage "homogeneity": fraction of members in the most common cluster (1.0 = all in same cluster)
- Only computes homogeneity for lineages with 2+ members in the agent set (single-member lineages are excluded from homogeneity stats)
- "Specialist lineage" threshold: >= 70% of members in the same cluster (consistent with PRD species detection criteria)
- Specialist lineages are sorted by homogeneity descending for easy inspection
- Agents not found in the LineageTracker are treated as their own root (graceful handling of unknown agents)
- Returns comprehensive dict: lineage_cluster_map, lineage_homogeneity, specialist_lineages, mean_homogeneity, num_lineages, num_specialist_lineages
- Added `from src.analysis.lineage import LineageTracker` import to specialization.py
**Files changed:**
- `src/analysis/specialization.py` — added `correlate_lineage_strategy()` function and `LineageTracker` import
- `tests/test_specialization.py` — added `TestLineageCorrelation` class with 14 tests covering: required keys, perfect alignment (homogeneity=1.0), mixed clusters (homogeneity<1), specialist detection, dominant cluster reporting, num_lineages count, cluster_map structure, single-agent lineage exclusion, grandchild root tracing, unknown agent handling, homogeneity range, list agent_ids, sorted specialists, full pipeline (cluster_agents → correlate_lineage_strategy)
**Validation:** mypy clean (31 files), 291/291 tests pass

---

### US-009: Diversity vs Performance Ablation ✓
**What:** Implemented specialization ablation comparing populations with divergent (specialized) weights vs uniform (cloned) weights vs random weights, plus a script to train and run the comparison.
**Decisions:**
- Added 3 weight conditions to compare: `divergent` (trained per-agent weights), `uniform` (all agents cloned to mean of alive agents' weights), `random_weights` (mean weights + Gaussian noise)
- `_make_uniform_params()` averages alive agents' weights and broadcasts to all slots — tests whether weight diversity matters
- `_make_random_params()` adds i.i.d. Gaussian noise (std=0.1) to mean weights — tests whether learned divergence is better than random noise
- `SpecializationAblationResult` extends the existing ablation pattern with food_collected and population_stability metrics
- `population_stability` measured as std of alive population count over time per episode (lower = more stable)
- `_run_specialization_episode()` tracks food collected via before/after food_collected diff at each step
- `print_specialization_ablation_results()` prints a formatted table and computes divergent-vs-uniform and divergent-vs-random comparisons with interpretive text
- `scripts/run_specialization_ablation.py` follows the pattern of `scripts/run_ablation.py`: trains briefly, computes pre-ablation specialization score, runs 3-condition ablation, prints results
- Script supports `--skip-training` + `--checkpoint` for loading pre-trained models, and saves checkpoint with both shared params and per-agent params
- Pre-ablation analysis reports weight divergence, specialization score breakdown, and optimal cluster count
**Files changed:**
- `src/analysis/ablation.py` — added `SpecializationAblationResult`, `_SpecEpisodeStats`, `_make_uniform_params`, `_make_random_params`, `_replace_agent_params`, `_run_specialization_episode`, `specialization_ablation_test`, `print_specialization_ablation_results`, `WeightCondition` type alias; added `Any` to typing imports; updated module docstring
- `scripts/run_specialization_ablation.py` (new) — full training + specialization analysis + ablation comparison script
**Validation:** mypy clean (31 files), 291/291 tests pass, script produces comparison output

---

### US-010: Specialization Visualization ✓
**What:** Implemented 4 plotting functions in `src/analysis/visualization.py` that produce PNG files from existing analysis data structures.
**Decisions:**
- `matplotlib.use("Agg")` set before any pyplot import for headless support
- `_save_and_close(fig, output_path)` helper: creates parent dirs, saves at dpi=150, always closes figure to prevent memory leaks, returns figure for test inspection
- `plot_behavior_clusters()`: PCA (default) or t-SNE to reduce 7D features to 2D scatter plot; PCA labels include explained variance %; t-SNE uses `perplexity=min(30, n_samples-1)`; edge case for `n_samples < 2` plots raw first 2 features
- `plot_weight_divergence_over_time()`: solid line for mean divergence with fill_between, dashed line for optional max divergence; grid enabled
- `plot_field_usage_by_cluster()`: grouped bar chart with 5 metrics (write_frequency, mean_field_value, movement_rate, spatial_spread, field_action_correlation) × N clusters; legend labels include cluster role (writer/reader/balanced)
- `plot_specialization_score_over_time()`: thick red line for composite score, dashed lines for optional components (silhouette/divergence/variance); y-axis fixed [0, 1.05]; gray dotted midpoint at 0.5
- Constants: `BEHAVIOR_FEATURE_NAMES` (7 names matching `extract_behavior_features` output order), `CLUSTER_COLORS` (tab10 palette)
- `__init__.py` left unchanged (project convention: import from submodules directly)
**Files changed:**
- `src/analysis/visualization.py` (new) — `_save_and_close`, `plot_behavior_clusters`, `plot_weight_divergence_over_time`, `plot_field_usage_by_cluster`, `plot_specialization_score_over_time`; constants `BEHAVIOR_FEATURE_NAMES`, `CLUSTER_COLORS`
- `tests/test_visualization.py` (new) — 18 tests across 4 classes: `TestPlotBehaviorClusters` (5 tests: figure type, PNG output, PCA axis labels, t-SNE method, single cluster), `TestPlotWeightDivergenceOverTime` (4 tests: figure type, PNG output, with/without max divergence), `TestPlotFieldUsageByCluster` (4 tests: figure type, PNG output, multiple clusters, roles in legend), `TestPlotSpecializationScoreOverTime` (5 tests: figure type, PNG output, y-axis bounds, with/without components)
**Validation:** ruff clean, mypy clean, black formatted, 309/309 tests pass

---

### US-011: Species Detection ✓
**What:** Implemented `detect_species()` function and `Species` dataclass in `src/analysis/specialization.py` for formally detecting when distinct "species" have emerged in the population.
**Decisions:**
- A "species" requires two conditions: (1) clear cluster boundaries (overall silhouette >= threshold, default 0.7), and (2) hereditary membership (>= 70% of parent-child pairs in same cluster)
- `Species` dataclass stores: cluster_id, num_members, agent_indices, centroid (standardized space), silhouette, heredity_score, mean_features (raw space), and optional role label
- Uses `find_optimal_clusters()` internally to determine optimal number of clusters and get silhouette score
- Global heredity_score computed as fraction of all parent-child pairs (where both are in the agent set) that share a cluster
- Per-cluster heredity computed separately — each cluster must individually pass the 70% heredity threshold to qualify as a species
- When no `lineage_tracker` is provided, heredity check is skipped (all qualifying clusters are treated as species, heredity=1.0 assumed)
- Returns comprehensive dict: species list, num_species, silhouette, optimal_k, all_labels, heredity_score, is_speciated bool
- `is_speciated` is True when at least one species is detected
**Files changed:**
- `src/analysis/specialization.py` — added `Species` dataclass and `detect_species()` function
- `tests/test_specialization.py` — added `TestSpeciesDetection` class with 14 tests covering: required keys, well-separated clusters detected, identical agents no species, species object attributes, all_labels shape, species cover agents, heredity with lineage tracker, heredity zero without lineage, high threshold reduces species, single agent, non-hereditary clusters excluded, mean features correct, silhouette matches clustering, full pipeline
**Validation:** mypy clean (32 files), 323/323 tests pass

---

### US-012: Specialization Report ✓
**What:** Created `scripts/generate_specialization_report.py` that generates a comprehensive markdown report with visualizations from a trained model's specialization analysis.
**Decisions:**
- Script follows the same pattern as `run_specialization_ablation.py`: supports both training from scratch and loading from checkpoint
- Report generation pipeline: (1) load/train model → (2) compute weight divergence → (3) record trajectories → (4) extract features & compute specialization score → (5) detect species → (6) analyze field usage → (7) run ablation (optional) → (8) generate visualizations → (9) write markdown report
- Multiple trajectory episodes (default 5) are recorded and averaged for more stable behavioral clustering
- Tracks specialization metrics during training (when training from scratch) to produce time-series plots
- Generates 5 PNG figures: PCA clusters, t-SNE clusters, weight divergence over time, field usage by cluster, specialization score over time
- Markdown report includes tables for: specialization score breakdown, species characteristics, field usage per cluster, lineage correlations (when available), and ablation results (when not skipped)
- Supports `--skip-ablation` for faster report generation, `--output-dir` for custom output location
- Report includes interpretive text (conclusions from ablation comparison)
**Files changed:**
- `scripts/generate_specialization_report.py` (new) — full report generation script with 5 phases: load/train, analysis, ablation, visualization, report writing
**Validation:** mypy clean (32 files), 323/323 tests pass, script produces report with markdown + 5 PNG figures

---

### US-013: Update Training to Encourage Specialization ✓
**What:** Added `SpecializationConfig` with three config options that encourage specialization: `diversity_bonus`, `niche_pressure`, and `layer_mutation_rates`. Implemented reward modification in `train_step` and per-layer mutation in reproduction.
**Decisions:**
- Created `SpecializationConfig` dataclass with three fields: `diversity_bonus` (default 0.0), `niche_pressure` (default 0.0), `layer_mutation_rates` (default None)
- **Diversity bonus**: Computes per-agent cosine distance from other alive agents in weight space; adds scaled bonus to rewards before GAE. Rewards agents with more unique weights, creating gradient pressure to maintain diversity.
- **Niche pressure**: Computes minimum cosine distance to nearest neighbor; penalizes agents too similar to their closest neighbor. `penalty = niche_pressure * (1 - min_dist)`, encouraging weight differentiation.
- Both bonuses computed inside JIT-compiled `train_step` using pure JAX operations (no numpy/scipy). The `_compute_specialization_bonuses` function uses vectorized cosine distance with alive masking.
- Bonuses are vmapped over environments and added to every timestep's rewards (broadcast from per-env per-agent snapshot to `(T, num_envs, max_agents)`).
- **Per-layer mutation rates**: `layer_mutation_rates` maps layer name substrings (e.g., `"Dense_0"`, `"Dense_2"`) to custom mutation std values. Uses `jax.tree_util.tree_flatten_with_path` to match leaf paths.
- Added `compute_per_leaf_mutation_rates()` to compute per-leaf rate tuples, and `mutate_agent_params_layered()` to apply them during reproduction.
- Per-leaf rates are pre-computed at trace time in `env.py` (Python-level, not traced) and captured in the scan closure, so they work within JIT.
- All features are opt-in (defaults are 0/None), so existing behavior is unchanged when not configured.
- Added `specialization` field to `Config` master dataclass and `from_yaml()` support.
**Files changed:**
- `src/configs.py` — added `SpecializationConfig` dataclass, added `specialization` field to `Config`, updated `from_yaml()`
- `src/training/train.py` — added `_compute_specialization_bonuses()` helper, integrated reward modification in `train_step`, added specialization config display in training banner
- `src/agents/reproduction.py` — added `compute_per_leaf_mutation_rates()`, `mutate_agent_params_layered()`
- `src/environment/env.py` — imported new reproduction functions, added conditional layered mutation in reproduction scan
- `scripts/verify_specialization_training.py` (new) — verification script testing all specialization features
**Validation:** mypy clean (32 files), 323/323 tests pass, verification script confirms all configs run without errors and bonuses are computed correctly

---

### US-014: Integration Test ✓
**What:** Added `TestPhase3Integration` class with `test_specialization_emerges` to `tests/test_integration.py` — end-to-end test of the full Phase 3 specialization detection pipeline.
**Decisions:**
- Test follows the established `TestPhase2Integration` pattern: small config, train a few updates, then run the full analysis pipeline
- Pipeline tested end-to-end: create_train_state → train_step (multiple updates) → record_episode → extract_behavior_features → specialization_score → compute_weight_divergence → cluster_agents → find_optimal_clusters → SpecializationTracker
- Verifies per-agent params are initialized when evolution is enabled
- Verifies trajectory recording returns correct structure and shapes (actions, positions, rewards, alive_mask, energy, births, field_values)
- Verifies behavioral features have correct shape (max_agents, 7) and all finite values
- Verifies specialization score result has all expected keys and score in [0, 1]
- Verifies weight divergence returns non-negative finite values
- Verifies clustering and optimal cluster finding work on alive agent features
- Verifies SpecializationTracker produces correct metrics and summary after multiple updates
- Compares trained model's specialization score against a random baseline (uniform random features) — both must be well-formed [0, 1] values
- 180-second timeout (slightly longer than Phase 2 tests due to trajectory recording overhead)
**Files changed:**
- `tests/test_integration.py` — updated module docstring for Phase 3, added `TestPhase3Integration` class with `test_specialization_emerges` test
**Validation:** mypy clean (32 files), 324/324 tests pass (1 new test added)

---

### US-015: Final Review ✓
**What:** Code quality review, documentation update, and final validation across all Phase 3 files.
**Decisions:**
- All 324 tests pass, mypy reports no errors across 32 source files
- Code quality review: all Phase 3 files (specialization.py, trajectory.py, visualization.py, ablation.py, reproduction.py, configs.py, train.py, env.py) follow existing project patterns, use proper type annotations, and have comprehensive docstrings
- Updated README.md with Phase 3 section covering: specialization detection overview, Phase 3 commands, specialization results/expected output, specialization parameters table, updated project structure with new files, updated tech stack (scikit-learn, matplotlib)
- Example outputs documented in README: specialization score breakdown, ablation comparison table, report generator output description
**Files changed:**
- `README.md` — added Phase 3 section (specialization detection, commands, results, parameters), updated project structure, updated tech stack
- `progress.txt` — added US-015 entry and completion signal
**Validation:** mypy clean (32 files), 324/324 tests pass

---

--- PHASE 3 COMPLETE ---


================================================================================
PHASE 4: Research Microscope
================================================================================
Started: Mon Feb  2 15:53:46 IST 2026

Goals:
- Fix gradient homogenization (agent-specific heads, freeze-evolve)
- Build live visualization dashboard (Svelte + Pixi.js)
- Implement emergence metrics (Transfer Entropy, DOL, Phase Detection)
- Make it understandable to non-technical users (tooltips, glossary)

---

## Task Log

### US-001: Agent-Specific Policy Heads ✓
**What:** Implemented `AgentSpecificActorCritic` class in `src/agents/network.py` — a shared encoder + per-agent output heads architecture. Added `agent_architecture` config option to `AgentConfig`.
**Decisions:**
- Architecture: shared MLP encoder (same Dense + LayerNorm + tanh layers) processes observations into features, then N separate actor heads (Dense → num_actions logits) and N separate critic heads (Dense → scalar value) produce per-agent outputs
- All heads are evaluated in the forward pass (all Dense layers run), then the correct head's output is selected via `jnp.stack` + array indexing by `agent_id`. This is JIT-compatible and avoids `jax.lax.switch` + Flax module tracer leak issues
- `agent_id` parameter is optional — when `None`, defaults to head 0 for backward compatibility with `ActorCritic` API
- Out-of-range `agent_id` values are safely clamped to `[0, n_agents-1]`
- Gradients only flow through the selected head's parameters (confirmed by test), while shared encoder receives gradients from all agents
- Named layers: `actor_head_{i}` and `critic_head_{i}` for clear parameter tree inspection
- Added `agent_architecture: Literal["shared", "agent_heads"]` to `AgentConfig` (default: "shared" for backward compatibility)
- Initial approach using `jax.lax.switch` with `_AgentHead` submodules failed due to Flax nn.Module creating parameters inside `switch` branches causing `UnexpectedTracerError`. Switched to stacked-output-then-index approach.
**Files changed:**
- `src/agents/network.py` — added `AgentSpecificActorCritic` class (import `jax` added)
- `src/configs.py` — added `agent_architecture` field to `AgentConfig`
- `tests/test_agent.py` — added `TestAgentSpecificHeads` class with 10 tests covering: output shapes, different agents different outputs, shared encoder structure, agent_id=None backward compat, out-of-range clamping, JIT compatibility, vmap over agents, gradient flow isolation, config field, different hidden dims
**Validation:** mypy clean (0 new errors; 3 pre-existing in train.py), 334/334 tests pass (10 new)

