# Phase 5 Progress: Prove Emergence

## Session Start
Date: 2026-02-03
Previous Phases: Phase 1 (Digital Petri Dish), Phase 2 (Evolutionary Pressure), Phase 3 (Specialization Detection), Phase 4 (Research Microscope), Phase 4B (Kaggle Infrastructure) — all COMPLETE

## Phase 5 Goal
Prove with rigorous information-theoretic metrics and statistical significance that collective intelligence emerges from simple agents + shared field + evolution. Beat classical swarm AND modern MARL baselines. Publication-ready figures.

## Key Insights from Previous Phases
- Random field HURTS agents (585 < 600) — they learned to READ the field for information
- Specialization emerges with evolution: scouts, exploiters, balanced agents
- Weight divergence tracks with behavioral clustering
- Population reaches equilibrium (32/32 maxed): 108 births + 108 deaths = perfect turnover
- 9.8M steps trained on Kaggle (checkpoint available)

## Codebase Patterns
- Trackers follow pattern: __init__(), update(), get_metrics(), get_summary() — see TransferEntropyTracker
- All baselines return standardized result dict: {"total_reward", "food_collected", "final_population", "per_agent_rewards"}
- EnvState is flax.struct.dataclass — adding fields requires updating EVERY constructor call (state.py, env.py, ablation.py)
- Phase 5 deps go in [project.optional-dependencies] under "phase5" group with @pytest.mark.skipif guards
- Config uses dataclass_field(default_factory=...) for nested configs
- Field conditions in ablation.py: use FieldCondition Literal type
- Tests use jax.random.PRNGKey for deterministic testing

---

## Task Log

### US-001: Phase 5 Dependencies and Library Verification ✓
**Date:** 2026-02-03
**What:** Added dit, hoi, rliable to pyproject.toml [phase5] group. Created smoke tests.
**Decisions:**
- jaxmarl excluded from phase5 deps — pins jax<=0.4.38 and scipy<=1.12, incompatible with our jax 0.9 + scipy 1.17. We write custom MAPPO instead.
- dit requires numpy compat patch (np.alltrue removed in NumPy 2.0) — monkey-patched in test setup
- rliable.metrics works fine; rliable.library bootstrap has arch 8.0 compat issue (passes None as random_state). Metrics-only usage is sufficient for US-011.
- hoi requires float64 data and jax_enable_x64 for proper computation
- System deps needed: brew install cddlib gmp glpk suite-sparse (for dit's cvxopt/pycddlib)
**Files changed:**
- `pyproject.toml` — added phase5 optional-dependencies group
- `tests/test_phase5_deps.py` — NEW: 13 smoke tests (12 pass, 1 skip for jaxmarl)
**Verification:** mypy clean, ruff clean, 744 tests pass (1 skip)

### US-002: O-Information Metric via hoi ✓
**Date:** 2026-02-03
**What:** Implemented O-information (Omega = TC - DTC) metric using hoi library. Omega < 0 indicates synergy-dominant emergence.
**Decisions:**
- Using hoi's Gaussian Copula method ("gc") for fast continuous-data O-info estimation
- Returns 0.0 for edge cases: <3 agents (O-info undefined), <10 samples, constant features, NaN inputs
- Added noise injection (1e-8 scale) to avoid numerical issues with near-constant dimensions
- OInformationTracker follows same pattern as TransferEntropyTracker with z-score phase transition detection
- Tracks synergy_ratio: cumulative fraction of negative O-info values (emergence signal)
**Files changed:**
- `src/analysis/o_information.py` — NEW: compute_o_information(), compute_o_information_by_condition(), OInfoEvent, OInformationTracker
- `tests/test_o_information.py` — NEW: 27 tests covering all functions and edge cases
**Verification:** mypy clean, ruff clean, 27/27 tests pass

### US-003: Pairwise PID Synergy via dit ✓
**Date:** 2026-02-03
**What:** Implemented Partial Information Decomposition (PID) for measuring synergy between agent pairs using dit library.
**Decisions:**
- Using coinformation (not interaction_information) from dit for correct sign convention (negative = synergy, matching PRD)
- Jeffreys smoothing (alpha=0.5 pseudocounts) applied to joint distributions before PID computation
- K=2 quantile bins for continuous variables (field_summary, future_food)
- Special handling in discretize_continuous() for discrete data with <= num_bins unique values (direct mapping instead of quantile binning)
- PID decomposition using Williams-Beer (PID_WB) from dit
- Row shuffle surrogate test for statistical significance (shuffles y to break cross-variable coordination)
- Column shuffle variant uses circular shift to break temporal dependencies
**Files changed:**
- `src/analysis/pid_synergy.py` — NEW: compute_interaction_information(), compute_pairwise_pid(), compute_median_synergy(), surrogate_significance_test(), discretize_continuous(), apply_jeffreys_smoothing()
- `tests/test_pid_synergy.py` — NEW: 31 tests covering all functions, edge cases, XOR synergy, copy redundancy, surrogate testing
**Verification:** mypy clean, ruff clean, 31/31 tests pass

### US-004: Causal Emergence (Effective Information + Rosas Psi) ✓
**Date:** 2026-02-03
**What:** Implemented two complementary causal emergence measures: Hoel's Effective Information (EI) and Rosas' Psi. EI gap > 0 and Psi > 0 indicate causal emergence at the macro scale.
**Decisions:**
- Implemented custom TPM building with Laplace smoothing (no external dependencies beyond numpy/scipy)
- EI = log2(N) - avg_row_entropy(TPM), following Hoel et al. 2013
- Psi = I(V_t; V_{t+1}) - sum_i I(X_i,t; V_{t+1}), following Rosas et al. 2020
- Discretization using quantile bins (default 4 bins, configurable)
- Macro variable candidates: population_count, mean_field_intensity, total_food_collected, spatial_dispersion, field_entropy
- extract_macro_variables() extracts all macro vars from trajectory dict
- Windowed analysis: compute_windowed_causal_emergence() with configurable window size and overlap (default 50%)
- CausalEmergenceTracker follows same pattern as TransferEntropyTracker with z-score phase transition detection
- Tracks emergence_ratio: cumulative fraction of timesteps with both positive EI gap AND positive Psi
**Files changed:**
- `src/analysis/causal_emergence.py` — NEW: compute_effective_information(), compute_rosas_psi(), compute_causal_emergence_from_trajectory(), compute_windowed_causal_emergence(), extract_macro_variables(), discretize_to_bins(), build_tpm(), compute_tpm_entropy(), compute_mutual_information_discrete(), CausalEmergenceEvent, CausalEmergenceTracker
- `tests/test_causal_emergence.py` — NEW: 54 tests covering all functions, edge cases, TPM building, MI computation, trajectory extraction, windowed analysis, tracker pattern
**Verification:** mypy clean, ruff clean, 54/54 tests pass, 856 total tests pass

### US-005: Surrogate Testing Framework ✓
**Date:** 2026-02-03
**What:** Created a reusable surrogate testing framework for statistical significance of all emergence metrics. Implements multiple shuffle methods and statistical tests.
**Decisions:**
- row_shuffle: breaks cross-agent coordination by shuffling agent dimension per timestep
- column_shuffle: breaks temporal dependencies by permuting time axis
- block_shuffle: preserves short-range structure, breaks long-range (shuffles blocks of timesteps)
- bootstrap_ci: implemented BCa (bias-corrected and accelerated) method for accurate confidence intervals
- mann_whitney_u: wrapper returning U-statistic, p-value, and rank-biserial effect size
- wilcoxon_signed_rank: wrapper for paired samples with r effect size (Z/sqrt(N))
- surrogate_test: generic function that takes any metric_fn and shuffle_fn, returns SurrogateResult
- Renamed TestResult to StatisticalTestResult to avoid pytest collection warning
- Added effect size interpretation functions (Cohen's d, Glass's delta, r)
- Added compare_conditions() for pairwise comparisons across multiple conditions
- Uses only numpy/scipy (no Phase 5 deps needed) — framework is universally usable
**Files changed:**
- `src/analysis/surrogates.py` — NEW: row_shuffle(), column_shuffle(), block_shuffle(), bootstrap_ci(), mann_whitney_u(), wilcoxon_signed_rank(), surrogate_test(), compute_cohens_d(), compute_glass_delta(), interpret_effect_size(), compare_conditions(), StatisticalTestResult, SurrogateResult
- `tests/test_surrogates.py` — NEW: 50 tests covering all functions, edge cases, integration tests
**Verification:** mypy clean, ruff clean, 50/50 tests pass

### US-006: IPPO Baseline (No Field, Shared Params) ✓
**Date:** 2026-02-03
**What:** Implemented IPPO baseline — the simplest baseline with no field communication and no evolution. This is the "no communication at all" lower bound for baseline comparisons.
**Decisions:**
- Field disabled via: write_strength=0.0, decay_rate=1.0 (field zeros out each step)
- Evolution disabled via: enabled=False, infinite starting_energy (1000000), energy_per_step=0, impossible reproduce_threshold
- max_agents set equal to num_agents (no population growth or death)
- Reuses existing ActorCritic and step() — zero new neural network code (as required by PRD)
- Returns standardized result dict: {"total_reward", "food_collected", "final_population", "per_agent_rewards"}
- evaluate_ippo() aggregates results across multiple episodes with mean/std/per-episode tracking
- Supports both stochastic (sampling) and deterministic (greedy) action selection
**Files changed:**
- `src/baselines/__init__.py` — NEW: Module docstring with baseline descriptions
- `src/baselines/ippo.py` — NEW: ippo_config(), run_ippo_episode(), evaluate_ippo(), create_ippo_network(), init_ippo_params()
- `tests/test_baselines.py` — NEW: 17 tests in TestIPPO and TestIPPOImportability classes
**Verification:** mypy clean, ruff clean, 17/17 IPPO tests pass, 923 total tests pass (1 skip)

### US-007: ACO-Fixed Baseline (+ ACO-Hybrid Variant) ✓
**Date:** 2026-02-03
**What:** Implemented two ACO baselines for comparison — ACO-Fixed (no neural network, pure hardcoded pheromone rules) and ACO-Hybrid (NN for movement, hardcoded field writes). This isolates the value of LEARNING the write behavior vs hardcoded ACO rules.
**Decisions:**
- ACO parameters follow Dorigo & Stutzle (2004): alpha=1.0 (pheromone importance), beta=2.0 (heuristic importance), rho=0.5 (evaporation = decay_rate), Q=1.0 (deposit quantity = write_strength)
- ACO-Fixed movement: p_ij = (tau_ij^alpha * eta_ij^beta) / sum (classic ACO formula)
- Food heuristic (eta): 1/(1 + manhattan_distance_to_nearest_food)
- Pheromone reading (tau): sum of field channels at each neighboring cell
- Extra pheromone deposit (ACO_Q * 2.0) when agent collects food — reinforces successful paths
- ACO-Hybrid: uses neural network for movement decisions, but field writes follow hardcoded ACO rules
- Both return standardized result dict matching IPPO format
- Evolution disabled in both variants (population constant)
- Uses existing field infrastructure (same diffusion, decay dynamics)
**Files changed:**
- `src/baselines/aco_fixed.py` — NEW: aco_config(), run_aco_fixed_episode(), run_aco_hybrid_episode(), evaluate_aco_fixed(), evaluate_aco_hybrid(), create_aco_hybrid_network(), init_aco_hybrid_params(), internal functions for movement probabilities and food heuristics
- `tests/test_baselines.py` — EXTENDED: 17 tests in TestACO + 2 tests in TestACOImportability
**Verification:** mypy clean, ruff clean, 19/19 ACO tests pass, 942 total tests pass (1 skip)

### US-008: MAPPO Baseline (Centralized Critic) ✓
**Date:** 2026-02-03
**What:** Implemented MAPPO baseline with centralized critic and decentralized actors. This is CTDE (Centralized Training, Decentralized Execution) — the actor sees only local observations while the critic sees all agents' observations concatenated.
**Decisions:**
- CentralizedCritic: Flax module that takes concatenated observations of all agents, outputs per-agent value estimates
- Hidden dims are 2x the actor's hidden dims for increased capacity (handles n_agents * obs_dim inputs)
- Field disabled via: write_strength=0.0, decay_rate=1.0 (no field communication — MAPPO handles coordination via centralized critic)
- Evolution disabled: population stays constant (no births/deaths)
- ~200 lines of code as specified in PRD (actually 220 lines)
- RunningMeanStd class for value normalization (running mean/std of returns)
- mappo_loss() uses vmap for vectorized forward pass over batch
- Death masking via alive_mask in loss computation
- create_mappo_train_state() sets up actor/critic optimizers with gradient clipping
**Files changed:**
- `src/baselines/mappo.py` — NEW: CentralizedCritic, RunningMeanStd, mappo_config(), create_mappo_network(), create_centralized_critic(), init_mappo_params(), mappo_loss(), run_mappo_episode(), evaluate_mappo(), create_mappo_train_state()
- `tests/test_baselines.py` — EXTENDED: 19 tests in TestMAPPO + 4 tests in TestMAPPOImportability
**Verification:** mypy clean, ruff clean, 23/23 MAPPO tests pass, 59/59 total baseline tests pass

### US-009: Experiment Harness and Multi-Seed Runner ✓
**Date:** 2026-02-03
**What:** Created a generic experiment harness for running methods across N seeds with paired seed support and standardized results compatible with rliable analysis.
**Decisions:**
- ExperimentConfig dataclass: method_name, n_seeds (default 20 per DR-4), env_config_name, paired_seeds (default True), seed_offset, n_episodes, save_per_seed_results
- ExperimentResult dataclass: stores per-seed results (rewards, food, population) and aggregate statistics (mean, std, median, IQM, bootstrap CI)
- run_experiment() takes a method_fn(seed) -> dict and runs it for each seed, aggregating results
- run_paired_experiment() runs multiple methods with same seed sequence for paired statistical tests (e.g., Wilcoxon signed-rank)
- compare_experiment_results() ranks methods by IQM and computes pairwise differences
- Save/load via pickle with auto-creation of parent directories
- compute_iqm() returns interquartile mean (25th-75th percentile) — robust to outliers
- bootstrap_ci_simple() returns 95% CI via percentile method (10000 bootstrap samples)
- Three environment configs: standard_config(), hidden_resources_config() (more agents, longer episodes for coordination), food_scarcity_config() (num_food=5, smaller grid, higher food value)
- get_env_config(name) utility for CLI-friendly config selection
**Files changed:**
- `src/experiments/__init__.py` — NEW: Module docstring
- `src/experiments/runner.py` — NEW: ExperimentConfig, ExperimentResult, run_experiment(), run_paired_experiment(), compare_experiment_results(), save_experiment_result(), load_experiment_result(), compute_iqm(), bootstrap_ci_simple()
- `src/experiments/configs.py` — NEW: standard_config(), hidden_resources_config(), food_scarcity_config(), get_env_config(), list_env_configs()
- `tests/test_experiments.py` — NEW: 51 tests covering all dataclasses, functions, configs, and edge cases
**Verification:** mypy clean, ruff clean, 51/51 tests pass, 1016 total tests pass (1 skip)

### US-010: Hidden Resources Environment Modification ✓
**Date:** 2026-02-03
**What:** Added hidden food mechanics — high-value food items that require K agents within distance D to reveal. Creates tasks that REQUIRE coordination; individuals cannot solve alone.
**Decisions:**
- HiddenFoodConfig nested dataclass with: enabled=False (backward compat), num_hidden=3, required_agents=3, reveal_distance=3, reveal_duration=10, hidden_food_value_multiplier=5.0
- Parameters based on Level-Based Foraging (LBF) benchmark (Papoudakis et al. 2021)
- Four new fields in EnvState: hidden_food_positions, hidden_food_revealed, hidden_food_reveal_timer, hidden_food_collected (all None when disabled)
- Reveal logic: count alive agents within Chebyshev distance D of each hidden food; if >= K, set revealed=True and start timer
- Collection logic: revealed food within distance 1 can be collected by closest agent; gives food_energy * value_multiplier (5x default)
- Timer mechanics: counts down when food is revealed but agents leave; when timer reaches 0, food re-hides and respawns at new position
- Respawn on collection: when collected, hidden food immediately respawns at new random position
- Step numbering updated: now 11 stages (was 10) with hidden food processing after normal food collection
- Info dict extended: "hidden_food_collected_this_step" always present (0.0 when disabled)
- All EnvState constructors updated: state.py (create_env_state), env.py (reset, step), ablation.py (_replace_field, _reset_energy, _replace_agent_params)
**Files changed:**
- `src/configs.py` — NEW: HiddenFoodConfig dataclass; EnvConfig now has hidden_food field
- `src/environment/state.py` — EnvState extended with 4 hidden food fields; create_env_state() initializes them
- `src/environment/env.py` — reset() initializes hidden food; step() adds stage 4 (hidden food reveal/collection)
- `src/analysis/ablation.py` — All 3 EnvState constructor calls updated with hidden food fields
- `tests/test_hidden_food.py` — NEW: 18 tests covering config, disabled mode, enabled mode, reveal mechanics, collection, respawn, backward compatibility, integration with evolution/field
**Verification:** mypy clean, ruff clean, 18/18 hidden food tests pass, 28/28 env tests pass (no regressions), 1034 total tests pass (1 skip)

### US-011: Statistical Reporting Module (rliable Integration) ✓
**Date:** 2026-02-03
**What:** Created a comprehensive statistical reporting module using rliable for IQM + bootstrap CI, performance profiles, and probability of improvement. Wraps scipy stats tests for convenience.
**Decisions:**
- IQM uses rliable.metrics.aggregate_iqm when available, falls back to simple implementation otherwise
- Bootstrap CI uses our own implementation (rliable.library has arch 8.0 compat issues) with percentile method
- Performance profiles compute CDFs of normalized scores, returns AUC per method
- Probability of improvement uses rliable when available, falls back to pairwise comparison U-statistic
- HypothesisTestResult dataclass (renamed from TestResult to avoid pytest collection warning)
- StatisticalReport dataclass: method_name, iqm, ci_lower, ci_upper, median, mean, std, n_seeds, scores
- mann_whitney_test(): wrapper returning rank-biserial effect size
- wilcoxon_test(): wrapper for paired samples with r = Z/sqrt(N) effect size
- welch_t_test(): wrapper returning Cohen's d effect size
- compare_methods(): comprehensive method comparison with rankings, pairwise tests, POI
- MethodComparison dataclass aggregates all comparison results
- All functions work without rliable installed (graceful fallback)
**Files changed:**
- `src/analysis/statistics.py` — NEW: IQMResult, StatisticalReport, HypothesisTestResult, MethodComparison, compute_iqm(), performance_profiles(), probability_of_improvement(), mann_whitney_test(), wilcoxon_test(), welch_t_test(), create_statistical_report(), compare_methods()
- `tests/test_statistics.py` — NEW: 68 tests covering all dataclasses, functions, rliable integration, fallback implementations, edge cases
**Verification:** mypy clean, ruff clean, 68/68 tests pass

### US-012: Stigmergy Ablation at Scale (Launch Script) ✓
**Date:** 2026-02-03
**What:** Extended ablation module with 3 new field conditions and created launch script for running all 6 conditions at scale with 20 seeds across 3 environment configs.
**Decisions:**
- Extended FieldCondition Literal type to ExtendedFieldCondition with all 6 conditions
- ALL_FIELD_CONDITIONS list for easy iteration over all conditions
- `frozen`: saves initial field state (or from checkpoint) and restores it after each step, preventing any new writes
- `no_field`: zeros out field portion of observations via `_zero_field_obs()` helper
- `write_only`: agents write to field normally but read zeros (field obs zeroed)
- New `_run_extended_episode_full()` handles all 6 conditions with optional frozen_field parameter
- New `ExtendedAblationResult` dataclass with same structure as AblationResult
- New `extended_ablation_test()` runs any subset of the 6 conditions
- Launch script supports dry-run, condition filtering, env config filtering, and checkpoint milestones
- Script aggregates per-seed results into final statistics (mean, std)
- Output saved to timestamped directory with pickle files per env config
**Files changed:**
- `src/analysis/ablation.py` — EXTENDED: ExtendedFieldCondition type, ALL_FIELD_CONDITIONS list, _zero_field_obs(), _run_extended_episode_full(), ExtendedAblationResult, extended_ablation_test(), print_extended_ablation_results()
- `scripts/run_stigmergy_ablation.py` — NEW: CLI for running 6 conditions × 20 seeds × 3 envs with dry-run support
- `tests/test_ablation_extended.py` — NEW: 25 tests covering all new conditions, helper functions, and integration
**Verification:** mypy clean, ruff clean, 25/25 ablation extended tests pass, 1127/1127 total tests pass

### US-013: Superlinear Scaling Experiment (Launch Script) ✓
**Date:** 2026-02-03
**What:** Created scaling analysis module and launch script for testing N = 1, 2, 4, 8, 16, 32 agents with 3 field conditions and 20 seeds to analyze superlinear scaling benefits from field-mediated coordination.
**Decisions:**
- ScalingResult dataclass: n_agents, field_condition, total_food, per_agent_food, efficiency, total_reward, episode_foods
- ScalingAnalysis dataclass: aggregates results with power law fit (alpha, CI, R²) and per-N statistics
- compute_per_agent_efficiency(): F_total(N) / (N * F_solo), values > 1.0 indicate coordination benefit
- fit_power_law(): log(F_total) = alpha * log(N) + c via scipy.stats.linregress
  - alpha > 1.0 => superlinear (field helps more with more agents)
  - alpha = 1.0 => linear (no coordination benefit)
  - alpha < 1.0 => sublinear (crowding hurts)
- Bootstrap CI for alpha with edge case handling (skip failed regressions when all x values identical)
- aggregate_scaling_results(): groups by n_agents, computes solo baseline, fits power law
- compare_scaling_analyses(): ranks conditions by alpha, identifies superlinear conditions
- Launch script: 6 N values × 3 conditions × 20 seeds × 10 episodes = 360 experiments per full run
- Dry-run support, condition/N filtering, checkpoint loading
- Reference: Hamann (2018), "Swarm Robotics: A Formal Approach"
**Files changed:**
- `src/analysis/scaling.py` — NEW: ScalingResult, ScalingAnalysis, compute_per_agent_efficiency(), fit_power_law(), compute_r_squared(), aggregate_scaling_results(), compare_scaling_analyses(), print_scaling_analysis(), print_scaling_comparison()
- `scripts/run_scaling_experiment.py` — NEW: CLI for running scaling experiments with --dry-run, --conditions, --n-agents, --n-seeds, --n-episodes flags
- `tests/test_scaling.py` — NEW: 46 tests covering all dataclasses, functions, edge cases (bootstrap with small datasets, two-point fits), print functions
**Verification:** mypy clean, ruff clean, 46/46 tests pass, 1173/1173 total tests pass

