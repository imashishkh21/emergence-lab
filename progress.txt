# Ralph Progress Log

## Completed Tasks

### US-001: Create project structure [x]
- Created src/ folder with submodules: environment, agents, training, field, analysis
- All __init__.py files in place
- `python -c "import src"` works

### US-003: Create base config dataclasses [x]
- Created src/configs.py with all config dataclasses
- EnvConfig, FieldConfig, AgentConfig, TrainConfig, LogConfig, AnalysisConfig, Config
- Config.from_yaml() and to_yaml() implemented
- Typecheck passes

### US-002: Set up dependencies and virtual environment [x]
- Verified scripts/setup.sh creates venv, installs deps, verifies JAX
- Fixed mypy errors in src/configs.py: renamed `field` import to `dataclass_field` to avoid name collision with Config.field attribute
- Added types-PyYAML to dev dependencies in pyproject.toml
- Fixed `to_dict` type annotation and AgentConfig construction for mypy compliance
- All acceptance criteria pass: setup.sh runs clean, pip install -e . works, jax.devices() returns [CpuDevice(id=0)], mypy passes

### US-004: Implement FieldState dataclass [x]
- Created src/field/field.py with FieldState as @flax.struct.dataclass
- FieldState contains `values: jnp.ndarray` with shape (H, W, C)
- Implemented `create_field(height, width, channels)` factory function returning zero-initialized FieldState
- All TestFieldState tests pass (2/2)
- Typecheck passes

### US-005: Implement field dynamics (diffusion + decay) [x]
- Created src/field/dynamics.py with three functions: diffuse(), decay(), step_field()
- diffuse() uses 3x3 Gaussian kernel (1,2,1/2,4,2/1,2,1 normalized to sum=1), edge-padded, blended by rate
- decay() multiplies field values by (1 - rate)
- step_field() chains diffuse then decay
- All functions JIT-compatible (static loop over 3x3 kernel unrolled at trace time)
- Used FieldState constructor instead of .replace() to satisfy mypy
- All 5 TestFieldDynamics tests pass
- Typecheck passes

### US-006: Implement field read/write operations [x]
- Created src/field/ops.py with two functions: write_local() and read_local()
- write_local(field, positions, values): uses jnp.ndarray.at[].add() to write values at agent (row, col) positions; shape (N, 2) positions, (N, C) values
- read_local(field, positions, radius): reads local field values; radius=0 returns (N, C) at exact positions, radius>0 returns flattened (2r+1)x(2r+1) neighborhood as (N, (2r+1)^2 * C)
- Out-of-bounds reads are clamped to field edges via jnp.clip
- Both functions are JIT-compatible (no Python control flow for radius>0 case; radius is a static int)
- All 4 TestFieldOps tests pass, all 11 field tests pass
- Typecheck passes

### US-007: Implement EnvState dataclass [x]
- Created src/environment/state.py with EnvState as @flax.struct.dataclass
- EnvState fields: agent_positions (num_agents, 2), food_positions (num_food, 2), food_collected (num_food,) bool, field_state (FieldState), step (scalar int), key (PRNG key)
- Implemented create_env_state(key, config) factory function that randomly places agents and food on the grid, initializes field to zeros
- All TestEnvState tests pass (1/1)
- Typecheck passes

### US-008: Implement environment reset [x]
- Created src/environment/env.py with `reset(key, config) -> EnvState`
- Used `jax.random.permutation` over flattened grid indices to guarantee non-overlapping agent positions
- Food positions are random via `jax.random.randint` (may overlap each other, which is fine)
- Field initialized to zeros via `create_field()`
- Added 2 extra tests: `test_reset_field_initialized_fresh` and `test_reset_food_not_collected`
- All 5 TestEnvReset tests pass, typecheck passes
- Files changed: src/environment/env.py (new), tests/test_env.py (added tests)

### US-009: Implement environment step [x]
- Implemented `step(state, actions, config) -> (EnvState, rewards, dones, info)` in src/environment/env.py
- Actions map: 0=stay, 1=up(-row), 2=down(+row), 3=left(-col), 4=right(+col) using action_deltas lookup table
- Agent positions clipped to grid boundaries after movement
- Food collection uses Chebyshev distance <= 1 (within 1 cell in any direction)
- Reward: shared +1 per food collected this step, broadcast to all agents
- Field dynamics: step_field (diffuse + decay), then agents write presence (write_strength * ones for all channels)
- Done when step >= max_steps
- Info dict includes food_collected_this_step and total_food_collected
- All 3 TestEnvStep tests pass, mypy clean
- Files changed: src/environment/env.py (added step function + imports for step_field, write_local)

### US-010: Implement observation function [x]
- Created src/environment/obs.py with get_observations() and obs_dim() functions
- Observation components per agent:
  1. Own position (2 values): normalized to [-1, 1] via (pos / (grid_size-1)) * 2 - 1
  2. Local field values ((2*radius+1)^2 * channels values): read via read_local(), clipped to [-1, 1]
  3. K nearest uncollected food (K*3 = 15 values): relative (dx, dy) normalized by grid_size + availability flag
- K_NEAREST_FOOD = 5 (hardcoded constant)
- Food outside observation_radius or already collected is masked out
- Uses Chebyshev distance to determine food visibility range
- Total obs_dim with defaults: 2 + 484 + 15 = 501
- All TestObservations tests pass (2/2), mypy clean
- Files changed: src/environment/obs.py (new)

### US-011: Vectorize environment with vmap [x]
- Created src/environment/vec_env.py with VecEnv class
- VecEnv.__init__ takes Config, creates vmapped versions of reset() and step()
- reset(key) splits the key into num_envs sub-keys, vmaps over them
- step(states, actions) vmaps over (state, actions) pairs, config is shared
- Uses jax.vmap lambdas that close over self.config so config is not mapped
- Batch shapes: (num_envs, num_agents, ...) for positions, rewards, etc.
- All 3 TestVecEnv tests pass (reset, step, JIT-compatible)
- Typecheck passes (mypy clean)
- Files changed: src/environment/vec_env.py (new)

### US-REVIEW-01: Review Environment Epic [x]
- All 25 environment tests pass (14 test_env.py + 11 test_field.py)
- Mypy typecheck clean: no issues found in 15 source files
- Manual verification:
  - Created env with default config (8 agents, 20x20 grid, 10 food, 4 field channels)
  - Reset produces correct shapes: agent_positions (8,2), food_positions (10,2), field (20,20,4), obs (8,501)
  - Stepped 10 times: step counter increments, rewards fire on food collection, done=False before max_steps
- Field verification:
  - Initial field sum = 0.0 (correct)
  - After 1 step: field sum = 32.0, 32 nonzero cells (8 agents × 4 channels = 32 writes)
  - After 6 steps: field sum = 169.54, 572 nonzero cells (diffusion spreading values)
  - Field values show clear gradient around agent positions (diffusion working)
- Food collection verification:
  - 8 out of 10 food collected in 50 random-action steps
  - food_collected mask correctly tracks which food items are consumed
  - Rewards are shared across all agents (+1 per food item)
- No issues found. Environment epic is solid and ready for training integration.

### US-012: Implement actor-critic network [x]
- Created src/agents/network.py with ActorCritic as a Flax nn.Module
- Architecture: shared MLP backbone -> actor head (logits) + critic head (scalar value)
- Backbone: Dense -> LayerNorm -> Tanh for each hidden layer, configurable hidden_dims
- Actor head: Dense(num_actions) with orthogonal init scale 0.01 (small logits initially)
- Critic head: Dense(1) with orthogonal init scale 1.0, squeezed to scalar
- Hidden layers use orthogonal init with scale sqrt(2) (standard for ReLU/Tanh networks)
- All bias initializers are zeros
- Works with single obs (obs_dim,) and batched via vmap (batch, obs_dim)
- All 4 TestNetwork tests pass, mypy clean (16 source files, no issues)
- Files changed: src/agents/network.py (new)

### US-013: Implement action sampling [x]
- Created src/agents/policy.py with two functions: sample_actions() and get_deterministic_actions()
- sample_actions(network, params, obs, key): flattens (num_envs, num_agents, obs_dim) into batch, vmaps forward pass with shared params, samples from categorical distribution, returns (actions, log_probs, values, entropy) all reshaped to (num_envs, num_agents)
- get_deterministic_actions(network, params, obs): same forward pass but uses argmax instead of sampling
- Used jnp.asarray() on values output to satisfy mypy (flax apply return type is a union)
- All 4 TestActionSampling tests pass, all 8 agent tests pass
- Mypy clean (17 source files, no issues)
- Files changed: src/agents/policy.py (new)

### US-014: Implement GAE calculation [x]
- Created src/training/gae.py with compute_gae() function
- Implements Generalized Advantage Estimation using jax.lax.scan with reverse=True
- TD residuals: delta_t = r_t + gamma * V(s_{t+1}) * (1 - done_t) - V(s_t)
- Reverse scan accumulates: gae_t = delta_t + gamma * lambda * (1 - done_t) * gae_{t+1}
- Returns (advantages, returns) where returns = advantages + values[:T]
- Handles episode boundaries by zeroing out future contributions on done flags
- JIT-compatible (uses jax.lax.scan, no Python control flow)
- All 4 TestGAE tests pass, mypy clean (18 source files, no issues)
- Files changed: src/training/gae.py (new)

### US-015: Implement PPO loss function [x]
- Created src/training/ppo.py with ppo_loss() function
- Implements standard PPO clipped surrogate objective:
  - Forward pass through ActorCritic network to get logits and values
  - Computes new log probabilities from categorical distribution over logits
  - Policy loss: clipped surrogate objective using min(ratio * adv, clip(ratio) * adv)
  - Value loss: MSE between predicted values and target returns
  - Entropy bonus: -sum(p * log(p)) averaged over batch for exploration
  - Total loss: policy_loss + vf_coef * value_loss - ent_coef * entropy
- Metrics returned: policy_loss, value_loss, entropy, approx_kl, clip_fraction
- Used jnp.asarray() on network.apply output to satisfy mypy (Flax union return type)
- All 3 TestPPOLoss tests pass, mypy clean (19 source files, no issues)
- Files changed: src/training/ppo.py (new)

### US-016: Implement rollout collection [x]
- Created src/training/rollout.py with RunnerState and collect_rollout
- RunnerState is a flax.struct.dataclass holding: params, opt_state, env_state, last_obs, key
- collect_rollout uses jax.lax.scan over num_steps for efficient trajectory collection
- Each step: sample_actions → vec_env.step → get_observations → store transition
- Batch output contains: obs, actions, rewards, dones, values, log_probs
- Shapes: (num_steps, num_envs, num_agents, ...) for all batch entries
- collect_rollout takes (runner_state, network, vec_env, config) — config provides num_steps
- Fixed mypy error: scan carry type annotation was tuple[RunnerState] instead of RunnerState
- All 2 TestRollout tests pass, mypy clean (20 source files, no issues)
- Files changed: src/training/rollout.py (new)

### US-017: Implement training step [x]
- Created src/training/train.py with create_train_state() and train_step() functions
- create_train_state(config, key): initializes network, optimizer (adam + grad clipping), vectorized env, and returns RunnerState
- train_step(runner_state, config): full PPO iteration:
  1. Collects rollout via collect_rollout (num_steps transitions)
  2. Computes bootstrap value from last observation
  3. Computes GAE advantages/returns via vmap over env*agent pairs
  4. Flattens rollout into training batch (num_steps * num_envs * num_agents samples)
  5. Runs num_epochs of minibatch PPO updates using jax.lax.scan
  6. Each minibatch: shuffles, normalizes advantages, computes PPO loss + gradients, applies optimizer update
- Gradient clipping via optax.clip_by_global_norm(max_grad_norm)
- Advantage normalization per minibatch: (adv - mean) / (std + 1e-8)
- Metrics returned: total_loss, policy_loss, value_loss, entropy, approx_kl, clip_fraction, mean_reward, mean_value, mean_advantage, mean_return
- All 1 TestTrainStep test passes, all 10 training tests pass
- Mypy clean (21 source files, no issues)
- Files changed: src/training/train.py (new)

### US-018: Implement full training loop [x]
- Added `train(config)` function to src/training/train.py as the main training entry point
- Initializes: env, network, optimizer, runner_state via `create_train_state()`
- JIT-compiles `train_step` by wrapping in a closure that captures `config` (avoids hashability issues with static args)
- Loops for `total_steps` with tqdm progress bar (stderr)
- Logs metrics (reward, loss, entropy, value) at `log_interval` step intervals
- Prints training summary with final metrics at completion
- NaN/Inf detection halts training early with warning
- Added `main()` CLI entry point using `tyro.cli(Config)` for argument parsing
- `python -m src.training.train --train.total-steps 10000` runs successfully
- Warm-up JIT compilation with first iteration, then efficient loop
- All 10 training tests pass, mypy clean (21 source files, no issues)
- Files changed: src/training/train.py (added train(), main(), if __name__ block)

### US-REVIEW-02: Review Training Epic [x]
- All 18 training tests pass (8 test_agent.py + 10 test_training.py)
- Mypy typecheck clean: no issues found in 21 source files
- Training stability verification:
  - Default config (32 envs, 128 steps, 8 agents): 1 iteration of 32768 env steps completes with no NaN/Inf
  - Final metrics healthy: loss=0.185, entropy=1.605, approx_kl=0.005, clip_fraction=0.022
  - Mean reward=0.062 (agents collecting some food even with random policy)
- Multi-iteration dynamics verification (small config: 4 envs, 32 steps, 4 agents):
  - Ran 50 iterations (512 steps each) with zero NaN/Inf
  - Loss trend: decreasing (0.024 → -0.016) — optimizer is working
  - Entropy trend: slowly decreasing (1.609 → 1.575) — policy is specializing
  - Reward is near 0 as expected early in training (small grid, few iterations)
- No issues found. Training epic is solid and ready for visualization/analysis integration.

### US-019: Implement W&B logging [x]
- Created src/utils/logging.py with four public functions: init_wandb, log_metrics, log_video, finish_wandb
- init_wandb(config): creates a W&B run under config.log.project, logs all hyperparameters as flat dict
- log_metrics(metrics, step): converts JAX arrays to Python floats and logs to W&B
- log_video(frames, name, step): stacks RGB frames into (T,C,H,W) array and logs as wandb.Video
- finish_wandb(): closes the current W&B run
- Integrated W&B logging into src/training/train.py:
  - init_wandb called at training start when config.log.wandb=True
  - log_metrics called at each log_interval
  - finish_wandb called at training end
- Used lazy `import wandb` inside each function to avoid import overhead when wandb disabled
- Used dataclasses.asdict(config) for mypy-clean config flattening
- Training with --log.no-wandb runs without error (10k steps test)
- All 18 training+agent tests pass, mypy clean (22 source files)
- Files changed: src/utils/logging.py (new), src/training/train.py (added W&B integration)

### US-020: Implement environment rendering [x]
- Created src/environment/render.py with render_frame() function and _draw_circle() helper
- render_frame(state, config, pixel_size=0) returns uint8 RGB numpy array
- Visualization layers (drawn in order):
  1. Field heatmap overlay: sums field across channels, normalizes, maps to blue-to-red colormap with alpha blending
  2. Grid lines: light gray lines at cell boundaries
  3. Food: green filled circles at uncollected food positions (collected food skipped)
  4. Agents: colored filled circles using 16 distinct colors (cycles for >16 agents)
- Auto-computes pixel_size to ensure >= 400x400 output (default 20x20 grid → 20px/cell → 400x400)
- Added 3 tests to tests/test_env.py::TestRender:
  - test_render_frame_shape_and_type: checks dtype, shape, minimum size
  - test_render_frame_with_field: renders after 5 steps to verify field overlay works
  - test_render_frame_save_png: saves to PNG via imageio, reads back and verifies
- All 17 env tests pass, mypy clean (23 source files)
- Files changed: src/environment/render.py (new), tests/test_env.py (added TestRender class)

### US-021: Implement episode video recording [x]
- Created src/utils/video.py with two public functions: record_episode and save_video
- record_episode(network, params, config, key): runs one full episode using deterministic (greedy) actions,
  renders each timestep via render_frame(), returns list of uint8 RGB numpy arrays
- save_video(frames, path, fps=30): saves frames as MP4 using imageio + ffmpeg (libx264 codec)
- Uses get_deterministic_actions for evaluation (argmax policy, no sampling)
- Adds batch dim (1, num_agents, obs_dim) for policy compatibility, removes it for env step
- Tested end-to-end: 21 frames (400x400 RGB) recorded for 20-step episode, saved as 63KB MP4
- Mypy clean (24 source files, no issues)
- Files changed: src/utils/video.py (new)

### US-022: Implement field analysis metrics [x]
- Created src/analysis/field_metrics.py with three public functions
- field_entropy(field): computes spatial entropy by treating absolute field values as a probability distribution
  over spatial locations per channel, then averaging entropy across channels. High entropy = uniform, low = concentrated.
- field_structure(field): measures spatial autocorrelation via Pearson correlation between each cell and the mean
  of its 4 cardinal neighbors (using jnp.roll). Averaged across channels, clamped to [0, 1]. Smooth fields > random.
- field_food_mi(field, food_positions): estimates mutual information between discretized field values and a binary
  food presence indicator. Uses 10-bin discretization, computes MI = H(field) + H(food) - H(joint). Non-negative.
- All functions JIT-compatible (field_entropy and field_structure directly; field_food_mi uses Python loops over
  fixed bin count which unrolls at trace time)
- All 4 TestFieldMetrics tests pass, mypy clean (25 source files)
- Files changed: src/analysis/field_metrics.py (new)

### US-023: Implement ablation test [x]
- Created src/analysis/ablation.py with ablation_test(), print_ablation_results(), and CLI main()
- ablation_test(network, params, config, num_episodes, seed) runs episodes under 3 field conditions:
  1. Normal: field operates as trained (diffusion + decay + agent writes)
  2. Zeroed: field values replaced with zeros before each observation (agents see no field info)
  3. Random: field values replaced with uniform random noise before each observation
- Returns dict mapping condition name to AblationResult dataclass (mean_reward, std_reward, episode_rewards)
- _run_episode() runs a single episode with deterministic policy, applying field ablation before get_observations()
- _replace_field() constructs new EnvState with replaced field_state (mypy-compatible, no .replace())
- CLI entry point: python -m src.analysis.ablation --checkpoint=path [--config=yaml] [--num-episodes=20] [--seed=0]
  - Loads pickled params from checkpoint file
  - Optionally loads Config from YAML (defaults otherwise)
  - Creates ActorCritic network matching config, verifies params with dummy forward pass
- print_ablation_results() shows table + gap analysis (normal vs zeroed, normal vs random)
- End-to-end verification: random policy shows normal=17.3, zeroed=10.7, random=16.0 (field helps even untrained)
- TestAblation::test_ablation_interface passes, mypy clean (26 source files)
- Files changed: src/analysis/ablation.py (new)

### US-024: Implement emergence detection [x]
- Created src/analysis/emergence.py with EmergenceTracker class, EmergenceEvent and MetricHistory dataclasses
- EmergenceTracker tracks field_entropy and field_structure metrics over training via rolling window
- Phase transition detection uses z-score method: flags when a new metric value exceeds z_threshold (default 3.0)
  standard deviations from the rolling window mean (default window_size=20)
- EmergenceEvent records step, metric_name, old_value, new_value, z_score for each detected transition
- get_metrics() returns dict with 'emergence/entropy', 'emergence/structure', 'emergence/num_events' for W&B logging
- get_summary() returns overall statistics and event list for end-of-training reporting
- Integrated into training loop (src/training/train.py):
  - EmergenceTracker initialized before training loop
  - Updated at analysis.emergence_check_interval steps using first env's field state
  - Detected events printed via tqdm.write() during training
  - Emergence metrics logged to W&B alongside training metrics
  - Summary printed at training completion if any events detected
- All 7 analysis tests pass (including 2 new EmergenceTracker tests), mypy clean (27 source files)
- Files changed: src/analysis/emergence.py (new), src/training/train.py (added emergence tracking integration)

### US-025: Create training launch script [x]
- Created scripts/train.sh as a training launch wrapper
- Script activates .venv, sets JAX performance flags (XLA_FLAGS, JAX_PLATFORM_NAME, XLA_PYTHON_CLIENT_PREALLOCATE)
- Passes all CLI arguments through to `python -m src.training.train` (tyro CLI)
- Detects project directory relative to script location (works from any CWD)
- Fails with clear error message if .venv not found (directs user to run setup.sh)
- Verified: `./scripts/train.sh --train.total-steps=10000` runs and completes successfully
- Mypy clean (27 source files, no issues)
- Files changed: scripts/train.sh (new)

### US-026: Update README with quick start [x]
- Rewrote README.md with accurate project documentation
- Sections added:
  1. Project overview explaining the shared learnable field hypothesis
  2. "How It Works" — per-timestep simulation loop
  3. Quick Start — install, train, evaluate commands
  4. Project structure — annotated tree of all modules
  5. Expected results — training metrics, field behavior phases, ablation interpretation
  6. Interpreting field visualizations — heatmap legend, pattern meanings
  7. Configuration table — all key parameters with defaults
  8. Tech stack listing
- Fixed inaccurate `--config` CLI flag (training uses tyro, not argparse with config files)
- Verified training starts successfully following README instructions
- Mypy clean (27 source files, no issues)
- Files changed: README.md (rewritten)

## Current Task
US-027: Full integration test

## Decisions Made
- Using JAX/Flax/Optax stack (not PyTorch)
- Dataclasses + YAML for config (not Hydra)
- W&B for experiment tracking
- Field has 4 channels by default
- 8 agents, 20x20 grid as starting point
- Shared rewards (team-based)

## Files Changed
- src/__init__.py
- src/configs.py (fixed mypy errors: renamed field import, fixed to_dict, fixed AgentConfig construction)
- src/environment/__init__.py
- src/agents/__init__.py
- src/training/__init__.py
- src/field/__init__.py
- src/analysis/__init__.py
- configs/default.yaml
- scripts/setup.sh
- pyproject.toml (added types-PyYAML to dev deps)
- tests/*.py (test stubs)
- src/field/field.py (FieldState dataclass and create_field)
- src/field/dynamics.py (diffuse, decay, step_field)
- src/field/ops.py (read_local, write_local)
- src/environment/state.py (EnvState dataclass and create_env_state)
- src/environment/env.py (added step function)
- src/environment/obs.py (get_observations, obs_dim)
- src/agents/network.py (ActorCritic module)
- src/agents/policy.py (sample_actions, get_deterministic_actions)
- src/training/rollout.py (RunnerState, collect_rollout)
- src/training/train.py (create_train_state, train_step, W&B integration)
- src/utils/logging.py (init_wandb, log_metrics, log_video, finish_wandb)
- src/utils/video.py (record_episode, save_video)
- src/analysis/field_metrics.py (field_entropy, field_structure, field_food_mi)
- src/analysis/ablation.py (ablation_test, print_ablation_results, CLI main)
- src/analysis/emergence.py (EmergenceTracker, EmergenceEvent, MetricHistory)

## Key Design Notes
- The FIELD is the key innovation (shared medium between agents)
- Field has its own dynamics (diffusion + decay)
- Agents read/write to field locally
- We test if field develops structure encoding collective knowledge
- Ablation test will show if field matters

## Notes for Future Tasks
- US-004 through US-006: Field implementation is critical — this is our novel contribution
- US-009: Environment step must update field each timestep
- US-022-024: Analysis is how we prove emergence — metrics must be meaningful
