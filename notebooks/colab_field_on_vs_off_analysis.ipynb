{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Field ON vs Field OFF: Statistical Comparison\n\n30 seeds per condition, 10M steps each, PROVEN 64-agent config.\n\n**All data loaded from Google Drive** (checkpoints for both conditions on Drive).\n\n## Setup\n1. Runtime > Change runtime type > **TPU v6e** + **High-RAM**\n2. Run all cells (Ctrl+F9)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "REPO_DIR = '/content/emergence-lab'\n",
    "GITHUB_USERNAME = \"imashishkh\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone https://github.com/{GITHUB_USERNAME}/emergence-lab.git {REPO_DIR}\n",
    "else:\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "!pip install -e \".[dev]\" -q\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport json\nimport pickle\nfrom pathlib import Path\n\nfrom src.analysis.statistics import (\n    compute_iqm, compare_methods, welch_t_test, mann_whitney_test,\n    probability_of_improvement,\n)\nfrom src.analysis.paper_figures import (\n    setup_publication_style, plot_performance_profiles,\n    save_figure,\n)\n\nFIELD_ON_DIR = '/content/drive/MyDrive/emergence-lab/field_on/'\nFIELD_OFF_DIR = '/content/drive/MyDrive/emergence-lab/field_off/'\nOUTPUT_DIR = '/content/drive/MyDrive/emergence-lab/analysis_results/'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(\"Imports loaded. Output dir:\", OUTPUT_DIR)\nprint(f\"Field ON dir:  {FIELD_ON_DIR} (exists: {os.path.exists(FIELD_ON_DIR)})\")\nprint(f\"Field OFF dir: {FIELD_OFF_DIR} (exists: {os.path.exists(FIELD_OFF_DIR)})\")"
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# TEST STEP - Verify everything works before running full analysis\n# =============================================================================\n# Tests: imports, stats, plots, BOTH Drive dirs, checkpoint loading, eval + divergence\n# If this passes, the full analysis will work. If it fails, fix before proceeding.\n# =============================================================================\nimport time\n\nprint(\"=\"*70)\nprint(\"TEST MODE: Running quick verification...\")\nprint(\"=\"*70)\n\nerrors = []\n\n# Test 1: Stats functions work\nprint(\"\\n[1/6] Testing statistics functions...\")\ntry:\n    test_a = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n    test_b = np.array([2.0, 3.0, 4.0, 5.0, 6.0])\n    iqm_test = compute_iqm(test_a, n_bootstrap=100, seed=0)\n    welch_test = welch_t_test(test_a, test_b)\n    mw_test = mann_whitney_test(test_a, test_b)\n    poi_test = probability_of_improvement(test_a, test_b, n_bootstrap=100, seed=0)\n    comp_test = compare_methods({\"A\": test_a, \"B\": test_b}, n_bootstrap=100, seed=0)\n    assert hasattr(iqm_test, 'iqm'), \"IQM result missing .iqm\"\n    assert hasattr(welch_test, 'p_value'), \"Welch result missing .p_value\"\n    assert 'prob_x_better' in poi_test, \"POI missing prob_x_better\"\n    assert hasattr(comp_test, 'summary'), \"Compare result missing .summary\"\n    print(\"   PASS: All statistics functions work correctly\")\nexcept Exception as e:\n    errors.append(f\"Stats functions: {e}\")\n    print(f\"   FAIL: {e}\")\n\n# Test 2: Plot functions work\nprint(\"\\n[2/6] Testing plot functions...\")\ntry:\n    setup_publication_style()\n    fig_test, ax_test = plt.subplots()\n    ax_test.plot([1, 2, 3])\n    plt.close(fig_test)\n    print(\"   PASS: Matplotlib + publication style working\")\nexcept Exception as e:\n    errors.append(f\"Plot functions: {e}\")\n    print(f\"   FAIL: {e}\")\n\n# Test 3: Field ON Drive directory\nprint(\"\\n[3/6] Testing Field ON Drive access...\")\ntry:\n    import glob as glob_mod\n    assert os.path.exists(FIELD_ON_DIR), f\"Directory not found: {FIELD_ON_DIR}\"\n    on_batches = [d for d in os.listdir(FIELD_ON_DIR) if d.startswith('batch_')]\n    assert len(on_batches) > 0, \"No batch directories found for Field ON\"\n    on_ckpts = 0\n    for bd in on_batches:\n        bp = os.path.join(FIELD_ON_DIR, bd)\n        for sd in os.listdir(bp):\n            sp = os.path.join(bp, sd)\n            if os.path.isdir(sp):\n                pkls = glob_mod.glob(os.path.join(sp, 'step_*.pkl'))\n                on_ckpts += len(pkls) > 0\n    print(f\"   PASS: Field ON -- {len(on_batches)} batches, {on_ckpts} seed checkpoints\")\nexcept Exception as e:\n    errors.append(f\"Field ON Drive: {e}\")\n    print(f\"   FAIL: {e}\")\n\n# Test 4: Field OFF Drive directory\nprint(\"\\n[4/6] Testing Field OFF Drive access...\")\ntry:\n    assert os.path.exists(FIELD_OFF_DIR), f\"Directory not found: {FIELD_OFF_DIR}\"\n    off_batches = [d for d in os.listdir(FIELD_OFF_DIR) if d.startswith('batch_')]\n    assert len(off_batches) > 0, \"No batch directories found for Field OFF\"\n    off_ckpts = 0\n    for bd in off_batches:\n        bp = os.path.join(FIELD_OFF_DIR, bd)\n        for sd in os.listdir(bp):\n            sp = os.path.join(bp, sd)\n            if os.path.isdir(sp):\n                pkls = glob_mod.glob(os.path.join(sp, 'step_*.pkl'))\n                off_ckpts += len(pkls) > 0\n    print(f\"   PASS: Field OFF -- {len(off_batches)} batches, {off_ckpts} seed checkpoints\")\nexcept Exception as e:\n    errors.append(f\"Field OFF Drive: {e}\")\n    print(f\"   FAIL: {e}\")\n\n# Test 5: Load one checkpoint from each condition\nprint(\"\\n[5/6] Testing checkpoint loading (one per condition)...\")\ntry:\n    import jax\n    import jax.numpy as jnp\n    from src.training.checkpointing import load_checkpoint\n    from src.agents.network import ActorCritic\n    from src.analysis.ablation import _run_episode_full\n    from src.analysis.specialization import compute_weight_divergence\n\n    for cond_name, cond_dir in [(\"Field ON\", FIELD_ON_DIR), (\"Field OFF\", FIELD_OFF_DIR)]:\n        test_ckpt = None\n        batch_dirs = sorted([d for d in os.listdir(cond_dir) if d.startswith('batch_')])\n        for bd in batch_dirs:\n            bp = os.path.join(cond_dir, bd)\n            for sd in sorted(os.listdir(bp)):\n                sp = os.path.join(bp, sd)\n                if os.path.isdir(sp):\n                    pkls = glob_mod.glob(os.path.join(sp, 'step_*.pkl'))\n                    if pkls:\n                        test_ckpt = sorted(pkls)[-1]\n                        break\n            if test_ckpt:\n                break\n        assert test_ckpt is not None, f\"No checkpoint found for {cond_name}\"\n        ckpt = load_checkpoint(test_ckpt)\n        config = ckpt['config']\n        assert hasattr(config, 'env'), f\"{cond_name} config not a dataclass\"\n        print(f\"   {cond_name}: seed={ckpt.get('seed_id', -1)}, grid={config.env.grid_size}, \"\n              f\"decay={config.field.decay_rate}, diffusion={config.field.diffusion_rate}\")\n    print(\"   PASS: Both conditions' checkpoints load correctly\")\nexcept Exception as e:\n    errors.append(f\"Checkpoint loading: {e}\")\n    print(f\"   FAIL: {e}\")\n\n# Test 6: Eval episode + weight divergence (on last loaded checkpoint)\nprint(\"\\n[6/6] Testing eval episode + weight divergence...\")\ntry:\n    agent_params = jax.tree_util.tree_map(lambda x: x[0], ckpt['agent_params'])\n    network = ActorCritic(hidden_dims=tuple(config.agent.hidden_dims), num_actions=6)\n    t0 = time.time()\n    key = jax.random.PRNGKey(42)\n    stats = _run_episode_full(\n        network=network, params=ckpt['params'], config=config,\n        key=key, condition=\"normal\", evolution=True,\n    )\n    elapsed = time.time() - t0\n    print(f\"   Eval: reward={stats.total_reward:.1f}, pop={stats.final_population} ({elapsed:.1f}s)\")\n\n    div = compute_weight_divergence(agent_params)\n    print(f\"   Divergence: mean={div['mean_divergence']:.4f}, n_agents={len(div['agent_indices'])}\")\n    print(f\"   PASS: Eval and weight divergence working\")\nexcept Exception as e:\n    errors.append(f\"Eval/divergence: {e}\")\n    print(f\"   FAIL: {e}\")\n\n# Summary\nprint()\nprint(\"=\"*70)\nif errors:\n    print(f\"TEST FAILED! {len(errors)} error(s):\")\n    for err in errors:\n        print(f\"  - {err}\")\n    print(\"\\nDO NOT proceed until all tests pass.\")\n    print(\"=\"*70)\n    raise RuntimeError(f\"Test failed with {len(errors)} error(s)\")\nelse:\n    print(\"ALL 6 TESTS PASSED!\")\n    print(\"=\"*70)\n    print(\"Proceed to run the full analysis below.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Phase 1: Load Training Data\n\nLoad training rewards from `training_summary.pkl` on Drive where available, with hardcoded fallback (verified by 4 independent audit agents)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== LOAD TRAINING-TIME DATA FROM DRIVE ==========\nimport glob as glob_mod\n\nFAILED_THRESHOLD = 1.0  # Seeds with reward below this are considered \"failed\"\n\ndef discover_checkpoints(drive_dir, condition_name):\n    \"\"\"Discover all checkpoint paths in a Drive directory.\"\"\"\n    paths = []\n    for batch_idx in range(10):\n        batch_dir = os.path.join(drive_dir, f'batch_{batch_idx}')\n        if not os.path.exists(batch_dir):\n            continue\n        for seed_dir in sorted(os.listdir(batch_dir)):\n            seed_path = os.path.join(batch_dir, seed_dir)\n            if not os.path.isdir(seed_path):\n                continue\n            pkl_files = glob_mod.glob(os.path.join(seed_path, 'step_*.pkl'))\n            if pkl_files:\n                paths.append(sorted(pkl_files)[-1])\n    print(f\"  {condition_name}: Found {len(paths)} checkpoints on Drive\")\n    return paths\n\ndef load_training_summary(drive_dir, condition_name):\n    \"\"\"Load training_summary.pkl and extract per-seed rewards + populations.\"\"\"\n    summary_path = os.path.join(drive_dir, 'training_summary.pkl')\n    if not os.path.exists(summary_path):\n        print(f\"  {condition_name}: No training_summary.pkl found\")\n        return None, None\n    with open(summary_path, 'rb') as f:\n        summary = pickle.load(f)\n    # Format: {all_results: [{batch, seed_ids, metrics: {mean_reward, population_size, ...}, success: bool}, ...]}\n    rewards = []\n    populations = []\n    skipped = 0\n    for batch in summary['all_results']:\n        if not batch.get('success', True):\n            skipped += 1\n            continue\n        rewards.extend(batch['metrics']['mean_reward'])\n        populations.extend(batch['metrics']['population_size'])\n    rewards = np.array(rewards)\n    populations = np.array(populations, dtype=int)\n    if skipped > 0:\n        print(f\"  {condition_name}: WARNING - skipped {skipped} failed batches\")\n    print(f\"  {condition_name}: Loaded {len(rewards)} rewards from training_summary.pkl\")\n    return rewards, populations\n\nprint(\"Discovering data on Drive...\")\nfield_on_ckpt_paths = discover_checkpoints(FIELD_ON_DIR, \"Field ON\")\nfield_off_ckpt_paths = discover_checkpoints(FIELD_OFF_DIR, \"Field OFF\")\n\n# ---- Load Field ON data ----\non_rewards_drive, on_pops_drive = load_training_summary(FIELD_ON_DIR, \"Field ON\")\nif on_rewards_drive is not None:\n    field_on_rewards = on_rewards_drive\n    field_on_populations = on_pops_drive\nelse:\n    # Hardcoded from EXPERIMENT_LOG.md (verified by 4 independent audit agents)\n    field_on_rewards = np.array([\n        5.19, 5.38, 4.70, 3.09, 4.84, 5.50, 2.54, 0.00, 5.18, 4.52,\n        5.09, 5.33, 5.38, 3.70, 5.24, 4.61, 3.46, 4.56, 5.48, 4.42,\n        5.43, 4.56, 5.30, 4.99, 4.22, 4.33, 5.51, 4.19, 4.67, 5.20,\n    ])\n    field_on_populations = np.array([\n        64, 64, 6, 22, 64, 64, 11, 0, 64, 20,\n        64, 64, 40, 30, 48, 62, 39, 58, 64, 64,\n        64, 58, 64, 30, 28, 50, 62, 52, 60, 58,\n    ])\n    print(\"  Field ON: Using hardcoded values (no training_summary.pkl)\")\n\n# ---- Load Field OFF data ----\noff_rewards_drive, off_pops_drive = load_training_summary(FIELD_OFF_DIR, \"Field OFF\")\nif off_rewards_drive is not None:\n    field_off_rewards = off_rewards_drive\n    field_off_populations_train = off_pops_drive\nelse:\n    # Hardcoded (verified against training_summary.pkl by audit)\n    field_off_rewards = np.array([\n        5.527, 5.614, 5.378, 5.728, 5.547, 5.624, 5.618, 5.386, 5.685, 5.600,\n        5.430, 4.785, 5.542, 5.556, 5.610, 5.709, 5.494, 5.487, 5.695, 5.661,\n        5.457, 5.605, 5.180, 5.588, 5.428, 5.399, 5.476, 5.297, 5.712, 5.670,\n    ])\n    field_off_populations_train = None\n    print(\"  Field OFF: Using hardcoded values (no training_summary.pkl)\")\n\n# Populations from eval episodes (populated in Phase 3)\nfield_on_eval_populations = None\nfield_off_eval_populations = None\n\nprint(f\"\\n{'='*60}\")\nprint(\"TRAINING DATA SUMMARY\")\nprint(f\"{'='*60}\")\nprint(f\"Field ON:  {len(field_on_rewards)} seeds, mean={field_on_rewards.mean():.3f} +/- {field_on_rewards.std(ddof=1):.3f}\")\nprint(f\"Field OFF: {len(field_off_rewards)} seeds, mean={field_off_rewards.mean():.3f} +/- {field_off_rewards.std(ddof=1):.3f}\")\nprint(f\"Field ON population: mean={field_on_populations.mean():.1f}, at max(64): {np.sum(field_on_populations==64)}/30\")\nif field_off_populations_train is not None:\n    print(f\"Field OFF population (train): mean={field_off_populations_train.mean():.1f}, at max(64): {np.sum(field_off_populations_train==64)}/30\")\nprint(f\"Field ON failed seeds (reward < {FAILED_THRESHOLD}): {np.sum(field_on_rewards < FAILED_THRESHOLD)}\")\nprint(f\"\\nCheckpoints on Drive: {len(field_on_ckpt_paths)} Field ON, {len(field_off_ckpt_paths)} Field OFF\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== DESCRIPTIVE STATISTICS ==========\nprint(\"=\"*60)\nprint(\"DESCRIPTIVE STATISTICS\")\nprint(\"=\"*60)\n\nfor name, rewards in [(\"Field ON\", field_on_rewards), (\"Field OFF\", field_off_rewards)]:\n    iqm = compute_iqm(rewards, n_bootstrap=10000, seed=42)\n    print(f\"\\n{name} (n={len(rewards)}):\")\n    print(f\"  Mean:   {rewards.mean():.4f} +/- {rewards.std(ddof=1):.4f}\")\n    print(f\"  Median: {np.median(rewards):.4f}\")\n    print(f\"  IQM:    {iqm.iqm:.4f} [{iqm.ci_lower:.4f}, {iqm.ci_upper:.4f}]\")\n    print(f\"  Min:    {rewards.min():.4f}\")\n    print(f\"  Max:    {rewards.max():.4f}\")\n    print(f\"  CoV:    {rewards.std(ddof=1)/rewards.mean():.4f}\")\n\n# Compute and store IQMs for later\niqm_on = compute_iqm(field_on_rewards, n_bootstrap=10000, seed=42)\niqm_off = compute_iqm(field_off_rewards, n_bootstrap=10000, seed=42)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== HYPOTHESIS TESTS ==========\nprint(\"=\"*60)\nprint(\"HYPOTHESIS TESTS\")\nprint(\"=\"*60)\n\n# Welch's t-test\nwelch = welch_t_test(field_off_rewards, field_on_rewards)\nprint(f\"\\n1. Welch's t-test:\")\nprint(f\"   t = {welch.statistic:.4f}, p = {welch.p_value:.6f}\")\nprint(f\"   Cohen's d = {welch.effect_size:.4f}\", end=\"\")\nd = abs(welch.effect_size)\nif d < 0.2: print(\" (negligible)\")\nelif d < 0.5: print(\" (small)\")\nelif d < 0.8: print(\" (MEDIUM)\")\nelse: print(\" (LARGE)\")\nprint(f\"   Significant at alpha=0.05: {welch.significant}\")\n\n# Mann-Whitney U\nmw = mann_whitney_test(field_off_rewards, field_on_rewards)\nprint(f\"\\n2. Mann-Whitney U test:\")\nprint(f\"   U = {mw.statistic:.1f}, p = {mw.p_value:.6f}\")\nprint(f\"   Rank-biserial r = {mw.effect_size:.4f}\")\nprint(f\"   Significant at alpha=0.05: {mw.significant}\")\n\n# Probability of Improvement\npoi = probability_of_improvement(field_off_rewards, field_on_rewards, n_bootstrap=5000, seed=42)\nprint(f\"\\n3. Probability of Improvement:\")\nprint(f\"   P(Field OFF > Field ON) = {poi['prob_x_better']:.4f}\")\nprint(f\"   P(Field ON > Field OFF) = {poi['prob_y_better']:.4f}\")\nprint(f\"   95% CI: [{poi['ci_lower']:.4f}, {poi['ci_upper']:.4f}]\")\n\n# Direction\nprint(f\"\\n4. Direction:\")\nprint(f\"   Field OFF mean: {field_off_rewards.mean():.4f}\")\nprint(f\"   Field ON mean:  {field_on_rewards.mean():.4f}\")\nprint(f\"   Gap: {field_off_rewards.mean() - field_on_rewards.mean():+.4f} (Field OFF {'higher' if field_off_rewards.mean() > field_on_rewards.mean() else 'lower'})\")\n\n# Sensitivity: exclude failed seeds (using consistent threshold)\nmask_on = field_on_rewards >= FAILED_THRESHOLD\non_filtered = field_on_rewards[mask_on]\nwelch_f = welch_t_test(field_off_rewards, on_filtered)\nprint(f\"\\n5. Sensitivity (excluding {np.sum(~mask_on)} failed Field ON seeds, threshold={FAILED_THRESHOLD}):\")\nprint(f\"   Field ON filtered: n={len(on_filtered)}, mean={on_filtered.mean():.4f}\")\nprint(f\"   Welch p = {welch_f.p_value:.6f}, Cohen's d = {welch_f.effect_size:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== FULL METHOD COMPARISON ==========\n",
    "print(\"=\"*60)\n",
    "print(\"FULL METHOD COMPARISON (rliable-style)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison = compare_methods(\n",
    "    {\"Field ON\": field_on_rewards, \"Field OFF\": field_off_rewards},\n",
    "    n_bootstrap=10000, seed=42,\n",
    ")\n",
    "print(comparison.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== COMPARISON PLOTS ==========\nsetup_publication_style()\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# 1. Bar chart: IQM with CI\nax = axes[0]\nmethods = ['Field ON\\n(Stigmergy)', 'Field OFF\\n(No Field)']\niqm_vals = [iqm_on.iqm, iqm_off.iqm]\niqm_lo = [iqm_on.iqm - iqm_on.ci_lower, iqm_off.iqm - iqm_off.ci_lower]\niqm_hi = [iqm_on.ci_upper - iqm_on.iqm, iqm_off.ci_upper - iqm_off.iqm]\ncolors = ['#009988', '#BBBBBB']\n\nbars = ax.bar(methods, iqm_vals, yerr=[iqm_lo, iqm_hi], color=colors,\n              edgecolor='black', linewidth=0.5, capsize=8)\nfor bar, val in zip(bars, iqm_vals):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(iqm_hi) + 0.05,\n            f'{val:.3f}', ha='center', fontsize=10)\nax.set_ylabel('IQM Reward')\nax.set_title('(a) IQM + 95% CI')\nsig = f\"p = {welch.p_value:.4f}\"\nif welch.p_value < 0.001: sig += \" ***\"\nelif welch.p_value < 0.01: sig += \" **\"\nelif welch.p_value < 0.05: sig += \" *\"\nax.text(0.5, max(iqm_vals) + max(iqm_hi) + 0.3, sig, ha='center', fontsize=11,\n        transform=ax.get_xaxis_transform())\n\n# 2. Violin + swarm\nax = axes[1]\nparts = ax.violinplot([field_on_rewards, field_off_rewards], positions=[1, 2],\n                       showmeans=True, showmedians=True, showextrema=False)\nfor i, body in enumerate(parts['bodies']):\n    body.set_facecolor(colors[i])\n    body.set_alpha(0.4)\n\nrng = np.random.default_rng(42)\nfor i, (data, pos) in enumerate(zip([field_on_rewards, field_off_rewards], [1, 2])):\n    jitter = rng.normal(0, 0.05, size=len(data))\n    ax.scatter(np.full_like(data, pos) + jitter, data, alpha=0.6, s=25,\n               color=colors[i], edgecolor='black', linewidth=0.3, zorder=3)\n\nax.scatter([1], [iqm_on.iqm], marker='D', s=80, color='red', zorder=5, label='IQM')\nax.scatter([2], [iqm_off.iqm], marker='D', s=80, color='red', zorder=5)\nax.set_xticks([1, 2])\nax.set_xticklabels(['Field ON', 'Field OFF'])\nax.set_ylabel('Mean Reward')\nax.set_title('(b) Distribution (all 60 seeds)')\nax.legend(fontsize=9)\n\n# 3. Population comparison (training-time)\nax = axes[2]\nax.hist(field_on_populations, bins=15, alpha=0.6, color=colors[0],\n        label='Field ON', edgecolor='black')\nif field_off_populations_train is not None:\n    ax.hist(field_off_populations_train, bins=15, alpha=0.6, color=colors[1],\n            label='Field OFF', edgecolor='black')\nax.set_xlabel('Final Population (training)')\nax.set_ylabel('Count')\nax.set_title('(c) Population Distribution')\nax.axvline(x=64, color='red', linestyle='--', alpha=0.5, label='Max capacity')\nax.legend(fontsize=9)\n\nplt.suptitle('Field ON vs Field OFF: 30-Seed Comparison', fontsize=14, y=1.02)\nplt.tight_layout()\nsave_figure(fig, os.path.join(OUTPUT_DIR, 'main_comparison'))\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== PERFORMANCE PROFILES ==========\nsetup_publication_style()\nfig = plot_performance_profiles(\n    {\"Field ON (Stigmergy)\": field_on_rewards, \"Field OFF (No Field)\": field_off_rewards},\n    output_path=os.path.join(OUTPUT_DIR, 'performance_profiles'),\n    tau_range=(0, 1.05),\n)\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== REWARD vs POPULATION ==========\nsetup_publication_style()\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Field ON\nax = axes[0]\nax.scatter(field_on_populations, field_on_rewards, color='#009988', label='Field ON',\n           s=50, alpha=0.7, edgecolor='black', linewidth=0.5)\nax.set_xlabel('Final Population')\nax.set_ylabel('Mean Reward')\nax.set_title('(a) Field ON: Reward vs Population')\n\n# Annotate failed seeds\nfailed_idx = np.where(field_on_rewards < FAILED_THRESHOLD)[0]\nfor idx in failed_idx:\n    ax.annotate(f'Seed {idx}\\n(died)', xy=(field_on_populations[idx], field_on_rewards[idx]),\n                fontsize=8, color='red', arrowprops=dict(arrowstyle='->', color='red'),\n                xytext=(field_on_populations[idx]+5, field_on_rewards[idx]+1))\n\n# Field OFF\nax = axes[1]\nif field_off_populations_train is not None:\n    ax.scatter(field_off_populations_train, field_off_rewards, color='#BBBBBB',\n               label='Field OFF', s=50, alpha=0.7, edgecolor='black', linewidth=0.5)\n    ax.set_xlabel('Final Population')\n    ax.set_ylabel('Mean Reward')\n    ax.set_title('(b) Field OFF: Reward vs Population')\nelse:\n    ax.text(0.5, 0.5, 'Field OFF populations\\navailable after eval (Phase 3)',\n            ha='center', va='center', transform=ax.transAxes, fontsize=12)\n    ax.set_title('(b) Field OFF: Reward vs Population')\n\nplt.tight_layout()\nsave_figure(fig, os.path.join(OUTPUT_DIR, 'reward_vs_population'))\nplt.show()\n\n# Correlation for Field ON (excluding failed seeds)\nfrom scipy import stats as scipy_stats\nmask = field_on_rewards >= FAILED_THRESHOLD\nr, p = scipy_stats.pearsonr(field_on_populations[mask], field_on_rewards[mask])\nprint(f\"Field ON correlation (pop vs reward, excl. failed): r={r:.3f}, p={p:.6f}\")\n\nif field_off_populations_train is not None:\n    r2, p2 = scipy_stats.pearsonr(field_off_populations_train, field_off_rewards)\n    print(f\"Field OFF correlation (pop vs reward): r={r2:.3f}, p={p2:.6f}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Phase 3: Checkpoint Analysis (BOTH Conditions)\n\nLoad ALL 60 checkpoints from Drive, run eval episodes and compute weight divergence for both Field ON and Field OFF.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ========== LOAD ALL CHECKPOINTS (BOTH CONDITIONS) ==========\nimport jax\nimport jax.numpy as jnp\nfrom src.training.checkpointing import load_checkpoint\nfrom src.agents.network import ActorCritic\nfrom src.analysis.ablation import _run_episode_full\n\ndef load_seed_data(ckpt_path):\n    \"\"\"Load a checkpoint and extract seed data for eval + analysis.\"\"\"\n    ckpt = load_checkpoint(ckpt_path)\n    config = ckpt['config']\n    agent_params = jax.tree_util.tree_map(lambda x: x[0], ckpt['agent_params'])\n    network = ActorCritic(hidden_dims=tuple(config.agent.hidden_dims), num_actions=6)\n    return {\n        'params': ckpt['params'],\n        'agent_params': agent_params,\n        'config': config,\n        'network': network,\n        'seed_id': ckpt.get('seed_id', -1),\n    }\n\n# Verify checkpoint counts\nprint(f\"Field ON checkpoints:  {len(field_on_ckpt_paths)}\")\nprint(f\"Field OFF checkpoints: {len(field_off_ckpt_paths)}\")\nassert len(field_on_ckpt_paths) == 30, f\"Expected 30 Field ON checkpoints, got {len(field_on_ckpt_paths)}\"\nassert len(field_off_ckpt_paths) == 30, f\"Expected 30 Field OFF checkpoints, got {len(field_off_ckpt_paths)}\"\n\n# Test load one from each condition\nfor name, paths in [(\"Field ON\", field_on_ckpt_paths), (\"Field OFF\", field_off_ckpt_paths)]:\n    test_data = load_seed_data(paths[0])\n    cfg = test_data['config']\n    print(f\"\\n{name} config verification:\")\n    print(f\"  seed={test_data['seed_id']}, grid={cfg.env.grid_size}, max_agents={cfg.evolution.max_agents}\")\n    print(f\"  diffusion={cfg.field.diffusion_rate}, decay={cfg.field.decay_rate}, write={cfg.field.write_strength}\")\n\nprint(f\"\\nAll checkpoints verified. Ready for eval episodes.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ========== EVAL EPISODES (BOTH CONDITIONS) ==========\n# Run 5 eval episodes per seed to get population dynamics + survival stats\n# Uses shared params (not per-agent); \"normal\" condition (field config differs between conditions)\n\nNUM_EVAL_EPISODES = 5\neval_results_on = []\neval_results_off = []\n\nfor cond_name, ckpt_paths, results_list in [\n    (\"Field ON\", field_on_ckpt_paths, eval_results_on),\n    (\"Field OFF\", field_off_ckpt_paths, eval_results_off),\n]:\n    print(f\"\\n{'='*60}\")\n    print(f\"EVALUATING {cond_name} ({len(ckpt_paths)} seeds x {NUM_EVAL_EPISODES} episodes)\")\n    print(f\"{'='*60}\")\n\n    for i, ckpt_path in enumerate(ckpt_paths):\n        seed_data = load_seed_data(ckpt_path)\n        config = seed_data['config']\n\n        seed_pops = []\n        seed_rewards = []\n        seed_births = []\n        seed_deaths = []\n\n        for ep in range(NUM_EVAL_EPISODES):\n            key = jax.random.PRNGKey(ep * 1000 + i)\n            stats = _run_episode_full(\n                network=seed_data['network'],\n                params=seed_data['params'],\n                config=config,\n                key=key,\n                condition=\"normal\",\n                evolution=True,\n            )\n            seed_pops.append(stats.final_population)\n            seed_rewards.append(stats.total_reward)\n            seed_births.append(stats.total_births)\n            seed_deaths.append(stats.total_deaths)\n\n        results_list.append({\n            'seed_id': seed_data['seed_id'],\n            'ckpt_path': ckpt_path,\n            'mean_total_reward': np.mean(seed_rewards),\n            'std_total_reward': np.std(seed_rewards),\n            'mean_population': np.mean(seed_pops),\n            'mean_births': np.mean(seed_births),\n            'mean_deaths': np.mean(seed_deaths),\n            'survival_rate': np.mean(seed_pops) / config.evolution.max_agents,\n            'all_rewards': seed_rewards,\n            'all_populations': seed_pops,\n        })\n\n        if (i + 1) % 5 == 0 or i == 0:\n            print(f\"  [{i+1}/{len(ckpt_paths)}] seed {seed_data['seed_id']}: \"\n                  f\"reward={np.mean(seed_rewards):.1f}, pop={np.mean(seed_pops):.1f}, \"\n                  f\"births={np.mean(seed_births):.0f}, deaths={np.mean(seed_deaths):.0f}\")\n\n# Extract populations and eval rewards\nfield_on_eval_populations = np.array([r['mean_population'] for r in eval_results_on])\nfield_off_eval_populations = np.array([r['mean_population'] for r in eval_results_off])\nfield_on_eval_rewards = np.array([r['mean_total_reward'] for r in eval_results_on])\nfield_off_eval_rewards = np.array([r['mean_total_reward'] for r in eval_results_off])\n\n# Summary\nfor cond_name, evals, pops, rewards in [\n    (\"Field ON\", eval_results_on, field_on_eval_populations, field_on_eval_rewards),\n    (\"Field OFF\", eval_results_off, field_off_eval_populations, field_off_eval_rewards),\n]:\n    print(f\"\\n{'='*60}\")\n    print(f\"{cond_name} EVAL SUMMARY ({len(evals)} seeds x {NUM_EVAL_EPISODES} episodes)\")\n    print(f\"  Mean total reward: {rewards.mean():.1f} +/- {rewards.std():.1f}\")\n    print(f\"  Mean population:   {pops.mean():.1f} +/- {pops.std():.1f}\")\n    print(f\"  At max capacity:   {np.sum(pops >= 60)}/{len(pops)}\")\n    print(f\"  Mean births:       {np.mean([r['mean_births'] for r in evals]):.1f}\")\n    print(f\"  Mean deaths:       {np.mean([r['mean_deaths'] for r in evals]):.1f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ========== WEIGHT DIVERGENCE (BOTH CONDITIONS) ==========\nfrom src.analysis.specialization import compute_weight_divergence\n\ndivergence_on = []\ndivergence_off = []\n\nfor cond_name, ckpt_paths, div_list in [\n    (\"Field ON\", field_on_ckpt_paths, divergence_on),\n    (\"Field OFF\", field_off_ckpt_paths, divergence_off),\n]:\n    print(f\"\\n{'='*60}\")\n    print(f\"WEIGHT DIVERGENCE: {cond_name}\")\n    print(f\"{'='*60}\")\n\n    for i, ckpt_path in enumerate(ckpt_paths):\n        seed_data = load_seed_data(ckpt_path)\n        div = compute_weight_divergence(seed_data['agent_params'])\n        div_list.append({\n            'seed_id': seed_data['seed_id'],\n            'mean_divergence': float(div['mean_divergence']),\n            'max_divergence': float(div['max_divergence']),\n            'n_agents': len(div['agent_indices']),\n        })\n\n        if (i + 1) % 10 == 0 or i == 0:\n            print(f\"  [{i+1}/{len(ckpt_paths)}] seed {seed_data['seed_id']}: \"\n                  f\"mean_div={div['mean_divergence']:.4f}, max_div={div['max_divergence']:.4f}\")\n\n# Extract arrays\non_mean_divs = np.array([r['mean_divergence'] for r in divergence_on])\noff_mean_divs = np.array([r['mean_divergence'] for r in divergence_off])\non_max_divs = np.array([r['max_divergence'] for r in divergence_on])\noff_max_divs = np.array([r['max_divergence'] for r in divergence_off])\n\n# Summary\nfor name, mean_d, max_d in [(\"Field ON\", on_mean_divs, on_max_divs),\n                              (\"Field OFF\", off_mean_divs, off_max_divs)]:\n    print(f\"\\n{name} DIVERGENCE SUMMARY:\")\n    print(f\"  Mean: {mean_d.mean():.4f} +/- {mean_d.std():.4f}\")\n    print(f\"  Max:  {max_d.mean():.4f} +/- {max_d.std():.4f}\")\n    print(f\"  Range: [{mean_d.min():.4f}, {mean_d.max():.4f}]\")\n\n# ========== PHASE 3 PLOTS ==========\nsetup_publication_style()\ncolors = ['#009988', '#BBBBBB']\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# (a) Weight divergence comparison\nax = axes[0, 0]\nax.hist(on_mean_divs, bins=12, alpha=0.6, color=colors[0], label='Field ON', edgecolor='black')\nax.hist(off_mean_divs, bins=12, alpha=0.6, color=colors[1], label='Field OFF', edgecolor='black')\nax.axvline(on_mean_divs.mean(), color=colors[0], linestyle='--', linewidth=2)\nax.axvline(off_mean_divs.mean(), color=colors[1], linestyle='--', linewidth=2)\nax.set_xlabel('Mean Pairwise Weight Divergence (cosine)')\nax.set_ylabel('Count')\nax.set_title('(a) Weight Divergence Distribution')\nax.legend()\n\n# (b) Eval population comparison\nax = axes[0, 1]\nax.hist(field_on_eval_populations, bins=15, alpha=0.6, color=colors[0],\n        label='Field ON', edgecolor='black')\nax.hist(field_off_eval_populations, bins=15, alpha=0.6, color=colors[1],\n        label='Field OFF', edgecolor='black')\nax.axvline(x=64, color='red', linestyle='--', alpha=0.5, label='Max capacity')\nax.set_xlabel('Eval Population')\nax.set_ylabel('Count')\nax.set_title('(b) Eval Population Distribution')\nax.legend()\n\n# (c) Eval reward vs population (both conditions)\nax = axes[1, 0]\nax.scatter(field_on_eval_populations, field_on_eval_rewards, color=colors[0],\n           s=50, alpha=0.7, edgecolor='black', linewidth=0.5, label='Field ON')\nax.scatter(field_off_eval_populations, field_off_eval_rewards, color=colors[1],\n           s=50, alpha=0.7, edgecolor='black', linewidth=0.5, label='Field OFF')\nax.set_xlabel('Eval Population')\nax.set_ylabel('Total Eval Reward')\nax.set_title('(c) Eval: Reward vs Population')\nax.legend()\n\n# (d) Divergence vs training reward\nax = axes[1, 1]\nax.scatter(on_mean_divs, field_on_rewards, color=colors[0],\n           s=50, alpha=0.7, edgecolor='black', linewidth=0.5, label='Field ON')\nax.scatter(off_mean_divs, field_off_rewards, color=colors[1],\n           s=50, alpha=0.7, edgecolor='black', linewidth=0.5, label='Field OFF')\nax.set_xlabel('Mean Weight Divergence')\nax.set_ylabel('Training Reward')\nax.set_title('(d) Divergence vs Training Reward')\nax.legend()\n\nplt.suptitle('Phase 3: Checkpoint Analysis (Both Conditions)', fontsize=14, y=1.01)\nplt.tight_layout()\nsave_figure(fig, os.path.join(OUTPUT_DIR, 'checkpoint_analysis'))\nplt.show()\n\n# Divergence statistical comparison\ndiv_welch = welch_t_test(on_mean_divs, off_mean_divs)\ndiv_mw = mann_whitney_test(on_mean_divs, off_mean_divs)\nprint(f\"\\nDivergence comparison:\")\nprint(f\"  Welch t-test: t={div_welch.statistic:.4f}, p={div_welch.p_value:.6f}, d={div_welch.effect_size:.4f}\")\nprint(f\"  Mann-Whitney: U={div_mw.statistic:.1f}, p={div_mw.p_value:.6f}, r={div_mw.effect_size:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Phase 4: Report & Save\n\nGenerate formatted comparison report and save all results to Drive.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ========== COMPARISON REPORT ==========\nfrom datetime import datetime\n\n# Determine significance level\nif welch.p_value < 0.001:\n    sig_str = \"p < 0.001 (***)\"\nelif welch.p_value < 0.01:\n    sig_str = f\"p = {welch.p_value:.4f} (**)\"\nelif welch.p_value < 0.05:\n    sig_str = f\"p = {welch.p_value:.4f} (*)\"\nelse:\n    sig_str = f\"p = {welch.p_value:.4f} (not significant)\"\n\nd_val = abs(welch.effect_size)\nif d_val < 0.2: d_str = \"negligible\"\nelif d_val < 0.5: d_str = \"small\"\nelif d_val < 0.8: d_str = \"medium\"\nelse: d_str = \"large\"\n\nwinner = \"Field OFF\" if field_off_rewards.mean() > field_on_rewards.mean() else \"Field ON\"\n\n# Sample std for CoV\ncov_on = field_on_rewards.std(ddof=1) / field_on_rewards.mean()\ncov_off = field_off_rewards.std(ddof=1) / field_off_rewards.mean()\n\nn_failed = int(np.sum(field_on_rewards < FAILED_THRESHOLD))\non_nonfailed = field_on_rewards[field_on_rewards >= FAILED_THRESHOLD]\n\nreport = f\"\"\"# Field ON vs Field OFF: 30-Seed Comparison Report\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n## Experiment Setup\n- **Conditions**: Field ON (stigmergy) vs Field OFF (no shared field)\n- **Seeds per condition**: 30\n- **Training steps**: 10M per seed\n- **Config**: 64-agent, grid=32, num_food=40, starting_energy=200\n- **Field ON**: diffusion=0.1, decay=0.05, write_strength=1.0\n- **Field OFF**: diffusion=0.0, decay=1.0, write_strength=0.0\n\n## Key Results\n\n### Training Reward Comparison\n| Metric | Field ON | Field OFF |\n|--------|----------|-----------|\n| Mean | {field_on_rewards.mean():.4f} +/- {field_on_rewards.std(ddof=1):.4f} | {field_off_rewards.mean():.4f} +/- {field_off_rewards.std(ddof=1):.4f} |\n| Median | {np.median(field_on_rewards):.4f} | {np.median(field_off_rewards):.4f} |\n| IQM | {iqm_on.iqm:.4f} [{iqm_on.ci_lower:.4f}, {iqm_on.ci_upper:.4f}] | {iqm_off.iqm:.4f} [{iqm_off.ci_lower:.4f}, {iqm_off.ci_upper:.4f}] |\n| Min | {field_on_rewards.min():.4f} | {field_off_rewards.min():.4f} |\n| Max | {field_on_rewards.max():.4f} | {field_off_rewards.max():.4f} |\n| CoV | {cov_on:.4f} | {cov_off:.4f} |\n\n### Statistical Tests\n- **Welch's t-test**: {sig_str}, Cohen's d = {welch.effect_size:.4f} ({d_str})\n- **Mann-Whitney U**: U = {mw.statistic:.1f}, p = {mw.p_value:.6f}, rank-biserial r = {mw.effect_size:.4f}\n- **P(Field OFF > Field ON)**: {poi['prob_x_better']:.4f}\n\n### Winner: **{winner}** (by mean reward)\n\n### Sensitivity Analysis (threshold = {FAILED_THRESHOLD})\n- Excluding {n_failed} failed Field ON seeds:\n  Field ON filtered mean = {on_nonfailed.mean():.4f} (n={len(on_nonfailed)})\n  Welch p = {welch_f.p_value:.6f}, Cohen's d = {welch_f.effect_size:.4f}\n\n### Eval Episodes ({NUM_EVAL_EPISODES} episodes/seed, BOTH conditions)\n| Metric | Field ON | Field OFF |\n|--------|----------|-----------|\n| Mean total reward | {field_on_eval_rewards.mean():.1f} +/- {field_on_eval_rewards.std():.1f} | {field_off_eval_rewards.mean():.1f} +/- {field_off_eval_rewards.std():.1f} |\n| Mean population | {field_on_eval_populations.mean():.1f} +/- {field_on_eval_populations.std():.1f} | {field_off_eval_populations.mean():.1f} +/- {field_off_eval_populations.std():.1f} |\n| At max capacity | {np.sum(field_on_eval_populations >= 60)}/{len(field_on_eval_populations)} | {np.sum(field_off_eval_populations >= 60)}/{len(field_off_eval_populations)} |\n| Mean births | {np.mean([r['mean_births'] for r in eval_results_on]):.1f} | {np.mean([r['mean_births'] for r in eval_results_off]):.1f} |\n| Mean deaths | {np.mean([r['mean_deaths'] for r in eval_results_on]):.1f} | {np.mean([r['mean_deaths'] for r in eval_results_off]):.1f} |\n\n### Weight Divergence (BOTH conditions)\n| Metric | Field ON | Field OFF |\n|--------|----------|-----------|\n| Mean divergence | {on_mean_divs.mean():.4f} +/- {on_mean_divs.std():.4f} | {off_mean_divs.mean():.4f} +/- {off_mean_divs.std():.4f} |\n| Max divergence | {on_max_divs.mean():.4f} +/- {on_max_divs.std():.4f} | {off_max_divs.mean():.4f} +/- {off_max_divs.std():.4f} |\n| Divergence Welch p | {div_welch.p_value:.6f} | Cohen's d = {div_welch.effect_size:.4f} |\n\n### Population Dynamics (training-time)\n| Metric | Field ON | Field OFF |\n|--------|----------|-----------|\n| Mean final pop | {field_on_populations.mean():.1f} | {f'{field_off_populations_train.mean():.1f}' if field_off_populations_train is not None else 'N/A (eval only)'} |\n| At max (64) | {np.sum(field_on_populations == 64)}/30 | {f'{np.sum(field_off_populations_train == 64)}/30' if field_off_populations_train is not None else 'N/A'} |\n| Failed seeds | {n_failed} | 0 |\n\n## Interpretation\n\n**Surprising finding**: Field OFF agents achieve {'higher' if field_off_rewards.mean() > field_on_rewards.mean() else 'lower'} mean reward than Field ON.\n\nKey observations:\n1. Field ON has MUCH higher variance (CoV {cov_on:.3f} vs {cov_off:.3f})\n2. Field ON has {n_failed} failed seed(s) with near-zero reward\n3. Field OFF is remarkably consistent across all 30 seeds\n4. When excluding failed seeds, the gap {'narrows' if abs(welch_f.effect_size) < abs(welch.effect_size) else 'remains'}\n5. Weight divergence comparison: {'Field ON has higher divergence' if on_mean_divs.mean() > off_mean_divs.mean() else 'Field OFF has higher divergence'} (p={div_welch.p_value:.4f})\n\nPossible explanations:\n- The shared field may introduce a coordination overhead that hurts some seeds\n- Field ON populations are more variable (some collapse, some max out)\n- The field may be a harder optimization landscape requiring more training\n- Field OFF is simpler: agents just learn individual foraging without field-reading costs\n- Higher weight divergence in Field ON could indicate the field enables specialization\n\n## Next Steps\n- Investigate why some Field ON seeds fail (population collapse analysis)\n- Try longer training (20M+ steps) to see if Field ON catches up\n- Test with diversity_bonus and niche_pressure enabled\n- Analyze field channel specialization in successful Field ON seeds\n- Compare behavioral clustering between conditions\n\"\"\"\n\nfrom IPython.display import Markdown, display\ndisplay(Markdown(report))\nprint(\"\\nReport generated successfully.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ========== SAVE RESULTS ==========\nresults = {\n    'metadata': {\n        'generated': datetime.now().isoformat(),\n        'field_on_seeds': 30,\n        'field_off_seeds': 30,\n        'steps_per_seed': 10_000_000,\n        'eval_episodes_per_seed': NUM_EVAL_EPISODES,\n        'data_source': 'Google Drive checkpoints + training_summary.pkl',\n    },\n    'field_on': {\n        'training_rewards': field_on_rewards.tolist(),\n        'training_populations': field_on_populations.tolist(),\n        'mean_reward': float(field_on_rewards.mean()),\n        'std_reward': float(field_on_rewards.std(ddof=1)),\n        'iqm': float(iqm_on.iqm),\n        'iqm_ci': [float(iqm_on.ci_lower), float(iqm_on.ci_upper)],\n        'eval': {\n            'populations': field_on_eval_populations.tolist(),\n            'total_rewards': field_on_eval_rewards.tolist(),\n            'per_seed': [{k: v for k, v in r.items() if k != 'ckpt_path'}\n                         for r in eval_results_on],\n        },\n        'weight_divergence': {\n            'mean_divergences': on_mean_divs.tolist(),\n            'max_divergences': on_max_divs.tolist(),\n            'per_seed': divergence_on,\n        },\n    },\n    'field_off': {\n        'training_rewards': field_off_rewards.tolist(),\n        'training_populations': field_off_populations_train.tolist() if field_off_populations_train is not None else None,\n        'mean_reward': float(field_off_rewards.mean()),\n        'std_reward': float(field_off_rewards.std(ddof=1)),\n        'iqm': float(iqm_off.iqm),\n        'iqm_ci': [float(iqm_off.ci_lower), float(iqm_off.ci_upper)],\n        'eval': {\n            'populations': field_off_eval_populations.tolist(),\n            'total_rewards': field_off_eval_rewards.tolist(),\n            'per_seed': [{k: v for k, v in r.items() if k != 'ckpt_path'}\n                         for r in eval_results_off],\n        },\n        'weight_divergence': {\n            'mean_divergences': off_mean_divs.tolist(),\n            'max_divergences': off_max_divs.tolist(),\n            'per_seed': divergence_off,\n        },\n    },\n    'tests': {\n        'welch_t': float(welch.statistic),\n        'welch_p': float(welch.p_value),\n        'cohens_d': float(welch.effect_size),\n        'mann_whitney_u': float(mw.statistic),\n        'mann_whitney_p': float(mw.p_value),\n        'rank_biserial_r': float(mw.effect_size),\n        'prob_off_better': float(poi['prob_x_better']),\n        'divergence_welch_p': float(div_welch.p_value),\n        'divergence_cohens_d': float(div_welch.effect_size),\n    },\n}\n\n# Save JSON\njson_path = os.path.join(OUTPUT_DIR, 'field_on_vs_off_results.json')\nwith open(json_path, 'w') as f:\n    json.dump(results, f, indent=2)\nprint(f\"JSON saved: {json_path}\")\n\n# Save pickle (preserves numpy arrays)\npkl_path = os.path.join(OUTPUT_DIR, 'field_on_vs_off_results.pkl')\nwith open(pkl_path, 'wb') as f:\n    pickle.dump(results, f)\nprint(f\"Pickle saved: {pkl_path}\")\n\n# Save report markdown\nmd_path = os.path.join(OUTPUT_DIR, 'comparison_report.md')\nwith open(md_path, 'w') as f:\n    f.write(report)\nprint(f\"Report saved: {md_path}\")\n\n# List all output files\nprint(f\"\\n{'='*60}\")\nprint(\"ALL OUTPUT FILES:\")\nfor fname in sorted(os.listdir(OUTPUT_DIR)):\n    fpath = os.path.join(OUTPUT_DIR, fname)\n    size_mb = os.path.getsize(fpath) / 1024 / 1024\n    print(f\"  {fname} ({size_mb:.2f} MB)\")\nprint(f\"\\nDone! All results saved to {OUTPUT_DIR}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}