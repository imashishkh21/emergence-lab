# Phase 3 Progress: Specialization Detection

## Session Start
Date: 2026-02-02
Previous Phases:

## Phase 3 Goal
Detect when agents evolve into different "species" with distinct strategies.
Prove that field enables DIVERSITY, not just higher reward.

## Key Insight from Phase 2
- Random field HURTS agents (585 < 600) → they learned to READ field
- Field might enable specialization, not just coordination

---

## Task Log

### US-001: Weight Divergence Metric ✓
**What:** Implemented `compute_weight_divergence()` and `flatten_agent_params()` in `src/analysis/specialization.py`.
**Decisions:**
- Used cosine distance (1 - cosine_similarity) as the divergence metric — standard for comparing high-dimensional weight vectors
- Supports alive_mask to only compare living agents
- Returns dict with mean_divergence, max_divergence, full divergence_matrix, and agent_indices
- Flattens all pytree leaves per-agent into a single 1D vector for comparison
**Files changed:**
- `src/analysis/specialization.py` (new) — `flatten_agent_params`, `compute_weight_divergence`
- `tests/test_specialization.py` (new) — 10 tests covering identical/different agents, symmetry, alive mask, edge cases, multi-leaf params
**Validation:** mypy clean, 184/184 tests pass

---

### US-002: Behavioral Feature Extraction ✓
**What:** Implemented `extract_behavior_features(trajectory)` in `src/analysis/specialization.py` that extracts 7 behavioral features per agent from trajectory data.
**Decisions:**
- Trajectory input is a simple dict of numpy arrays (actions, positions, rewards, alive_mask, energy, optional births/field_values) — keeps it generic and decoupled from US-003's TrajectoryRecorder
- 7 features per agent: movement_entropy, food_collection_rate, distance_per_step, reproduction_rate, mean_energy, exploration_ratio, action_stay_fraction
- Movement entropy normalized to [0,1] using scipy.stats.entropy / log(num_actions)
- Reproduction rate: uses explicit 'births' key if available, otherwise infers from action 5 count
- Dead agents (never alive in trajectory) get all-zero feature vectors
- Only alive steps are used for all metrics (via alive_mask filtering)
- Stay fraction counts both action 0 (stay) and action 5 (reproduce) since both keep agent stationary
- Added helper functions `_movement_entropy()`, `_distance_traveled()`, `_exploration_ratio()` for modularity
**Files changed:**
- `src/analysis/specialization.py` — added `extract_behavior_features`, `_movement_entropy`, `_distance_traveled`, `_exploration_ratio`; added `scipy.stats.entropy` import
- `tests/test_specialization.py` — added `TestBehaviorFeatures` class with 15 tests covering shape, finiteness, dead agents, deterministic/uniform entropy, food rate, distance, reproduction from births/actions, energy, exploration, stay fraction, partial alive, different agents
**Validation:** mypy clean, 199/199 tests pass

---

### US-003: Trajectory Recording ✓
**What:** Implemented `TrajectoryRecorder` class and `record_episode()` function in `src/analysis/trajectory.py` for recording per-agent, per-step data during evaluation episodes.
**Decisions:**
- `TrajectoryRecorder` is a simple append-based collector that stores per-step numpy arrays, then stacks them into a trajectory dict via `get_trajectory()`
- Accepts both JAX arrays and numpy arrays (converts to numpy on record)
- Optional fields: `births` (bool) and `field_values` (float) — only included in output if provided
- `record_episode()` runs a single episode using the same pattern as `video.py`'s `record_episode` (single env, Python loop)
- Supports both stochastic (default) and deterministic action modes via `deterministic` kwarg
- Births detected by comparing `agent_alive` before and after step (new alive = birth)
- Field values recorded as mean across channels at agent positions (using `read_local` with radius=0)
- `alive_mask` captures pre-step alive state (consistent with `extract_behavior_features` expectations)
- Output format directly compatible with `extract_behavior_features()` from US-002
**Files changed:**
- `src/analysis/trajectory.py` (new) — `TrajectoryRecorder` class, `record_episode()` function
- `tests/test_specialization.py` — added `TestTrajectoryRecording` class with 13 tests covering: basic shapes, num_steps property, empty raises, optional births/field_values, data preservation, JAX array acceptance, compatibility with behavior features, record_episode runs/shapes/births+field/deterministic/features pipeline/alive mask
**Validation:** mypy clean (31 files), 212/212 tests pass

---

### US-004: Behavioral Clustering ✓
**What:** Implemented `cluster_agents()` and `find_optimal_clusters()` in `src/analysis/specialization.py` for K-means-based behavioral clustering with silhouette score evaluation.
**Decisions:**
- Features are standardized (StandardScaler: zero mean, unit variance) before clustering so all features contribute equally regardless of scale
- `cluster_agents()` uses sklearn KMeans with n_init=10 for robust convergence
- n_clusters is clamped to the number of unique data points to avoid degenerate K-means
- Silhouette score guard: returns 0.0 when n_samples <= n_labels (sklearn requires n_samples > n_labels)
- `find_optimal_clusters()` tries k=2..min(max_k, n_unique) and picks the k with the highest silhouette score
- Degenerate cases (all identical features, single agent) gracefully return k=1 with silhouette=0.0
- Returns dict with labels, centroids (in standardized space), silhouette score, and n_clusters
**Files changed:**
- `src/analysis/specialization.py` — added `cluster_agents`, `find_optimal_clusters`; added sklearn imports (KMeans, silhouette_score, StandardScaler)
- `tests/test_specialization.py` — added `TestClustering` class with 15 tests covering: required keys, label shapes, centroid shapes, silhouette range, well-separated clusters, identical features, two agents, k clamping, find_optimal keys, correct k detection, silhouette_scores dict, identical data, single agent, reproducibility, full pipeline
**Validation:** mypy clean (31 files), 227/227 tests pass

---

### US-005: Specialization Score ✓
**What:** Implemented `specialization_score()` and `novelty_score()` in `src/analysis/specialization.py`.
**Decisions:**
- `specialization_score()` combines three normalized [0,1] components with configurable weights:
  - Silhouette component (weight 0.5): optimal clustering silhouette score clamped to [0,1] (negative values = poor clustering → 0)
  - Weight divergence component (weight 0.25): mean cosine distance / 2.0 (cosine distance range is [0,2])
  - Behavioral variance component (weight 0.25): tanh(mean variance of standardized features) — maps naturally to [0,1]
- When `agent_params` is None, divergence weight is redistributed equally to silhouette and variance components
- `novelty_score()` implements Lehman & Stanley (2011) k-NN distance metric
  - Uses `scipy.spatial.distance.cdist` for efficient Euclidean distance computation
  - Uses `np.argpartition` for O(n) k-nearest selection vs O(n log n) full sort
  - Gracefully handles edge cases: empty archive, k=0, k > archive_size
**Files changed:**
- `src/analysis/specialization.py` — added `specialization_score`, `novelty_score`; added `scipy.spatial.distance.cdist` import
- `tests/test_specialization.py` — added `TestSpecializationScore` (11 tests) and `TestNoveltyScore` (9 tests) covering: required keys, score range, identical/separated clusters, single agent, with/without params, component ranges, diversity ordering, custom weights, pipeline, output shape, non-negativity, identical-to-archive, novel agent, empty archive, k edge cases, single agent/archive, k=0
**Validation:** mypy clean (31 files), 247/247 tests pass

---

### US-006: Field Usage Analysis ✓
**What:** Implemented `analyze_field_usage()` and `_classify_cluster_role()` in `src/analysis/specialization.py`.
**Decisions:**
- Since all alive agents write to the field automatically (not action-dependent), "write frequency" = fraction of steps an agent is alive
- Per-cluster statistics computed: write_frequency, mean_field_value, field_value_std, movement_rate, spatial_spread, field_action_correlation
- Movement rate: fraction of alive steps where position changed (distinguishes movers from sitters)
- Spatial spread: unique positions / alive steps (how spread out an agent's trajectory is)
- Field-action correlation: Pearson correlation between field_value[t] and whether agent moved at step t+1 — captures whether agents are "guided" by the field
- Role classification heuristic: writers = high movement + low field values, readers = low movement + high field values, balanced = neither extreme
- Gracefully handles missing field_values key (returns 0.0 for field-based stats)
- Dead agents contribute zero to all metrics
**Files changed:**
- `src/analysis/specialization.py` — added `analyze_field_usage`, `_classify_cluster_role`
- `tests/test_specialization.py` — added `TestFieldUsage` class with 15 tests covering: required keys, num_clusters, per-cluster stats, cluster roles, write_frequency range, all-alive frequency, dead agent stats, stationary/moving agents, reader/writer detection, without field_values, single cluster, spatial_spread range, field_action_correlation range, full pipeline
**Validation:** mypy clean (31 files), 262/262 tests pass

---

### US-007: Specialization Tracker ✓
**What:** Implemented `SpecializationTracker` class and `SpecializationEvent` dataclass in `src/analysis/specialization.py`, integrated into `src/training/train.py`.
**Decisions:**
- Follows the `EmergenceTracker` pattern: rolling window z-score detection for phase transitions
- Tracks three metrics per update: `weight_divergence` (mean cosine distance), `max_divergence`, and `num_alive`
- Detects "specialization events" when any metric deviates beyond `z_threshold` (default 3.0) standard deviations from the rolling window mean (default window_size=20)
- Added `specialization_check_interval: int = 20000` to `AnalysisConfig` — set to 2x emergence_check_interval since weight divergence computation requires flattening all agent params
- `get_metrics()` returns dict with `specialization/` prefix for W&B logging
- `get_summary()` returns final/mean/std for each metric plus all detected events
- Integration in train.py: only runs when `evolution.enabled`, uses first env's per-agent params and alive mask, logs to tqdm and W&B
- Prints specialization summary after training (final divergence, mean divergence, events)
- Added `import numpy as np` to train.py (was missing, needed for `np.asarray` of alive mask)
**Files changed:**
- `src/configs.py` — added `specialization_check_interval: int = 20000` to `AnalysisConfig`
- `src/analysis/specialization.py` — added `SpecializationEvent` dataclass, `SpecializationTracker` class with `update()`, `get_metrics()`, `get_summary()`; added `from src.configs import Config` import
- `src/training/train.py` — imported `SpecializationTracker` and `numpy`, instantiated tracker, added specialization check block in training loop, added specialization summary printing
- `tests/test_specialization.py` — added `TestSpecializationTracker` class with 15 tests covering: init, update returns, step count increment, history recording, identical/different agent divergence, alive count tracking, get_metrics keys and finiteness, get_summary keys, sudden divergence event detection, steady divergence no events, JAX alive mask acceptance, event str output, history growth
**Validation:** mypy clean (31 files), 277/277 tests pass

---

### US-008: Lineage-Strategy Correlation ✓
**What:** Implemented `correlate_lineage_strategy()` in `src/analysis/specialization.py` to analyze whether agents from the same lineage (shared ancestry) cluster into the same behavioral strategies.
**Decisions:**
- A "lineage" is defined by the root ancestor (parent_id == -1) — each agent is traced upward through the parent chain to its root
- Computes per-lineage "homogeneity": fraction of members in the most common cluster (1.0 = all in same cluster)
- Only computes homogeneity for lineages with 2+ members in the agent set (single-member lineages are excluded from homogeneity stats)
- "Specialist lineage" threshold: >= 70% of members in the same cluster (consistent with PRD species detection criteria)
- Specialist lineages are sorted by homogeneity descending for easy inspection
- Agents not found in the LineageTracker are treated as their own root (graceful handling of unknown agents)
- Returns comprehensive dict: lineage_cluster_map, lineage_homogeneity, specialist_lineages, mean_homogeneity, num_lineages, num_specialist_lineages
- Added `from src.analysis.lineage import LineageTracker` import to specialization.py
**Files changed:**
- `src/analysis/specialization.py` — added `correlate_lineage_strategy()` function and `LineageTracker` import
- `tests/test_specialization.py` — added `TestLineageCorrelation` class with 14 tests covering: required keys, perfect alignment (homogeneity=1.0), mixed clusters (homogeneity<1), specialist detection, dominant cluster reporting, num_lineages count, cluster_map structure, single-agent lineage exclusion, grandchild root tracing, unknown agent handling, homogeneity range, list agent_ids, sorted specialists, full pipeline (cluster_agents → correlate_lineage_strategy)
**Validation:** mypy clean (31 files), 291/291 tests pass

---

### US-009: Diversity vs Performance Ablation ✓
**What:** Implemented specialization ablation comparing populations with divergent (specialized) weights vs uniform (cloned) weights vs random weights, plus a script to train and run the comparison.
**Decisions:**
- Added 3 weight conditions to compare: `divergent` (trained per-agent weights), `uniform` (all agents cloned to mean of alive agents' weights), `random_weights` (mean weights + Gaussian noise)
- `_make_uniform_params()` averages alive agents' weights and broadcasts to all slots — tests whether weight diversity matters
- `_make_random_params()` adds i.i.d. Gaussian noise (std=0.1) to mean weights — tests whether learned divergence is better than random noise
- `SpecializationAblationResult` extends the existing ablation pattern with food_collected and population_stability metrics
- `population_stability` measured as std of alive population count over time per episode (lower = more stable)
- `_run_specialization_episode()` tracks food collected via before/after food_collected diff at each step
- `print_specialization_ablation_results()` prints a formatted table and computes divergent-vs-uniform and divergent-vs-random comparisons with interpretive text
- `scripts/run_specialization_ablation.py` follows the pattern of `scripts/run_ablation.py`: trains briefly, computes pre-ablation specialization score, runs 3-condition ablation, prints results
- Script supports `--skip-training` + `--checkpoint` for loading pre-trained models, and saves checkpoint with both shared params and per-agent params
- Pre-ablation analysis reports weight divergence, specialization score breakdown, and optimal cluster count
**Files changed:**
- `src/analysis/ablation.py` — added `SpecializationAblationResult`, `_SpecEpisodeStats`, `_make_uniform_params`, `_make_random_params`, `_replace_agent_params`, `_run_specialization_episode`, `specialization_ablation_test`, `print_specialization_ablation_results`, `WeightCondition` type alias; added `Any` to typing imports; updated module docstring
- `scripts/run_specialization_ablation.py` (new) — full training + specialization analysis + ablation comparison script
**Validation:** mypy clean (31 files), 291/291 tests pass, script produces comparison output

---

### US-010: Specialization Visualization ✓
**What:** Implemented 4 plotting functions in `src/analysis/visualization.py` that produce PNG files from existing analysis data structures.
**Decisions:**
- `matplotlib.use("Agg")` set before any pyplot import for headless support
- `_save_and_close(fig, output_path)` helper: creates parent dirs, saves at dpi=150, always closes figure to prevent memory leaks, returns figure for test inspection
- `plot_behavior_clusters()`: PCA (default) or t-SNE to reduce 7D features to 2D scatter plot; PCA labels include explained variance %; t-SNE uses `perplexity=min(30, n_samples-1)`; edge case for `n_samples < 2` plots raw first 2 features
- `plot_weight_divergence_over_time()`: solid line for mean divergence with fill_between, dashed line for optional max divergence; grid enabled
- `plot_field_usage_by_cluster()`: grouped bar chart with 5 metrics (write_frequency, mean_field_value, movement_rate, spatial_spread, field_action_correlation) × N clusters; legend labels include cluster role (writer/reader/balanced)
- `plot_specialization_score_over_time()`: thick red line for composite score, dashed lines for optional components (silhouette/divergence/variance); y-axis fixed [0, 1.05]; gray dotted midpoint at 0.5
- Constants: `BEHAVIOR_FEATURE_NAMES` (7 names matching `extract_behavior_features` output order), `CLUSTER_COLORS` (tab10 palette)
- `__init__.py` left unchanged (project convention: import from submodules directly)
**Files changed:**
- `src/analysis/visualization.py` (new) — `_save_and_close`, `plot_behavior_clusters`, `plot_weight_divergence_over_time`, `plot_field_usage_by_cluster`, `plot_specialization_score_over_time`; constants `BEHAVIOR_FEATURE_NAMES`, `CLUSTER_COLORS`
- `tests/test_visualization.py` (new) — 18 tests across 4 classes: `TestPlotBehaviorClusters` (5 tests: figure type, PNG output, PCA axis labels, t-SNE method, single cluster), `TestPlotWeightDivergenceOverTime` (4 tests: figure type, PNG output, with/without max divergence), `TestPlotFieldUsageByCluster` (4 tests: figure type, PNG output, multiple clusters, roles in legend), `TestPlotSpecializationScoreOverTime` (5 tests: figure type, PNG output, y-axis bounds, with/without components)
**Validation:** ruff clean, mypy clean, black formatted, 309/309 tests pass

---

### US-011: Species Detection ✓
**What:** Implemented `detect_species()` function and `Species` dataclass in `src/analysis/specialization.py` for formally detecting when distinct "species" have emerged in the population.
**Decisions:**
- A "species" requires two conditions: (1) clear cluster boundaries (overall silhouette >= threshold, default 0.7), and (2) hereditary membership (>= 70% of parent-child pairs in same cluster)
- `Species` dataclass stores: cluster_id, num_members, agent_indices, centroid (standardized space), silhouette, heredity_score, mean_features (raw space), and optional role label
- Uses `find_optimal_clusters()` internally to determine optimal number of clusters and get silhouette score
- Global heredity_score computed as fraction of all parent-child pairs (where both are in the agent set) that share a cluster
- Per-cluster heredity computed separately — each cluster must individually pass the 70% heredity threshold to qualify as a species
- When no `lineage_tracker` is provided, heredity check is skipped (all qualifying clusters are treated as species, heredity=1.0 assumed)
- Returns comprehensive dict: species list, num_species, silhouette, optimal_k, all_labels, heredity_score, is_speciated bool
- `is_speciated` is True when at least one species is detected
**Files changed:**
- `src/analysis/specialization.py` — added `Species` dataclass and `detect_species()` function
- `tests/test_specialization.py` — added `TestSpeciesDetection` class with 14 tests covering: required keys, well-separated clusters detected, identical agents no species, species object attributes, all_labels shape, species cover agents, heredity with lineage tracker, heredity zero without lineage, high threshold reduces species, single agent, non-hereditary clusters excluded, mean features correct, silhouette matches clustering, full pipeline
**Validation:** mypy clean (32 files), 323/323 tests pass

---

### US-012: Specialization Report ✓
**What:** Created `scripts/generate_specialization_report.py` that generates a comprehensive markdown report with visualizations from a trained model's specialization analysis.
**Decisions:**
- Script follows the same pattern as `run_specialization_ablation.py`: supports both training from scratch and loading from checkpoint
- Report generation pipeline: (1) load/train model → (2) compute weight divergence → (3) record trajectories → (4) extract features & compute specialization score → (5) detect species → (6) analyze field usage → (7) run ablation (optional) → (8) generate visualizations → (9) write markdown report
- Multiple trajectory episodes (default 5) are recorded and averaged for more stable behavioral clustering
- Tracks specialization metrics during training (when training from scratch) to produce time-series plots
- Generates 5 PNG figures: PCA clusters, t-SNE clusters, weight divergence over time, field usage by cluster, specialization score over time
- Markdown report includes tables for: specialization score breakdown, species characteristics, field usage per cluster, lineage correlations (when available), and ablation results (when not skipped)
- Supports `--skip-ablation` for faster report generation, `--output-dir` for custom output location
- Report includes interpretive text (conclusions from ablation comparison)
**Files changed:**
- `scripts/generate_specialization_report.py` (new) — full report generation script with 5 phases: load/train, analysis, ablation, visualization, report writing
**Validation:** mypy clean (32 files), 323/323 tests pass, script produces report with markdown + 5 PNG figures

---

### US-013: Update Training to Encourage Specialization ✓
**What:** Added `SpecializationConfig` with three config options that encourage specialization: `diversity_bonus`, `niche_pressure`, and `layer_mutation_rates`. Implemented reward modification in `train_step` and per-layer mutation in reproduction.
**Decisions:**
- Created `SpecializationConfig` dataclass with three fields: `diversity_bonus` (default 0.0), `niche_pressure` (default 0.0), `layer_mutation_rates` (default None)
- **Diversity bonus**: Computes per-agent cosine distance from other alive agents in weight space; adds scaled bonus to rewards before GAE. Rewards agents with more unique weights, creating gradient pressure to maintain diversity.
- **Niche pressure**: Computes minimum cosine distance to nearest neighbor; penalizes agents too similar to their closest neighbor. `penalty = niche_pressure * (1 - min_dist)`, encouraging weight differentiation.
- Both bonuses computed inside JIT-compiled `train_step` using pure JAX operations (no numpy/scipy). The `_compute_specialization_bonuses` function uses vectorized cosine distance with alive masking.
- Bonuses are vmapped over environments and added to every timestep's rewards (broadcast from per-env per-agent snapshot to `(T, num_envs, max_agents)`).
- **Per-layer mutation rates**: `layer_mutation_rates` maps layer name substrings (e.g., `"Dense_0"`, `"Dense_2"`) to custom mutation std values. Uses `jax.tree_util.tree_flatten_with_path` to match leaf paths.
- Added `compute_per_leaf_mutation_rates()` to compute per-leaf rate tuples, and `mutate_agent_params_layered()` to apply them during reproduction.
- Per-leaf rates are pre-computed at trace time in `env.py` (Python-level, not traced) and captured in the scan closure, so they work within JIT.
- All features are opt-in (defaults are 0/None), so existing behavior is unchanged when not configured.
- Added `specialization` field to `Config` master dataclass and `from_yaml()` support.
**Files changed:**
- `src/configs.py` — added `SpecializationConfig` dataclass, added `specialization` field to `Config`, updated `from_yaml()`
- `src/training/train.py` — added `_compute_specialization_bonuses()` helper, integrated reward modification in `train_step`, added specialization config display in training banner
- `src/agents/reproduction.py` — added `compute_per_leaf_mutation_rates()`, `mutate_agent_params_layered()`
- `src/environment/env.py` — imported new reproduction functions, added conditional layered mutation in reproduction scan
- `scripts/verify_specialization_training.py` (new) — verification script testing all specialization features
**Validation:** mypy clean (32 files), 323/323 tests pass, verification script confirms all configs run without errors and bonuses are computed correctly

---

### US-014: Integration Test ✓
**What:** Added `TestPhase3Integration` class with `test_specialization_emerges` to `tests/test_integration.py` — end-to-end test of the full Phase 3 specialization detection pipeline.
**Decisions:**
- Test follows the established `TestPhase2Integration` pattern: small config, train a few updates, then run the full analysis pipeline
- Pipeline tested end-to-end: create_train_state → train_step (multiple updates) → record_episode → extract_behavior_features → specialization_score → compute_weight_divergence → cluster_agents → find_optimal_clusters → SpecializationTracker
- Verifies per-agent params are initialized when evolution is enabled
- Verifies trajectory recording returns correct structure and shapes (actions, positions, rewards, alive_mask, energy, births, field_values)
- Verifies behavioral features have correct shape (max_agents, 7) and all finite values
- Verifies specialization score result has all expected keys and score in [0, 1]
- Verifies weight divergence returns non-negative finite values
- Verifies clustering and optimal cluster finding work on alive agent features
- Verifies SpecializationTracker produces correct metrics and summary after multiple updates
- Compares trained model's specialization score against a random baseline (uniform random features) — both must be well-formed [0, 1] values
- 180-second timeout (slightly longer than Phase 2 tests due to trajectory recording overhead)
**Files changed:**
- `tests/test_integration.py` — updated module docstring for Phase 3, added `TestPhase3Integration` class with `test_specialization_emerges` test
**Validation:** mypy clean (32 files), 324/324 tests pass (1 new test added)

---

### US-015: Final Review ✓
**What:** Code quality review, documentation update, and final validation across all Phase 3 files.
**Decisions:**
- All 324 tests pass, mypy reports no errors across 32 source files
- Code quality review: all Phase 3 files (specialization.py, trajectory.py, visualization.py, ablation.py, reproduction.py, configs.py, train.py, env.py) follow existing project patterns, use proper type annotations, and have comprehensive docstrings
- Updated README.md with Phase 3 section covering: specialization detection overview, Phase 3 commands, specialization results/expected output, specialization parameters table, updated project structure with new files, updated tech stack (scikit-learn, matplotlib)
- Example outputs documented in README: specialization score breakdown, ablation comparison table, report generator output description
**Files changed:**
- `README.md` — added Phase 3 section (specialization detection, commands, results, parameters), updated project structure, updated tech stack
- `progress.txt` — added US-015 entry and completion signal
**Validation:** mypy clean (32 files), 324/324 tests pass

---

--- PHASE 3 COMPLETE ---


================================================================================
PHASE 4: Research Microscope
================================================================================
Started: Mon Feb  2 15:53:46 IST 2026

Goals:
- Fix gradient homogenization (agent-specific heads, freeze-evolve)
- Build live visualization dashboard (Svelte + Pixi.js)
- Implement emergence metrics (Transfer Entropy, DOL, Phase Detection)
- Make it understandable to non-technical users (tooltips, glossary)

---

## Task Log

### US-001: Agent-Specific Policy Heads ✓
**What:** Implemented `AgentSpecificActorCritic` class in `src/agents/network.py` — a shared encoder + per-agent output heads architecture. Added `agent_architecture` config option to `AgentConfig`.
**Decisions:**
- Architecture: shared MLP encoder (same Dense + LayerNorm + tanh layers) processes observations into features, then N separate actor heads (Dense → num_actions logits) and N separate critic heads (Dense → scalar value) produce per-agent outputs
- All heads are evaluated in the forward pass (all Dense layers run), then the correct head's output is selected via `jnp.stack` + array indexing by `agent_id`. This is JIT-compatible and avoids `jax.lax.switch` + Flax module tracer leak issues
- `agent_id` parameter is optional — when `None`, defaults to head 0 for backward compatibility with `ActorCritic` API
- Out-of-range `agent_id` values are safely clamped to `[0, n_agents-1]`
- Gradients only flow through the selected head's parameters (confirmed by test), while shared encoder receives gradients from all agents
- Named layers: `actor_head_{i}` and `critic_head_{i}` for clear parameter tree inspection
- Added `agent_architecture: Literal["shared", "agent_heads"]` to `AgentConfig` (default: "shared" for backward compatibility)
- Initial approach using `jax.lax.switch` with `_AgentHead` submodules failed due to Flax nn.Module creating parameters inside `switch` branches causing `UnexpectedTracerError`. Switched to stacked-output-then-index approach.
**Files changed:**
- `src/agents/network.py` — added `AgentSpecificActorCritic` class (import `jax` added)
- `src/configs.py` — added `agent_architecture` field to `AgentConfig`
- `tests/test_agent.py` — added `TestAgentSpecificHeads` class with 10 tests covering: output shapes, different agents different outputs, shared encoder structure, agent_id=None backward compat, out-of-range clamping, JIT compatibility, vmap over agents, gradient flow isolation, config field, different hidden dims
**Validation:** mypy clean (0 new errors; 3 pre-existing in train.py), 334/334 tests pass (10 new)

---

### US-002: Agent ID Embedding ✓
**What:** Added learnable agent identity embedding to both `ActorCritic` and `AgentSpecificActorCritic` networks. Each agent gets a unique embedding vector (a "name tag") that is concatenated to its observation before the encoder processes it, allowing the network to learn agent-specific behavior even with shared weights.
**Decisions:**
- Added `agent_embed_dim` field to `AgentConfig` (default: 0 = disabled). When >0, a `nn.Embed(n_agents, agent_embed_dim)` lookup table is created inside the network
- Embedding is concatenated to the observation vector before the first Dense layer, so the encoder input dim becomes `obs_dim + agent_embed_dim`
- Both `ActorCritic` and `AgentSpecificActorCritic` support the embedding via new `agent_embed_dim` and `n_agents` module attributes
- `ActorCritic.__call__` now accepts an optional `agent_id` parameter (default None). When `agent_id` is None or `agent_embed_dim=0`, no embedding is applied — full backward compatibility
- `AgentSpecificActorCritic` reuses its existing `agent_id` parameter for both head selection and embedding lookup, with safe clamping
- When `agent_embed_dim > 0`, the network must be initialized with an `agent_id` to properly shape the embedding params and first Dense layer. Subsequent calls also require `agent_id`
- Gradients flow only through the selected agent's embedding row (verified by test)
- The `obs.py` observation function is unchanged — the embedding is entirely internal to the network, not part of the environment observation
**Files changed:**
- `src/agents/network.py` — added `agent_embed_dim` and `n_agents` attributes to `ActorCritic`, added optional `agent_id` parameter to `ActorCritic.__call__`, added `agent_embed_dim` attribute to `AgentSpecificActorCritic`, both classes now optionally create and concatenate `nn.Embed` before encoding
- `src/configs.py` — added `agent_embed_dim: int = 0` to `AgentConfig`
- `tests/test_agent.py` — added `TestAgentEmbedding` class with 10 tests covering: output shapes with embedding, embedding param creation, no embedding when disabled, different agents produce different outputs, backward compat without embedding, agent-specific with embedding, JIT compatibility, vmap over agents, gradient flow through embedding table, config field
**Validation:** mypy clean (0 new errors; 3 pre-existing in train.py), 28/28 test_agent.py tests pass (10 new)

---

### US-003: Freeze-Evolve Training Mode ✓
**What:** Implemented alternating freeze-gradient and evolve-only training phases. Added `TrainingMode` enum (GRADIENT, EVOLVE, FREEZE_EVOLVE), `FreezeEvolveConfig` dataclass, and `evolve_step()` function to the training pipeline.
**Decisions:**
- Added `TrainingMode` enum with three modes: GRADIENT (default PPO), EVOLVE (pure evolution, no gradient updates), and FREEZE_EVOLVE (alternating cycles)
- `FreezeEvolveConfig` has three parameters: `gradient_steps` (default 10000), `evolve_steps` (default 1000), `evolve_mutation_boost` (default 5.0x multiplier on mutation_std during evolve phases)
- `evolve_step()` collects a rollout (agents still act using frozen shared policy) but skips all gradient computation — GAE, PPO loss, optimizer updates, and param syncing are all skipped
- During evolve phases, per-agent params diverge freely via reproduction + mutation in `env.step()`, with no shared-param sync overwriting them
- Mutation boost during evolve: a separate `evolve_config` is created using `dataclasses.replace()` with boosted `evolution.mutation_std`, and JIT-compiled separately. This keeps both configs static at JIT time (no traced values)
- Phase switching tracked in Python loop (not inside JIT): `phase_step_counter` tracks steps in current phase, switches when threshold exceeded
- Phase transitions logged to tqdm output and recorded for summary
- `evolve_step` returns zero placeholders for gradient metrics (total_loss, policy_loss, entropy, clip_fraction) so logging code doesn't break
- NaN/Inf check only runs during gradient phases (evolve always has zero loss)
- W&B logging includes `freeze_evolve/phase` metric (1.0=gradient, 0.0=evolve) for visualizing phase cycles
- `training_mode` field added to `TrainConfig` with default GRADIENT for full backward compatibility
- `freeze_evolve` field added to master `Config` with `from_yaml()` support
**Files changed:**
- `src/configs.py` — added `TrainingMode` enum, `FreezeEvolveConfig` dataclass, `training_mode` field to `TrainConfig`, `freeze_evolve` field to `Config`, updated `from_yaml()`
- `src/training/train.py` — added `evolve_step()` function, imported `TrainingMode`, updated training banner, modified JIT compilation to handle evolve step with boosted mutation config, added phase switching logic in training loop, added phase-aware logging and NaN checking, added freeze-evolve summary printing
- `tests/test_training.py` — added `TestFreezeEvolve` class with 13 tests covering: TrainingMode enum values, FreezeEvolveConfig defaults, Config integration, training mode configurability, evolve_step execution and metric keys, evolve_step no shared param change, evolve_step JIT compatibility, gradient mode train_step, mutation boost calculation, modified config creation, phase switching logic simulation, population metrics from evolve_step, full gradient→evolve→gradient cycle
**Validation:** mypy clean (0 new errors; 3 pre-existing in train.py), 357/357 tests pass (13 new)

---

### US-004: MAP-Elites Behavioral Archive ✓
**What:** Implemented `BehavioralArchive` class and `extract_descriptors()` function in `src/analysis/archive.py` — a MAP-Elites grid archive that stores the best-fitness agent params for each cell in a 2D behavioral descriptor space. Added `ArchiveConfig` to `src/configs.py`.
**Decisions:**
- Archive is a sparse 2D grid indexed by two behavioral descriptors: movement_entropy (axis 0, [0,1]) and field_write_frequency (axis 1, [0,1])
- Each cell stores at most one agent — the one with the highest fitness observed for that cell. New agents only replace if strictly higher fitness.
- `_descriptor_to_cell()` maps continuous [0,1]^2 descriptors to grid indices via `floor(value * grid_size)`, with clipping for edge cases (values at exactly 1.0, out-of-range)
- `sample(n)` uses without-replacement when n <= size, with-replacement when n > size
- `extract_descriptors()` computes the two MAP-Elites axes from trajectory data: movement_entropy (normalized Shannon entropy of action distribution) and field_write_frequency (fraction of total steps the agent was alive)
- `as_feature_array()` exports all occupied cells' descriptors for use with `novelty_score()` from specialization.py
- Helper methods: `coverage()`, `fitness_stats()`, `get_cell()`, `clear()` for archive inspection and management
- `ArchiveConfig` has `grid_size` (default 100) and `enabled` (default False) fields — opt-in feature that doesn't affect existing behavior
- Uses sparse dict storage (only occupied cells stored) rather than pre-allocated dense array
**Files changed:**
- `src/analysis/archive.py` (new) — `ArchiveEntry` dataclass, `BehavioralArchive` class with `add()`, `sample()`, `as_feature_array()`, `get_cell()`, `clear()`, `coverage()`, `fitness_stats()`; `extract_descriptors()` function
- `src/configs.py` — added `ArchiveConfig` dataclass, added `archive` field to `Config`, updated `from_yaml()`
- `tests/test_archive.py` (new) — 49 tests across 8 classes: `TestBehavioralArchiveInit` (6 tests), `TestBehavioralArchiveAdd` (8 tests), `TestBehavioralArchiveSample` (7 tests), `TestBehavioralArchiveQuery` (9 tests), `TestDescriptorMapping` (6 tests), `TestExtractDescriptors` (7 tests), `TestArchiveWithDescriptors` (3 tests: full pipeline, fitness preservation, novelty_score compatibility), `TestArchiveConfig` (3 tests)
**Validation:** mypy clean (0 new errors; 3 pre-existing in train.py), 406/406 tests pass (49 new)

---

### US-005: FastAPI WebSocket Server ✓
**What:** Created a FastAPI WebSocket server for real-time training visualization. Implemented `TrainingBridge` class for thread-safe communication between the training loop and the async server, plus a binary MessagePack streaming protocol at 30Hz.
**Decisions:**
- **Architecture:** `TrainingBridge` acts as a decoupled bridge — training loop calls `publish_frame()` from its thread, server's async loop reads via `get_latest_frame()`. Thread-safe via `threading.Lock` on the shared latest frame buffer.
- **Frame format:** `Frame` dataclass holds numpy arrays (positions, alive, energy, food, field, optional cluster labels) plus a metrics dict. All JAX arrays are converted to numpy at the bridge boundary.
- **Serialization:** `pack_frame()` converts Frame to MessagePack binary. Each numpy array is serialized as `{shape, dtype, data}` dict so clients can reconstruct typed arrays. MessagePack chosen over JSON for binary efficiency (field array alone is 20×20×4 float32 = 6.4KB).
- **Rate limiting:** `TrainingBridge` enforces target_fps (default 30Hz) — `publish_frame()` returns False if called too frequently, avoiding overwhelming the WebSocket.
- **Bidirectional commands:** Dashboard clients send JSON commands over the same WebSocket (pause, resume, set_param). Commands are queued in the bridge via `push_command()` and consumed by the training loop via `pop_commands()`.
- **Mock mode:** `python -m src.server.main --mock` generates synthetic training data for frontend development without running actual training.
- **CORS:** Allow all origins so dashboard at `localhost:5173` can connect to server at `localhost:8765`.
- **Endpoints:** `GET /` (health), `GET /config` (server state), `WS /ws/training` (binary frame stream).
- **`create_frame_from_state()`:** Convenience method on TrainingBridge that extracts data from batched JAX EnvState (takes first env) and converts metrics to plain floats, ready for publishing.
- Added `httpx` as implicit dev dependency (required by FastAPI's TestClient via starlette).
**Files changed:**
- `pyproject.toml` — added fastapi, uvicorn, websockets, msgpack to dependencies
- `src/server/__init__.py` (new) — package init
- `src/server/streaming.py` (new) — `Frame` dataclass, `pack_frame()`, `_pack_array()`, `TrainingBridge` class with publish/consume/command queue/create_frame_from_state
- `src/server/main.py` (new) — FastAPI app with health/config/WebSocket endpoints, `_handle_command()`, `_generate_mock_frame()`, `_mock_training_loop()`, `start_server()`, CLI entry point
- `tests/test_server.py` (new) — 38 tests across 8 classes: TestFrame (3), TestPackFrame (8), TestTrainingBridge (7), TestCommandQueue (7), TestFastAPIApp (3), TestWebSocket (2), TestMockData (5), TestCreateFrameFromState (3)
**Validation:** mypy clean (0 new errors; 3 pre-existing in train.py), 444/444 tests pass (38 new). Server starts, accepts WebSocket connections, and streams mock data at 30fps.

---

### US-006: Svelte Dashboard Scaffold ✓
**What:** Created a Svelte 5 dashboard application with WebSocket client, MessagePack decoding, reactive state store, and basic layout (header, canvas area, metrics panel, controls). The dashboard connects to the FastAPI server and displays live training data.
**Decisions:**
- **Svelte 5 with runes:** Used `$state`, `$derived`, `$effect`, and `$props` runes for reactive state management. Store file uses `.svelte.js` extension to enable rune syntax outside components.
- **WebSocket client with auto-reconnect:** Exponential backoff reconnection (1s initial, 1.5x multiplier, 10s max). Binary `arraybuffer` mode for MessagePack frames. JSON text commands for pause/resume/set_param.
- **MessagePack decoding:** `unpackArray()` reconstructs typed arrays (Float32Array, Int8Array, Uint8Array) from the server's `{shape, dtype, data}` format. Handles `<f4`, `<i1`, `|u1`/`<u1` dtype strings.
- **Reactive state store:** `createTrainingStore()` returns a reactive object with getters for all frame data, derived values (aliveCount, maxAgents), rolling metrics history (1000-point window), and methods for connection/control.
- **Canvas 2D renderer:** Temporary Canvas 2D implementation for agent visualization (will be upgraded to Pixi.js in US-007). Renders field heatmap, grid lines, food dots, and agents colored by cluster with energy-based sizing and glow effects.
- **Layout:** Dark theme (matches research tool aesthetic). Header with step counter and connection status. Main area split between canvas (flexible) and sidebar (340px fixed). Sidebar contains metrics panel and control panel.
- **Package versions:** Svelte 5, `@sveltejs/vite-plugin-svelte@^5.0.0` (requires vite ^6), `vite@^6.0.0`, `msgpack-lite` for MessagePack decoding.
- **Glossary data:** Created `static/glossary.json` with all terms from the PRD glossary section for the future help system (US-013).
**Files changed:**
- `dashboard/package.json` (new) — project config with svelte 5, vite 6, msgpack-lite dependencies
- `dashboard/vite.config.js` (new) — Vite config with Svelte plugin
- `dashboard/svelte.config.js` (new) — Svelte preprocessor config
- `dashboard/index.html` (new) — HTML entry point
- `dashboard/public/favicon.svg` (new) — SVG favicon with agent dots
- `dashboard/src/main.js` (new) — Svelte mount entry point
- `dashboard/src/App.svelte` (new) — Root component with layout, auto-connect
- `dashboard/src/stores/training.svelte.js` (new) — Reactive store: WebSocket client, MessagePack decoder, frame state, metrics history, control methods
- `dashboard/src/lib/Header.svelte` (new) — Header with title, step counter, connection status indicator
- `dashboard/src/lib/AgentCanvas.svelte` (new) — Canvas 2D renderer: field heatmap, food, agents with cluster colors, energy sizing, disconnected overlay
- `dashboard/src/lib/MetricsPanel.svelte` (new) — Metrics display with tooltips: reward, loss, entropy, population, energy, alive/max/fps summary
- `dashboard/src/lib/ControlPanel.svelte` (new) — Pause/resume button with status hints
- `dashboard/static/glossary.json` (new) — Glossary terms for help system
**Validation:** mypy clean (0 new errors; 3 pre-existing in train.py), 444/444 tests pass (no new Python tests — this is a frontend task). `npm run build` succeeds. `npm run dev` starts and serves the dashboard at http://localhost:5173. Dashboard displays "Waiting for server connection..." overlay when server is not running.

---

### US-007: Pixi.js Agent Renderer ✓
**What:** Replaced the Canvas 2D renderer with a Pixi.js v8 WebGPU/WebGL renderer. Created `dashboard/src/lib/renderer.js` as a standalone rendering engine, and rewrote `AgentCanvas.svelte` to use it. Added smooth position interpolation, optional trail effects, and a glow effect for agents.
**Decisions:**
- **Pixi.js v8 async init:** Used `new Application()` + `await app.init({ preference: "webgpu" })` pattern as required by v8. Falls back to WebGL automatically if WebGPU is unavailable.
- **Scene graph layers:** 6 layers in render order: fieldLayer (heatmap), trailLayer (movement trails), gridLayer (grid lines), foodLayer (food dots), agentLayer (agent circles), overlayLayer (disconnected state). Each layer is a separate `Container` for clean separation.
- **Pre-allocated sprite pool:** 64 `Graphics` objects pre-created in agentLayer (MAX_POOL_SIZE=64). Objects are shown/hidden via `.visible` — no dynamic creation/destruction during rendering.
- **Position interpolation:** Lerp factor 0.3 — each frame, agents move 30% of the remaining distance to their target position. `interpX`/`interpY` Float32Arrays track interpolated positions. First-frame or respawned agents snap immediately (no lerp).
- **Food rendering:** Uses `GraphicsContext` (shared geometry) for food dots — one circle template, multiple `Graphics` instances. Pool expands on demand via `ensureFoodPool()`.
- **Field heatmap:** Per-cell `Graphics` objects that are cleared and redrawn each frame. Sums across all field channels, normalizes to 0-1 intensity, renders as blue rectangles with alpha = intensity * 0.6.
- **Agent glow effect:** Two concentric circles per agent — outer circle at 1.5x radius with 0.15 alpha (glow), inner circle at full opacity. Color determined by cluster ID.
- **Trail effect:** Optional toggle (button in canvas-controls). Stores trail points as `{ x, y, color, age }` array. Points age out after 60 frames. Max 2000 trail points. Alpha fades linearly with age.
- **Overlay:** Moved from Pixi overlay to CSS overlay with `pointer-events: none` — simpler and avoids Pixi Text rendering overhead.
- **Renderer API:** `createRenderer()` returns `{ update, resize, destroy, setShowTrails, canvas, showTrails }` — fully decoupled from Svelte component.
- **Cleanup:** `$effect` cleanup function calls `renderer.destroy()` which runs `app.destroy(true, { children: true, texture: true })` to free all GPU resources.
**Files changed:**
- `dashboard/package.json` — added `pixi.js: ^8.15.0` to dependencies
- `dashboard/src/lib/renderer.js` (new) — Pixi.js v8 rendering engine with field heatmap, food dots, agent sprites with lerp interpolation, trail effect, and glow
- `dashboard/src/lib/AgentCanvas.svelte` — rewrote to use renderer.js, added trail toggle button, moved disconnected overlay to CSS
**Validation:** mypy clean (0 new errors; 3 pre-existing in train.py), 444/444 tests pass (no new Python tests — frontend task). `npm run build` succeeds (852 modules, 1.37s). Pixi.js v8 tree-shakes to ~294KB gzipped main bundle + ~37KB WebGPU renderer chunk + ~63KB WebGL renderer chunk.

---

### US-008: Real-Time Metrics Charts ✓
**What:** Replaced the static metrics display in MetricsPanel.svelte with 4 live-updating Plotly.js charts (Reward, Weight Divergence, Specialization Score, Population). Added `plotly.js-dist-min` dependency. Updated mock server to stream `weight_divergence` and `specialization_score` metrics with simulated gradual improvement over time.
**Decisions:**
- **Plotly.js with WebGL (scattergl):** Used `scattergl` trace type for GPU-accelerated rendering of time-series lines, matching the PRD requirement. Plotly is dynamically imported via `import("plotly.js-dist-min")` to avoid SSR issues and enable code splitting.
- **Efficient incremental updates:** Uses `Plotly.extendTraces()` for appending new data points (O(1) per frame). When the rolling window (1000 points) is exceeded, switches to a full `Plotly.react()` call with trimmed data to prevent unbounded memory growth.
- **Rolling window tracking:** A `lastHistoryLen` counter tracks which history entries have been charted, so only new entries are processed each reactive cycle. This avoids re-processing the entire history every frame.
- **Chart per metric:** 4 charts stacked vertically in the sidebar: Reward (#4ecdc4 teal), Weight Divergence (#ff6b6b red), Specialization (#ffd93d gold, fixed y-axis 0–1), Population (#6bcb77 green). Each has current value displayed next to the title.
- **Dark theme layout:** Transparent backgrounds, gray grid lines, minimal margins (l:40, r:8, t:4, b:24) to fit the 340px sidebar. No mode bar, no legend.
- **Helper tooltips:** Each chart has an info icon (i) with title attribute explaining the metric in plain English (e.g., "Are agents getting better at collecting food?"). Hover on chart points shows value + step.
- **Mock server improvements:** Mock frame metrics now simulate gradual improvement over 500 steps (reward increases, loss decreases, divergence grows from 0→0.12, specialization from 0→0.75). This makes the dashboard demo more realistic.
- **Summary row preserved:** Alive/Max/FPS summary cards remain at the bottom of the panel below the charts.
**Files changed:**
- `dashboard/package.json` — added `plotly.js-dist-min: ^2.30.0` to dependencies
- `dashboard/src/lib/MetricsPanel.svelte` — rewrote from static metric cards to 4 live Plotly.js charts with dark theme, incremental updates, rolling window, and helper tooltips
- `src/server/main.py` — added `weight_divergence` and `specialization_score` to mock frame metrics with gradual improvement simulation
**Validation:** mypy clean (0 new errors; 3 pre-existing in train.py), 444/444 tests pass (no regressions). `npm run build` succeeds (855 modules). Plotly.js chunk is ~1.4MB gzipped (dynamically loaded, does not block initial render).

---

### US-009: Training Controls ✓
**What:** Added pause/resume, speed control, mutation rate slider, diversity bonus slider, and training mode indicator to the dashboard control panel. Extended the server's command protocol with `set_speed` command and added `training_mode` to the frame streaming protocol.
**Decisions:**
- **Speed control via button group:** 5 speed options (0.5x, 1x, 2x, 4x, 8x) as toggle buttons rather than a slider — discrete options are clearer for users and avoid ambiguous intermediate values. Speed multiplier clamped to [0.25, 16.0] on the bridge.
- **Training mode indicator:** Colored badge showing current phase — teal for "Learning" (gradient), gold for "Evolving" (evolve), gray for "Paused". Uses emoji icons (brain, DNA, pause) per PRD spec. Mode is streamed in every frame from server.
- **Parameter sliders:** Mutation rate (0.001–0.1, step 0.001) and diversity bonus (0–1.0, step 0.01) with human-readable labels ("Stable" to "Chaotic", "None" to "Strong"). Each slider has an info icon (i) with tooltip explaining the parameter in plain English.
- **Frame protocol extension:** Added `training_mode` string field to `Frame` dataclass (default "gradient"), included in MessagePack serialization via `pack_frame()`. This allows the dashboard to display the current training phase without polling.
- **Bridge state extension:** Added `speed_multiplier` (float, default 1.0) and `training_mode` (string, default "gradient") properties to `TrainingBridge`. Speed setter clamps to [0.25, 16.0].
- **`set_speed` command:** New command type handled by `_handle_command()` in server. Sets `bridge.speed_multiplier` directly. Forwarded to training loop via command queue.
- **Mock server enhancement:** Mock training loop now simulates freeze-evolve cycles (200 gradient steps, 50 evolve steps) and responds to speed multiplier by advancing steps proportionally. Training mode is updated on the bridge and included in mock frames.
- **Store updates:** Added `speedMultiplier`, `trainingMode` state variables and `setSpeed()` method to the training store. `processFrame()` extracts `training_mode` from incoming frames.
- **Config endpoint:** `/config` now returns `speed_multiplier` and `training_mode` in addition to existing fields.
**Files changed:**
- `src/server/streaming.py` — added `training_mode` field to `Frame`, added to `pack_frame()`, added `speed_multiplier` and `training_mode` properties to `TrainingBridge`
- `src/server/main.py` — added `set_speed` command handling, updated `/config` endpoint, updated mock frame generation and mock training loop with mode cycling and speed response
- `dashboard/src/stores/training.svelte.js` — added `speedMultiplier`, `trainingMode` state, `setSpeed()` method, frame processing for `training_mode`
- `dashboard/src/lib/ControlPanel.svelte` — full rewrite with mode indicator, play/pause, speed buttons, mutation rate slider, diversity bonus slider, info tooltips
- `tests/test_server.py` — added `TestTrainingControls` class with 16 tests covering: speed multiplier defaults/setting/clamping, training mode defaults/setting, set_speed command handling, frame training_mode, packed frame includes mode, config endpoint, mock frames, WebSocket speed and set_param commands
**Validation:** mypy clean (0 new errors; 3 pre-existing in train.py), 460/460 tests pass (16 new). `npm run build` succeeds (855 modules).

---

### US-010: Transfer Entropy Metric ✓
**What:** Implemented transfer entropy computation between agent pairs using the KSG (Kraskov-Stögbauer-Grassberger) k-nearest-neighbor estimator. Created `src/analysis/information.py` with core TE functions, pairwise TE matrix computation, trajectory convenience wrapper, and a `TransferEntropyTracker` class for training loop integration. Added mock TE metrics to the server streaming.
**Decisions:**
- Used the KSG estimator (Kraskov et al., 2004) for continuous transfer entropy estimation — it's non-parametric, works for any distribution, and handles the discrete action data well with small jitter added to avoid zero distances in k-NN lookups
- `compute_transfer_entropy(source, target, lag, k)` computes TE(X→Y) as conditional mutual information I(Y_future; X_past | Y_past) using the KSG formula: ψ(k) - <ψ(n_z1) + ψ(n_z2) - ψ(n_z3)> where ψ is the digamma function
- Uses Chebyshev (L-infinity) distance with `scipy.spatial.KDTree` for efficient neighbor counting in joint and marginal spaces
- `compute_te_matrix()` computes full pairwise TE matrix with alive_mask support. Returns mean_te, max_te, te_density (fraction of pairs with TE > 0.01 threshold) as aggregate metrics
- `transfer_entropy_from_trajectory()` is a convenience wrapper that accepts trajectory dicts (from TrajectoryRecorder) and supports both 'actions' and 'positions' features
- `TransferEntropyTracker` follows the EmergenceTracker/SpecializationTracker pattern: rolling window z-score detection for sudden changes in TE metrics, prefixed as `information/` for W&B logging
- TE is always non-negative (clamped to 0.0 minimum) — negative values from estimation noise are zeroed out
- Graceful handling of edge cases: empty sequences, short sequences, single agent, all-dead agents all return 0.0
- Added `transfer_entropy` and `te_density` to mock frame metrics in server (simulates gradual increase 0→0.15 and 0→0.6 over 500 steps)
- Dashboard store already handles metrics generically — new TE metrics flow through automatically to metricsHistory
**Files changed:**
- `src/analysis/information.py` (new) — `compute_transfer_entropy()`, `compute_te_matrix()`, `transfer_entropy_from_trajectory()`, `TEEvent` dataclass, `TransferEntropyTracker` class
- `src/server/main.py` — added `transfer_entropy` and `te_density` to mock frame metrics
- `tests/test_information.py` (new) — 35 tests across 4 classes: `TestComputeTransferEntropy` (9 tests: independent/correlated sequences, non-negativity, short/empty input, different lags, multidimensional, self-TE), `TestComputeTEMatrix` (10 tests: output keys, matrix shape, zero diagonal, alive mask, single agent, non-negative, 3D input, mean/max correctness, density range), `TestTransferEntropyFromTrajectory` (5 tests: actions/positions features, invalid feature, alive mask, partial alive), `TestTransferEntropyTracker` (11 tests: init, update, step count, history, metrics keys/finiteness, summary, event detection, steady state, event str, None alive mask)
**Validation:** mypy clean (0 new errors; 3 pre-existing in train.py), 495/495 tests pass (35 new).

---

### US-011: Division of Labor Index ✓
**What:** Implemented `compute_division_of_labor()` in `src/analysis/specialization.py` — a composite metric that quantifies how well agents have differentiated into distinct task roles. Added `division_of_labor` to mock server metrics streaming.
**Decisions:**
- DOL uses two complementary components combined multiplicatively:
  - **Task allocation** (0–1): Normalized Shannon entropy of cluster membership distribution. 1.0 = agents perfectly balanced across all K task types. Measures whether all roles are filled.
  - **Individual specialization** (0–1): Mean per-agent concentration on a single task type, computed as `1 - normalized_entropy(soft_task_probs)`. 1.0 = each agent belongs entirely to one cluster.
- `dol_index = task_allocation * individual_specialization` — both components needed for true DOL (roles must exist AND agents must specialize in them)
- Soft task-type membership computed via softmin over Euclidean distances to cluster centroids in standardized feature space (same StandardScaler as clustering)
- Supports both automatic cluster count (via `find_optimal_clusters`) and fixed `n_clusters` parameter
- Edge cases: single agent → DOL=0, identical agents → DOL=0 (degenerate clustering yields k=1), two well-separated agents → valid DOL
- Returns comprehensive dict: `dol_index`, `task_allocation`, `individual_specialization`, `n_task_types`, `task_counts`, `agent_task_probs`
- Added `division_of_labor` to mock frame metrics in server (simulates gradual increase 0→0.65 over 500 steps)
**Files changed:**
- `src/analysis/specialization.py` — added `compute_division_of_labor()` function (placed before `SpecializationEvent` class)
- `src/server/main.py` — added `division_of_labor` to mock frame metrics
- `tests/test_specialization.py` — added `TestDivisionOfLabor` class with 14 tests covering: required keys, index range [0,1], identical agents (low DOL), well-separated groups (high DOL), single agent, fixed n_clusters, task counts sum, probs shape, probs sum to 1, probs non-negative, three groups > uniform, two agents, full pipeline (features→DOL), reproducibility
**Validation:** mypy clean (0 new errors; 3 pre-existing in train.py), 509/509 tests pass (14 new).

---

### US-012: Phase Transition Detection ✓
**What:** Implemented `PhaseTransitionDetector` class and `PhaseTransitionEvent` dataclass in `src/analysis/emergence.py`. Detects phase transitions using the classical statistical mechanics approach: susceptibility spikes (variance of order parameter) combined with critical slowing down (increasing autocorrelation time). Added `phase_transition` mock metric to the server.
**Decisions:**
- **Susceptibility as variance:** Susceptibility is computed as the variance of the order parameter over a rolling window. Near a phase transition, fluctuations diverge, causing a spike in variance.
- **Autocorrelation time via lag-1 autocorrelation:** Estimated from the lag-1 autocorrelation coefficient r using tau = -1/ln(|r|). Higher tau means the system "remembers" longer — a hallmark of critical slowing down near phase transitions.
- **Dual condition for detection:** Both susceptibility spike (z > threshold relative to susceptibility history) AND increasing autocorrelation time must hold simultaneously. This avoids false positives from random variance spikes alone.
- **`_autocorrelation_time()` helper:** Extracted as a module-level function for testability. Handles edge cases: short series, constant series, and near-zero autocorrelation all gracefully return 0.0.
- **Decoupled from specific metrics:** `PhaseTransitionDetector` takes any scalar "order parameter" — not hardcoded to specialization score. Can be used to detect transitions in any metric (specialization, field entropy, DOL, etc.).
- **Follows existing tracker pattern:** `get_metrics()` returns `phase_transition/`-prefixed dict for W&B logging. `get_summary()` returns overall statistics. Events stored as `PhaseTransitionEvent` dataclass with str() formatting including ⚡ emoji.
- **Mock server:** Added `phase_transition` metric to mock frames (fires at step multiples of 250) for dashboard development.
**Files changed:**
- `src/analysis/emergence.py` — added `PhaseTransitionEvent` dataclass, `_autocorrelation_time()` helper, `PhaseTransitionDetector` class with `update()`, `get_metrics()`, `get_summary()`
- `src/server/main.py` — added `phase_transition` to mock frame metrics
- `tests/test_emergence.py` (new) — 34 tests across 7 classes: `TestAutocorrelationTime` (6 tests: short/constant series, positive autocorrelation, noisy series, window limiting, non-negativity), `TestPhaseTransitionEvent` (2 tests: str representation, fields), `TestPhaseTransitionDetectorInit` (2 tests: default/custom params), `TestPhaseTransitionDetectorUpdate` (6 tests: recording, multiple updates, susceptibility timing, returns, step recording), `TestPhaseTransitionDetection` (5 tests: sudden jump detection, gradual change no detection, event fields, steady-then-variable, dual condition requirement), `TestPhaseTransitionDetectorMetrics` (8 tests: empty/after-updates metrics, finiteness, empty/after-updates summary, event strings, latest value, correct statistics), `TestPhaseTransitionDetectorEdgeCases` (5 tests: negative values, large values, constant values, alternating no false positive, single update)
**Validation:** mypy clean (0 new errors; 3 pre-existing in train.py), 543/543 tests pass (34 new).

---

### US-013: Help System & Tooltips ✓
**What:** Added a comprehensive help system to the dashboard with reusable Tooltip component, Glossary panel, "What's happening?" plain-English summary, onboarding tour for first-time users, info icons next to all metrics and controls, and a color legend for the agent canvas.
**Decisions:**
- **Tooltip component (`Tooltip.svelte`):** Reusable, positioned via `fixed` CSS with viewport-aware placement (above/below trigger). Uses Svelte 5 `children` snippet for slot content. Shows a styled dark popup on hover/focus with smooth animation. When no children provided, renders a default (i) icon.
- **GlossaryPanel (`GlossaryPanel.svelte`):** Slide-out panel from the right side. Loads terms from `static/glossary.json` with search/filter functionality. Each term card shows the term name, simple explanation, and an analogy with a red left-border accent. Closes on backdrop click or Escape key.
- **glossary.json expanded:** Added 8 new terms (Cluster, Field, Gradient, Freeze-Evolve, Fitness, Lineage, Heatmap) to the original 13, for a total of 21 glossary entries. All terms from the PRD glossary are covered plus dashboard-specific terms.
- **HelpSystem (`HelpSystem.svelte`):** Two features in one component:
  1. "What's happening?" panel at the top of the sidebar — generates plain-English summaries from live metrics (population status, specialization progress, divergence, transfer entropy, division of labor, phase transitions, training mode). Conditionally assembles sentences based on metric values and thresholds.
  2. Onboarding tour — 5-step guided walkthrough covering: welcome, canvas, metrics, controls, and completion. Uses localStorage to show only on first visit. Tour can be restarted via (?) button.
- **Tooltip integration:** Replaced bare `title` attributes on `<span class="info-icon">` elements in MetricsPanel and ControlPanel with the new `<Tooltip>` component. Removed the `.info-icon` CSS from both components since Tooltip handles its own styling. Each metric chart and each control slider now has a rich hover tooltip.
- **Color legend:** Added to AgentCanvas with 4 cluster colors (matching renderer.js), plus Food (green) and Field/heatmap (blue) indicators. Positioned bottom-right of the canvas with a semi-transparent dark background. Includes a Tooltip explaining what the colors mean.
- **Layout integration in App.svelte:** HelpSystem placed at the top of the sidebar (above MetricsPanel), GlossaryPanel rendered at root level (portal-style). Glossary open state managed via `$state` in App and toggled via book icon in HelpSystem.
**Files changed:**
- `dashboard/src/lib/Tooltip.svelte` (new) — Reusable tooltip with viewport-aware positioning, hover/focus triggers, default (i) icon, animation
- `dashboard/src/lib/GlossaryPanel.svelte` (new) — Slide-out glossary panel with search, term cards, backdrop close
- `dashboard/src/lib/HelpSystem.svelte` (new) — "What's happening?" summary panel + 5-step onboarding tour with localStorage persistence
- `dashboard/src/lib/MetricsPanel.svelte` — Replaced `<span class="info-icon" title=...>` with `<Tooltip>` component, removed .info-icon CSS
- `dashboard/src/lib/ControlPanel.svelte` — Replaced 3 info-icon spans with `<Tooltip>` components, added Tooltip to mode indicator, removed .info-icon CSS
- `dashboard/src/lib/AgentCanvas.svelte` — Added color legend with cluster colors, food, and field indicators; imported Tooltip
- `dashboard/src/App.svelte` — Imported and integrated HelpSystem and GlossaryPanel; added glossaryOpen state
- `dashboard/static/glossary.json` — Expanded from 13 to 21 terms (added Cluster, Field, Gradient, Freeze-Evolve, Fitness, Lineage, Heatmap)
**Validation:** mypy clean (0 new errors; 3 pre-existing in train.py), 543/543 tests pass (no new Python tests — frontend task). `npm run build` succeeds (861 modules). Every metric has hover help, glossary contains all terms.

---

### US-014: Dashboard Integration Test ✓
**What:** Created comprehensive end-to-end integration tests for the full training → server → dashboard pipeline. Tests verify that real training data flows through the TrainingBridge to WebSocket clients, controls affect training state, frame serialization round-trips correctly, help system data is available, and dashboard build artifacts exist.
**Decisions:**
- **No Playwright dependency**: Used FastAPI's `TestClient` with WebSocket support instead of headless browser testing, since Playwright is listed as an optional dependency and the Python-side pipeline is the critical integration path
- **Real training data tests**: 3 tests in `TestTrainingToServer` class run actual JAX training (`create_train_state` → `train_step`) and verify the resulting frames contain valid data — positions, alive masks, energy, and finite metrics. This proves the full training → bridge → serialize pipeline works end-to-end
- **Background thread training**: `TestBackgroundTrainingIntegration` class (3 tests) simulates the real deployment pattern — training runs in a background thread publishing frames, while a WebSocket client in the main thread receives and decodes them. Tests use `threading.Event` for clean shutdown
- **Control round-trip tests**: `TestControlsAffectTraining` class (5 tests) verifies pause/resume, speed changes, and parameter updates flow correctly from WebSocket → server → bridge → training thread. Includes a full pipeline test that trains, publishes, connects, and sends multiple commands
- **Frame serialization tests**: `TestFrameSerialization` (2 tests) verifies MessagePack pack/unpack round-trip preserves all fields including positions, alive masks, cluster labels, metrics, and training mode
- **Help system validation**: `TestHelpSystemData` (3 tests) verifies glossary.json exists, is valid JSON with correct structure (`{terms: [{term, simple, analogy}]}`), contains all expected terms (agent, emergence, specialization, evolution), and has analogies for laypeople
- **Dashboard artifact checks**: `TestDashboardBuildArtifacts` (4 tests) verifies package.json has all required dependencies (svelte, pixi.js, plotly.js, msgpack-lite), key source files exist (App.svelte, renderer.js, all component files), and vite config + index.html are present
- **Mock data pipeline tests**: `TestMockDataPipeline` (4 tests) verifies mock frames stream correctly, metrics simulate gradual improvement over time, arrays have correct dtypes/shapes, and training mode cycling works
- **Small config optimized for speed**: Used minimal config (2 envs, 4 agents, grid 10, 32 hidden dims) to keep tests fast (~21s total)
- **8 test classes, 27 tests total** covering all PRD acceptance criteria except headless browser rendering (which would require Playwright installation)
**Files changed:**
- `tests/test_dashboard_integration.py` (new) — 27 tests across 8 classes: `TestTrainingToServer` (3), `TestServerEndpoints` (3), `TestControlsAffectTraining` (5), `TestMockDataPipeline` (4), `TestFrameSerialization` (2), `TestHelpSystemData` (3), `TestBackgroundTrainingIntegration` (3), `TestDashboardBuildArtifacts` (4)
**Validation:** mypy clean (0 new errors; 3 pre-existing in train.py), 570/570 tests pass (27 new).

---

### US-015: Agent Lineage Visualization ✓
**What:** Added lineage visualization to the dashboard. Extended the streaming protocol with agent IDs, parent IDs, birth steps, and lineage summary data. Created a LineagePanel component showing a tree/graph visualization of agent parent-child relationships, dominant lineage rankings, and agent detail tooltips.
**Decisions:**
- **Streaming protocol extension:** Added 4 optional fields to `Frame`: `agent_ids` (int32), `parent_ids` (int32), `birth_steps` (int32), and `lineage_data` (dict with dominant_lineages, max_depth, total_births). All fields are optional and backward-compatible (None by default, omitted from MessagePack when None).
- **`create_frame_from_state` updated:** Automatically extracts `agent_ids`, `agent_parent_ids`, and `agent_birth_step` from `EnvState` when those attributes exist. Accepts optional `lineage_data` dict parameter.
- **Int32 dtype support:** Added `<i4` / `int32` dtype handling in the dashboard's `unpackArray()` function for proper Int32Array reconstruction from MessagePack.
- **Mock lineage data:** Mock frames now generate realistic lineage data: first 8 agents are originals (parent_id=-1), remaining agents have parents from the original 8, with computed dominant lineage rankings.
- **Tree layout algorithm:** BFS-based depth computation from roots, agents grouped by generation depth, positioned in rows with even horizontal spacing. Only alive agents are shown in the tree to avoid clutter.
- **LineagePanel features:**
  - Tree/graph SVG visualization with nodes (agents) and edges (parent-child lines)
  - Color by cluster (behavioral group) or fitness (energy-based gradient)
  - Dominant lineages bar chart showing top 3 lineages by descendant count
  - Click agent to select (highlight with white ring), click again to deselect
  - Hover shows detailed tooltip: agent ID, parent, age, energy, cluster group
  - Stats footer: generational depth, total births, alive count
  - Deaths shown as faded/dashed edges
- **Store integration:** Added `agentIds`, `parentIds`, `birthSteps`, `lineageData`, `selectedAgentIndex` state variables and `selectAgent(index)` method to the training store.
- **Layout placement:** LineagePanel sits between MetricsPanel and ControlPanel in the sidebar, matching the PRD file structure.
**Files changed:**
- `src/server/streaming.py` — added `agent_ids`, `parent_ids`, `birth_steps`, `lineage_data` fields to `Frame`; updated `pack_frame()` to include lineage arrays; updated `create_frame_from_state()` to extract lineage data from EnvState
- `src/server/main.py` — updated `_generate_mock_frame()` to generate mock lineage data (agent IDs, parent-child relationships, birth steps, dominant lineages)
- `dashboard/src/stores/training.svelte.js` — added Int32Array dtype support in `unpackArray()`; added lineage state variables and `selectAgent()` method; added lineage data decoding in `processFrame()`
- `dashboard/src/lib/LineagePanel.svelte` (new) — Tree/graph visualization with SVG, dominant lineage bars, agent click/hover, color modes, stats footer
- `dashboard/src/App.svelte` — imported and integrated LineagePanel in sidebar
- `tests/test_server.py` — added `TestLineageData` class with 14 tests covering: Frame lineage fields default/set, pack without/with lineage, mock frame lineage shapes/ids/originals/children/structure/fields, create_frame_from_state with/without lineage, packed lineage WebSocket round-trip
**Validation:** mypy clean (0 new errors; 3 pre-existing in train.py), 584/584 tests pass (14 new). `npm run build` succeeds (863 modules).

---

### US-016: Export and Replay ✓
**What:** Implemented a recording and replay system for training sessions. Created `src/server/replay.py` with `SessionRecorder` and `SessionPlayer` classes, added recording control endpoints and a `/ws/replay` WebSocket endpoint to the server, and built a `ReplayControls.svelte` dashboard component with timeline scrubber, speed controls, and bookmark navigation.
**Decisions:**
- **Recording format:** Sessions stored as directories containing `metadata.json`, `frames.msgpack.gz` (gzip-compressed MessagePack), and `bookmarks.json`. Gzip compression reduces storage significantly for the large per-frame numpy arrays.
- **`SessionRecorder`:** Accumulates serialized frames in memory, flushes to disk on `save()`. Supports `sample_rate` (record every Nth frame) and `max_frames` limits to control storage. Context manager support for clean start/stop.
- **`SessionPlayer`:** Loads a recorded session from disk, supports play/pause, variable speed (0.25x–16x clamped), seek to frame index, seek to training step (finds closest frame), and seek to bookmark index. Progress reported as 0.0–1.0 fraction.
- **`Bookmark` dataclass:** Stores step, frame_index, label, and timestamp. Round-trip serializable via `to_dict()`/`from_dict()`.
- **Frame serialization:** `_serialize_frame()` produces the same dict format as `pack_frame()` but returns a dict instead of bytes, so multiple frames can be batched into one MessagePack file. Stored frames can be sent directly to clients via `msgpack.packb()` without re-conversion.
- **Server endpoints:** `GET /sessions` lists all recorded sessions. `POST /record/start` starts recording (creates a timestamped session directory). `POST /record/stop` stops and saves. `POST /record/bookmark` adds a bookmark during recording. Recording is attached to the bridge via a `_recorder` attribute.
- **Replay WebSocket (`/ws/replay`):** Separate from `/ws/training`. Client sends JSON `load` command with session path, then receives binary MessagePack frames when playing. Status updates (position, total, step, playing, speed, bookmarks, metadata) sent as JSON text messages after load/seek/speed changes.
- **Dashboard integration:** `ReplayControls.svelte` in the sidebar below ControlPanel. Shows session list (fetched via `/sessions` API) when no replay is active. After loading, shows timeline scrubber, play/pause + speed buttons, bookmark list, and session metadata. Replay frames feed into the same `processFrame()` pipeline, so the canvas, metrics charts, and lineage panel all update from replay data.
- **Store extension:** Added `replayWs`, `replayConnected`, `replayStatus` state, plus `connectReplay()`, `disconnectReplay()`, `sendReplayCommand()` methods. Replay WebSocket URL derived from the training WebSocket URL by replacing `/ws/training` with `/ws/replay`.
- **Mock training loop:** Updated to record frames when a recorder is attached to the bridge, so recording works in mock mode for development/testing.
**Files changed:**
- `src/server/replay.py` (new) — `Bookmark` dataclass, `_serialize_frame()`, `SessionRecorder` class (start/stop/record/add_bookmark/save/context_manager), `SessionPlayer` class (load/play/pause/seek/seek_to_step/seek_to_bookmark/advance/current_frame/current_step/progress), `list_sessions()` utility
- `src/server/main.py` — added `import msgpack`, `import os`, replay imports; added `DEFAULT_RECORDINGS_DIR` and `_recorder` global; added `/sessions`, `/record/start`, `/record/stop`, `/record/bookmark` endpoints; added `/ws/replay` WebSocket with `_send_replay_status()` helper; updated mock training loop to record frames; updated module docstring
- `dashboard/src/lib/ReplayControls.svelte` (new) — Session selector, timeline scrubber, play/pause + speed controls, bookmark navigation, session info, close button
- `dashboard/src/stores/training.svelte.js` — added replay WebSocket state (`replayWs`, `replayConnected`, `replayStatus`), `connectReplay()`, `disconnectReplay()`, `sendReplayCommand()` methods, and replay state getters in return object
- `dashboard/src/App.svelte` — imported and integrated ReplayControls in sidebar
- `tests/test_replay.py` (new) — 72 tests across 9 classes: `TestBookmark` (5 tests), `TestFrameSerialization` (7 tests), `TestSessionRecorder` (14 tests), `TestSessionPlayer` (24 tests), `TestListSessions` (5 tests), `TestServerRecording` (6 tests), `TestReplayWebSocket` (9 tests), `TestRecordReplayRoundTrip` (2 tests)
**Validation:** mypy clean (0 new errors; 3 pre-existing in train.py), 656/656 tests pass (72 new). `npm run build` succeeds (865 modules). Can replay saved session at variable speed.

