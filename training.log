============================================================
Emergence Lab — Phase 1: Digital Petri Dish
============================================================
Grid: 20x20
Agents: 8, Food: 10
Field channels: 4
Num envs: 32
Total steps: 1000000
Steps per rollout: 128
Seed: 42
============================================================
Steps per iteration: 32768
Number of iterations: 30
Log interval: 1000 steps

Initializing training state...
Training state initialized.
JIT compiling train_step...
JIT compilation done (1.1s)

Training:   0%|          | 0/29 [00:00<?, ?iter/s]Training:   0%|          | 0/29 [00:00<?, ?iter/s, entropy=1.5990, loss=0.0233, reward=0.0112, steps=65536, value=-0.1485]Training:   3%|▎         | 1/29 [00:00<00:22,  1.27iter/s, entropy=1.5990, loss=0.0233, reward=0.0112, steps=65536, value=-0.1485]Training:   3%|▎         | 1/29 [00:01<00:22,  1.27iter/s, entropy=1.5855, loss=-0.0027, reward=0.0037, steps=98304, value=0.0359]Training:   7%|▋         | 2/29 [00:01<00:14,  1.86iter/s, entropy=1.5855, loss=-0.0027, reward=0.0037, steps=98304, value=0.0359]Training:   7%|▋         | 2/29 [00:01<00:14,  1.86iter/s, entropy=1.5825, loss=-0.0142, reward=0.0007, steps=131072, value=0.1060]Training:  10%|█         | 3/29 [00:01<00:11,  2.36iter/s, entropy=1.5825, loss=-0.0142, reward=0.0007, steps=131072, value=0.1060]Training:  10%|█         | 3/29 [00:01<00:11,  2.36iter/s, entropy=1.5887, loss=-0.0174, reward=0.0000, steps=163840, value=0.0714]Training:  14%|█▍        | 4/29 [00:01<00:09,  2.63iter/s, entropy=1.5887, loss=-0.0174, reward=0.0000, steps=163840, value=0.0714]Training:  14%|█▍        | 4/29 [00:02<00:09,  2.63iter/s, entropy=1.5776, loss=-0.0172, reward=0.0000, steps=196608, value=-0.0042]Training:  17%|█▋        | 5/29 [00:02<00:08,  2.90iter/s, entropy=1.5776, loss=-0.0172, reward=0.0000, steps=196608, value=-0.0042]Training:  17%|█▋        | 5/29 [00:02<00:08,  2.90iter/s, entropy=1.5739, loss=-0.0175, reward=0.0000, steps=229376, value=-0.0002]Training:  21%|██        | 6/29 [00:02<00:07,  2.88iter/s, entropy=1.5739, loss=-0.0175, reward=0.0000, steps=229376, value=-0.0002]Training:  21%|██        | 6/29 [00:02<00:07,  2.88iter/s, entropy=1.5789, loss=-0.0174, reward=0.0000, steps=262144, value=0.0061] Training:  24%|██▍       | 7/29 [00:02<00:07,  2.93iter/s, entropy=1.5789, loss=-0.0174, reward=0.0000, steps=262144, value=0.0061]Training:  24%|██▍       | 7/29 [00:02<00:07,  2.93iter/s, entropy=1.5746, loss=-0.0174, reward=0.0000, steps=294912, value=-0.0092]Training:  28%|██▊       | 8/29 [00:03<00:06,  3.09iter/s, entropy=1.5746, loss=-0.0174, reward=0.0000, steps=294912, value=-0.0092]Training:  28%|██▊       | 8/29 [00:03<00:06,  3.09iter/s, entropy=1.5722, loss=-0.0190, reward=0.0000, steps=327680, value=0.0063] Training:  31%|███       | 9/29 [00:03<00:06,  3.21iter/s, entropy=1.5722, loss=-0.0190, reward=0.0000, steps=327680, value=0.0063]Training:  31%|███       | 9/29 [00:03<00:06,  3.21iter/s, entropy=1.5766, loss=-0.0181, reward=0.0000, steps=360448, value=-0.0001]Training:  34%|███▍      | 10/29 [00:03<00:05,  3.30iter/s, entropy=1.5766, loss=-0.0181, reward=0.0000, steps=360448, value=-0.0001]Training:  34%|███▍      | 10/29 [00:03<00:05,  3.30iter/s, entropy=1.5717, loss=-0.0171, reward=0.0000, steps=393216, value=-0.0050]Training:  38%|███▊      | 11/29 [00:03<00:05,  3.18iter/s, entropy=1.5717, loss=-0.0171, reward=0.0000, steps=393216, value=-0.0050]Training:  38%|███▊      | 11/29 [00:04<00:05,  3.18iter/s, entropy=1.5706, loss=-0.0188, reward=0.0002, steps=425984, value=0.0032] Training:  41%|████▏     | 12/29 [00:04<00:05,  3.12iter/s, entropy=1.5706, loss=-0.0188, reward=0.0002, steps=425984, value=0.0032]Training:  41%|████▏     | 12/29 [00:04<00:05,  3.12iter/s, entropy=1.5693, loss=-0.0185, reward=0.0000, steps=458752, value=-0.0051]Training:  45%|████▍     | 13/29 [00:04<00:05,  3.15iter/s, entropy=1.5693, loss=-0.0185, reward=0.0000, steps=458752, value=-0.0051]Training:  45%|████▍     | 13/29 [00:04<00:05,  3.15iter/s, entropy=1.5723, loss=-0.0190, reward=0.0000, steps=491520, value=0.0016] Training:  48%|████▊     | 14/29 [00:04<00:04,  3.13iter/s, entropy=1.5723, loss=-0.0190, reward=0.0000, steps=491520, value=0.0016]Training:  48%|████▊     | 14/29 [00:05<00:04,  3.13iter/s, entropy=1.5646, loss=-0.0193, reward=0.0000, steps=524288, value=-0.0010]Training:  52%|█████▏    | 15/29 [00:05<00:04,  3.24iter/s, entropy=1.5646, loss=-0.0193, reward=0.0000, steps=524288, value=-0.0010]Training:  52%|█████▏    | 15/29 [00:05<00:04,  3.24iter/s, entropy=1.5566, loss=-0.0190, reward=0.0000, steps=557056, value=-0.0023]Training:  55%|█████▌    | 16/29 [00:05<00:04,  3.03iter/s, entropy=1.5566, loss=-0.0190, reward=0.0000, steps=557056, value=-0.0023]Training:  55%|█████▌    | 16/29 [00:05<00:04,  3.03iter/s, entropy=1.5605, loss=-0.0185, reward=0.0000, steps=589824, value=-0.0014]Training:  59%|█████▊    | 17/29 [00:05<00:03,  3.04iter/s, entropy=1.5605, loss=-0.0185, reward=0.0000, steps=589824, value=-0.0014]Training:  59%|█████▊    | 17/29 [00:06<00:03,  3.04iter/s, entropy=1.5497, loss=-0.0187, reward=0.0000, steps=622592, value=0.0016] Training:  62%|██████▏   | 18/29 [00:06<00:03,  3.10iter/s, entropy=1.5497, loss=-0.0187, reward=0.0000, steps=622592, value=0.0016]Training:  62%|██████▏   | 18/29 [00:06<00:03,  3.10iter/s, entropy=1.5522, loss=-0.0179, reward=0.0000, steps=655360, value=0.0008]Training:  66%|██████▌   | 19/29 [00:06<00:03,  3.22iter/s, entropy=1.5522, loss=-0.0179, reward=0.0000, steps=655360, value=0.0008]Training:  66%|██████▌   | 19/29 [00:06<00:03,  3.22iter/s, entropy=1.5595, loss=-0.0191, reward=0.0000, steps=688128, value=-0.0003]Training:  69%|██████▉   | 20/29 [00:06<00:02,  3.09iter/s, entropy=1.5595, loss=-0.0191, reward=0.0000, steps=688128, value=-0.0003]Training:  69%|██████▉   | 20/29 [00:07<00:02,  3.09iter/s, entropy=1.5412, loss=-0.0188, reward=0.0000, steps=720896, value=-0.0041]Training:  72%|███████▏  | 21/29 [00:07<00:02,  3.07iter/s, entropy=1.5412, loss=-0.0188, reward=0.0000, steps=720896, value=-0.0041]Training:  72%|███████▏  | 21/29 [00:07<00:02,  3.07iter/s, entropy=1.5334, loss=-0.0188, reward=0.0000, steps=753664, value=-0.0016]Training:  76%|███████▌  | 22/29 [00:07<00:02,  3.19iter/s, entropy=1.5334, loss=-0.0188, reward=0.0000, steps=753664, value=-0.0016]Training:  76%|███████▌  | 22/29 [00:07<00:02,  3.19iter/s, entropy=1.5440, loss=-0.0197, reward=0.0000, steps=786432, value=0.0007] Training:  79%|███████▉  | 23/29 [00:07<00:01,  3.17iter/s, entropy=1.5440, loss=-0.0197, reward=0.0000, steps=786432, value=0.0007]Training:  79%|███████▉  | 23/29 [00:08<00:01,  3.17iter/s, entropy=1.5496, loss=-0.0188, reward=0.0000, steps=819200, value=0.0005]Training:  83%|████████▎ | 24/29 [00:08<00:01,  3.25iter/s, entropy=1.5496, loss=-0.0188, reward=0.0000, steps=819200, value=0.0005]Training:  83%|████████▎ | 24/29 [00:08<00:01,  3.25iter/s, entropy=1.5316, loss=-0.0192, reward=0.0000, steps=851968, value=0.0013]Training:  86%|████████▌ | 25/29 [00:08<00:01,  3.05iter/s, entropy=1.5316, loss=-0.0192, reward=0.0000, steps=851968, value=0.0013]Training:  86%|████████▌ | 25/29 [00:08<00:01,  3.05iter/s, entropy=1.5252, loss=-0.0189, reward=0.0000, steps=884736, value=0.0007]Training:  90%|████████▉ | 26/29 [00:08<00:00,  3.13iter/s, entropy=1.5252, loss=-0.0189, reward=0.0000, steps=884736, value=0.0007]Training:  90%|████████▉ | 26/29 [00:08<00:00,  3.13iter/s, entropy=1.5161, loss=-0.0194, reward=0.0000, steps=917504, value=0.0003]Training:  93%|█████████▎| 27/29 [00:08<00:00,  3.24iter/s, entropy=1.5161, loss=-0.0194, reward=0.0000, steps=917504, value=0.0003]Training:  93%|█████████▎| 27/29 [00:09<00:00,  3.24iter/s, entropy=1.5252, loss=-0.0183, reward=0.0000, steps=950272, value=0.0001]Training:  97%|█████████▋| 28/29 [00:09<00:00,  3.32iter/s, entropy=1.5252, loss=-0.0183, reward=0.0000, steps=950272, value=0.0001]Training:  97%|█████████▋| 28/29 [00:09<00:00,  3.32iter/s, entropy=1.5424, loss=-0.0185, reward=0.0000, steps=983040, value=0.0002]Training: 100%|██████████| 29/29 [00:09<00:00,  3.28iter/s, entropy=1.5424, loss=-0.0185, reward=0.0000, steps=983040, value=0.0002]Training: 100%|██████████| 29/29 [00:09<00:00,  3.02iter/s, entropy=1.5424, loss=-0.0185, reward=0.0000, steps=983040, value=0.0002]

============================================================
Training complete!
Total env steps: 983040
Final metrics:
  approx_kl: 0.003901
  clip_fraction: 0.027672
  entropy: 1.542418
  mean_advantage: -0.000162
  mean_return: 0.000000
  mean_reward: 0.000000
  mean_value: 0.000162
  policy_loss: -0.003065
  total_loss: -0.018464
  value_loss: 0.000049
============================================================
