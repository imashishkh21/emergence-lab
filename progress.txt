# Phase 5 Progress: Prove Emergence

## Session Start
Date: 2026-02-03
Previous Phases: Phase 1 (Digital Petri Dish), Phase 2 (Evolutionary Pressure), Phase 3 (Specialization Detection), Phase 4 (Research Microscope), Phase 4B (Kaggle Infrastructure) — all COMPLETE

## Phase 5 Goal
Prove with rigorous information-theoretic metrics and statistical significance that collective intelligence emerges from simple agents + shared field + evolution. Beat classical swarm AND modern MARL baselines. Publication-ready figures.

## Key Insights from Previous Phases
- Random field HURTS agents (585 < 600) — they learned to READ the field for information
- Specialization emerges with evolution: scouts, exploiters, balanced agents
- Weight divergence tracks with behavioral clustering
- Population reaches equilibrium (32/32 maxed): 108 births + 108 deaths = perfect turnover
- 9.8M steps trained on Kaggle (checkpoint available)

## Codebase Patterns
- Trackers follow pattern: __init__(), update(), get_metrics(), get_summary() — see TransferEntropyTracker
- All baselines return standardized result dict: {"total_reward", "food_collected", "final_population", "per_agent_rewards"}
- EnvState is flax.struct.dataclass — adding fields requires updating EVERY constructor call (state.py, env.py, ablation.py)
- Phase 5 deps go in [project.optional-dependencies] under "phase5" group with @pytest.mark.skipif guards
- Config uses dataclass_field(default_factory=...) for nested configs
- Field conditions in ablation.py: use FieldCondition Literal type
- Tests use jax.random.PRNGKey for deterministic testing

---

## Task Log

### US-001: Phase 5 Dependencies and Library Verification ✓
**Date:** 2026-02-03
**What:** Added dit, hoi, rliable to pyproject.toml [phase5] group. Created smoke tests.
**Decisions:**
- jaxmarl excluded from phase5 deps — pins jax<=0.4.38 and scipy<=1.12, incompatible with our jax 0.9 + scipy 1.17. We write custom MAPPO instead.
- dit requires numpy compat patch (np.alltrue removed in NumPy 2.0) — monkey-patched in test setup
- rliable.metrics works fine; rliable.library bootstrap has arch 8.0 compat issue (passes None as random_state). Metrics-only usage is sufficient for US-011.
- hoi requires float64 data and jax_enable_x64 for proper computation
- System deps needed: brew install cddlib gmp glpk suite-sparse (for dit's cvxopt/pycddlib)
**Files changed:**
- `pyproject.toml` — added phase5 optional-dependencies group
- `tests/test_phase5_deps.py` — NEW: 13 smoke tests (12 pass, 1 skip for jaxmarl)
**Verification:** mypy clean, ruff clean, 744 tests pass (1 skip)

### US-002: O-Information Metric via hoi ✓
**Date:** 2026-02-03
**What:** Implemented O-information (Omega = TC - DTC) metric using hoi library. Omega < 0 indicates synergy-dominant emergence.
**Decisions:**
- Using hoi's Gaussian Copula method ("gc") for fast continuous-data O-info estimation
- Returns 0.0 for edge cases: <3 agents (O-info undefined), <10 samples, constant features, NaN inputs
- Added noise injection (1e-8 scale) to avoid numerical issues with near-constant dimensions
- OInformationTracker follows same pattern as TransferEntropyTracker with z-score phase transition detection
- Tracks synergy_ratio: cumulative fraction of negative O-info values (emergence signal)
**Files changed:**
- `src/analysis/o_information.py` — NEW: compute_o_information(), compute_o_information_by_condition(), OInfoEvent, OInformationTracker
- `tests/test_o_information.py` — NEW: 27 tests covering all functions and edge cases
**Verification:** mypy clean, ruff clean, 27/27 tests pass

### US-003: Pairwise PID Synergy via dit ✓
**Date:** 2026-02-03
**What:** Implemented Partial Information Decomposition (PID) for measuring synergy between agent pairs using dit library.
**Decisions:**
- Using coinformation (not interaction_information) from dit for correct sign convention (negative = synergy, matching PRD)
- Jeffreys smoothing (alpha=0.5 pseudocounts) applied to joint distributions before PID computation
- K=2 quantile bins for continuous variables (field_summary, future_food)
- Special handling in discretize_continuous() for discrete data with <= num_bins unique values (direct mapping instead of quantile binning)
- PID decomposition using Williams-Beer (PID_WB) from dit
- Row shuffle surrogate test for statistical significance (shuffles y to break cross-variable coordination)
- Column shuffle variant uses circular shift to break temporal dependencies
**Files changed:**
- `src/analysis/pid_synergy.py` — NEW: compute_interaction_information(), compute_pairwise_pid(), compute_median_synergy(), surrogate_significance_test(), discretize_continuous(), apply_jeffreys_smoothing()
- `tests/test_pid_synergy.py` — NEW: 31 tests covering all functions, edge cases, XOR synergy, copy redundancy, surrogate testing
**Verification:** mypy clean, ruff clean, 31/31 tests pass

### US-004: Causal Emergence (Effective Information + Rosas Psi) ✓
**Date:** 2026-02-03
**What:** Implemented two complementary causal emergence measures: Hoel's Effective Information (EI) and Rosas' Psi. EI gap > 0 and Psi > 0 indicate causal emergence at the macro scale.
**Decisions:**
- Implemented custom TPM building with Laplace smoothing (no external dependencies beyond numpy/scipy)
- EI = log2(N) - avg_row_entropy(TPM), following Hoel et al. 2013
- Psi = I(V_t; V_{t+1}) - sum_i I(X_i,t; V_{t+1}), following Rosas et al. 2020
- Discretization using quantile bins (default 4 bins, configurable)
- Macro variable candidates: population_count, mean_field_intensity, total_food_collected, spatial_dispersion, field_entropy
- extract_macro_variables() extracts all macro vars from trajectory dict
- Windowed analysis: compute_windowed_causal_emergence() with configurable window size and overlap (default 50%)
- CausalEmergenceTracker follows same pattern as TransferEntropyTracker with z-score phase transition detection
- Tracks emergence_ratio: cumulative fraction of timesteps with both positive EI gap AND positive Psi
**Files changed:**
- `src/analysis/causal_emergence.py` — NEW: compute_effective_information(), compute_rosas_psi(), compute_causal_emergence_from_trajectory(), compute_windowed_causal_emergence(), extract_macro_variables(), discretize_to_bins(), build_tpm(), compute_tpm_entropy(), compute_mutual_information_discrete(), CausalEmergenceEvent, CausalEmergenceTracker
- `tests/test_causal_emergence.py` — NEW: 54 tests covering all functions, edge cases, TPM building, MI computation, trajectory extraction, windowed analysis, tracker pattern
**Verification:** mypy clean, ruff clean, 54/54 tests pass, 856 total tests pass

### US-005: Surrogate Testing Framework ✓
**Date:** 2026-02-03
**What:** Created a reusable surrogate testing framework for statistical significance of all emergence metrics. Implements multiple shuffle methods and statistical tests.
**Decisions:**
- row_shuffle: breaks cross-agent coordination by shuffling agent dimension per timestep
- column_shuffle: breaks temporal dependencies by permuting time axis
- block_shuffle: preserves short-range structure, breaks long-range (shuffles blocks of timesteps)
- bootstrap_ci: implemented BCa (bias-corrected and accelerated) method for accurate confidence intervals
- mann_whitney_u: wrapper returning U-statistic, p-value, and rank-biserial effect size
- wilcoxon_signed_rank: wrapper for paired samples with r effect size (Z/sqrt(N))
- surrogate_test: generic function that takes any metric_fn and shuffle_fn, returns SurrogateResult
- Renamed TestResult to StatisticalTestResult to avoid pytest collection warning
- Added effect size interpretation functions (Cohen's d, Glass's delta, r)
- Added compare_conditions() for pairwise comparisons across multiple conditions
- Uses only numpy/scipy (no Phase 5 deps needed) — framework is universally usable
**Files changed:**
- `src/analysis/surrogates.py` — NEW: row_shuffle(), column_shuffle(), block_shuffle(), bootstrap_ci(), mann_whitney_u(), wilcoxon_signed_rank(), surrogate_test(), compute_cohens_d(), compute_glass_delta(), interpret_effect_size(), compare_conditions(), StatisticalTestResult, SurrogateResult
- `tests/test_surrogates.py` — NEW: 50 tests covering all functions, edge cases, integration tests
**Verification:** mypy clean, ruff clean, 50/50 tests pass

### US-006: IPPO Baseline (No Field, Shared Params) ✓
**Date:** 2026-02-03
**What:** Implemented IPPO baseline — the simplest baseline with no field communication and no evolution. This is the "no communication at all" lower bound for baseline comparisons.
**Decisions:**
- Field disabled via: write_strength=0.0, decay_rate=1.0 (field zeros out each step)
- Evolution disabled via: enabled=False, infinite starting_energy (1000000), energy_per_step=0, impossible reproduce_threshold
- max_agents set equal to num_agents (no population growth or death)
- Reuses existing ActorCritic and step() — zero new neural network code (as required by PRD)
- Returns standardized result dict: {"total_reward", "food_collected", "final_population", "per_agent_rewards"}
- evaluate_ippo() aggregates results across multiple episodes with mean/std/per-episode tracking
- Supports both stochastic (sampling) and deterministic (greedy) action selection
**Files changed:**
- `src/baselines/__init__.py` — NEW: Module docstring with baseline descriptions
- `src/baselines/ippo.py` — NEW: ippo_config(), run_ippo_episode(), evaluate_ippo(), create_ippo_network(), init_ippo_params()
- `tests/test_baselines.py` — NEW: 17 tests in TestIPPO and TestIPPOImportability classes
**Verification:** mypy clean, ruff clean, 17/17 IPPO tests pass, 923 total tests pass (1 skip)

### US-007: ACO-Fixed Baseline (+ ACO-Hybrid Variant) ✓
**Date:** 2026-02-03
**What:** Implemented two ACO baselines for comparison — ACO-Fixed (no neural network, pure hardcoded pheromone rules) and ACO-Hybrid (NN for movement, hardcoded field writes). This isolates the value of LEARNING the write behavior vs hardcoded ACO rules.
**Decisions:**
- ACO parameters follow Dorigo & Stutzle (2004): alpha=1.0 (pheromone importance), beta=2.0 (heuristic importance), rho=0.5 (evaporation = decay_rate), Q=1.0 (deposit quantity = write_strength)
- ACO-Fixed movement: p_ij = (tau_ij^alpha * eta_ij^beta) / sum (classic ACO formula)
- Food heuristic (eta): 1/(1 + manhattan_distance_to_nearest_food)
- Pheromone reading (tau): sum of field channels at each neighboring cell
- Extra pheromone deposit (ACO_Q * 2.0) when agent collects food — reinforces successful paths
- ACO-Hybrid: uses neural network for movement decisions, but field writes follow hardcoded ACO rules
- Both return standardized result dict matching IPPO format
- Evolution disabled in both variants (population constant)
- Uses existing field infrastructure (same diffusion, decay dynamics)
**Files changed:**
- `src/baselines/aco_fixed.py` — NEW: aco_config(), run_aco_fixed_episode(), run_aco_hybrid_episode(), evaluate_aco_fixed(), evaluate_aco_hybrid(), create_aco_hybrid_network(), init_aco_hybrid_params(), internal functions for movement probabilities and food heuristics
- `tests/test_baselines.py` — EXTENDED: 17 tests in TestACO + 2 tests in TestACOImportability
**Verification:** mypy clean, ruff clean, 19/19 ACO tests pass, 942 total tests pass (1 skip)

### US-008: MAPPO Baseline (Centralized Critic) ✓
**Date:** 2026-02-03
**What:** Implemented MAPPO baseline with centralized critic and decentralized actors. This is CTDE (Centralized Training, Decentralized Execution) — the actor sees only local observations while the critic sees all agents' observations concatenated.
**Decisions:**
- CentralizedCritic: Flax module that takes concatenated observations of all agents, outputs per-agent value estimates
- Hidden dims are 2x the actor's hidden dims for increased capacity (handles n_agents * obs_dim inputs)
- Field disabled via: write_strength=0.0, decay_rate=1.0 (no field communication — MAPPO handles coordination via centralized critic)
- Evolution disabled: population stays constant (no births/deaths)
- ~200 lines of code as specified in PRD (actually 220 lines)
- RunningMeanStd class for value normalization (running mean/std of returns)
- mappo_loss() uses vmap for vectorized forward pass over batch
- Death masking via alive_mask in loss computation
- create_mappo_train_state() sets up actor/critic optimizers with gradient clipping
**Files changed:**
- `src/baselines/mappo.py` — NEW: CentralizedCritic, RunningMeanStd, mappo_config(), create_mappo_network(), create_centralized_critic(), init_mappo_params(), mappo_loss(), run_mappo_episode(), evaluate_mappo(), create_mappo_train_state()
- `tests/test_baselines.py` — EXTENDED: 19 tests in TestMAPPO + 4 tests in TestMAPPOImportability
**Verification:** mypy clean, ruff clean, 23/23 MAPPO tests pass, 59/59 total baseline tests pass

