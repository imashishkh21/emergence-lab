# Phase 2 Progress: Evolutionary Pressure

## Session Start
Date: 2026-02-01
Previous Phase: Phase 1 COMPLETE — field ablation showed Normal (63.36) >> Zeroed (22.88)

## Key Technical Decisions

1. **JAX Static Shapes**: Use fixed max_agents array with alive mask instead of dynamic population
2. **Weight Inheritance**: Per-agent params with vmapped forward pass
3. **Reproduction**: Action 5 triggers reproduction if energy >= threshold
4. **Death**: Energy <= 0 sets alive = False, slot reusable

---

## Task Log

### US-001: Add Evolution Config ✓
**What:** Added `EvolutionConfig` dataclass and `evolution` field to main `Config`.
**Decisions:** Used same dataclass pattern as existing configs. All defaults match PRD spec.
**Files changed:**
- `src/configs.py` — Added `EvolutionConfig` dataclass, added `evolution` field to `Config`, updated `from_yaml` to parse evolution section
- `configs/phase2.yaml` — Created with all Phase 2 settings including evolution config
**Issues:** None. All 56 tests pass, mypy clean.

---

### US-002: Extend EnvState with Energy and Alive Mask ✓
**What:** Added evolution tracking fields to `EnvState` and updated `agent_positions` from `(num_agents, 2)` to `(max_agents, 2)` with padding for dead agents.
**Decisions:**
- `agent_positions` padded to `(max_agents, 2)` — dead slots get `(0, 0)`.
- `agent_energy` is `float32` for smooth gradient-friendly math later.
- `agent_alive` is `bool_` mask — True for first `num_agents`, False for rest.
- `agent_ids` starts at `[0, 1, ..., num_agents-1, -1, -1, ...]`.
- `agent_parent_ids` all `-1` initially (no parents for original agents).
- `next_agent_id` initialized to `num_agents` (scalar counter).
- `step()` pads incoming actions to `(max_agents,)` so dead agents get action 0 (stay).
- Dead agents masked out of movement, food collection, and field writes.
- `obs.py` slices `[:num_agents]` for observations (US-010 will generalize).
- `render.py` skips dead agents when drawing.
- `ablation.py` `_replace_field` updated to pass through all new fields.
**Files changed:**
- `src/environment/state.py` — Added 5 new fields to `EnvState`, updated `create_env_state`
- `src/environment/env.py` — Updated `reset()` and `step()` for `(max_agents, 2)` shapes and alive masking
- `src/environment/obs.py` — Slice `[:num_agents]` for active agents only
- `src/environment/render.py` — Skip dead agents when drawing
- `src/analysis/ablation.py` — Updated `_replace_field` to include new fields
- `tests/test_env.py` — Updated shape assertions for `max_agents`, added checks for new fields
**Issues:** None. All 56 tests pass, mypy clean.

---

### US-003: Update Environment Reset for Evolution ✓
**What:** Verified that `reset()` already correctly initializes all evolution state fields (done in US-002). Added value-level tests to ensure correctness beyond shape checks.
**Decisions:**
- The `reset()` implementation was already complete from US-002 — energy values, alive mask, agent IDs, parent IDs, next_agent_id, and dead slot positions were all initialized correctly.
- Added 3 new tests to `TestEnvReset` to verify actual values: `test_reset_evolution_energy_values`, `test_reset_evolution_ids`, `test_reset_dead_agent_positions`.
**Files changed:**
- `tests/test_env.py` — Added 3 tests verifying evolution field values (energy, IDs, dead positions)
**Issues:** None. All 59 tests pass, mypy clean.

---

### US-004: Implement Energy Drain Per Step ✓
**What:** Added energy drain logic to `step()` — alive agents lose `energy_per_step` each step, clamped to 0. Created `tests/test_energy.py` with 5 tests covering drain, alive-only masking, floor clamping, multi-step accumulation, and JIT compatibility.
**Decisions:**
- Used `jnp.where(agent_alive, max(energy - cost, 0), energy)` pattern — clean, JIT-friendly, no mutation.
- Energy drain happens after movement and food collection but before step counter advance (section 5 in step()).
- Dead agent slots (energy=0, alive=False) are untouched by drain.
**Files changed:**
- `src/environment/env.py` — Added energy drain logic in `step()`, renumbered subsequent sections
- `tests/test_energy.py` — Created with 5 tests in `TestEnergyDrain` class
**Issues:** None. All 64 tests pass, mypy clean.

---

### US-005: Implement Death from Starvation ✓
**What:** Added death-from-starvation logic to `step()` — agents with energy <= 0 after energy drain have `agent_alive` set to False. Death count is logged in the info dict.
**Decisions:**
- Death check happens immediately after energy drain (section 6 in step()), before step counter advance.
- Used `starved = state.agent_alive & (new_energy <= 0)` pattern — only previously alive agents can die.
- `new_alive = state.agent_alive & ~starved` — clean boolean masking, no mutation.
- Dead agents remain in arrays (positions, IDs preserved) but are masked out of movement and actions on subsequent steps.
- Added `deaths_this_step` to info dict as `jnp.int32` count.
- Added 7 tests: death on zero energy, death count in info, no deaths with enough energy, dead agents stay in arrays, dead agents don't move, partial death (mixed energy levels), JIT compatibility.
**Files changed:**
- `src/environment/env.py` — Added death-from-starvation logic (section 6), updated `new_state` to use `new_alive`, added `deaths_this_step` to info dict, renumbered subsequent sections
- `tests/test_energy.py` — Added `TestDeath` class with 7 tests
**Issues:** None. All 71 tests pass, mypy clean.

---

### US-006: Implement Food Restores Energy ✓
**What:** Food collection now adds `food_energy` to the closest alive agent's energy, capped at `max_energy`. Rewards changed from shared team reward to individual energy-gained per agent.
**Decisions:**
- Used Chebyshev distance to determine closest alive agent per food item. Ties broken by lowest agent index (via `jnp.argmin`).
- One-hot encoding + sum pattern to compute per-agent energy gain from potentially multiple food items — clean, JIT-friendly, no loops.
- Food energy is applied before energy drain: agent gains food energy, then loses `energy_per_step`.
- Energy capped at `max_energy` before drain.
- Rewards changed from shared (`num_collected` broadcast) to individual (`energy_gained` per agent, sliced to `num_agents`).
- Updated existing energy drain and death tests to set `food_energy=0` so random food proximity doesn't interfere with their assertions.
**Files changed:**
- `src/environment/env.py` — Rewrote food collection section (section 2) to track closest agent per food, compute per-agent energy gain, cap at max_energy. Updated energy drain (section 5) to use `energy_after_food`. Changed reward computation (section 3) to individual energy-gained.
- `tests/test_energy.py` — Added `_make_state_with_positions` helper and `TestFoodEnergy` class with 8 tests: basic energy gain, max energy cap, closest-agent selection, dead agents can't collect, multiple food items, individual rewards, no food nearby, JIT compatibility. Fixed existing drain/death tests to disable food energy.
**Issues:** None. All 79 tests pass, mypy clean.

---

### US-007: Implement Reproduction Action ✓
**What:** Added action 5 (reproduce) to the environment. Updated `ActorCritic` to output 6 actions. Reproduction succeeds when an alive agent chooses action 5, has energy >= `reproduce_threshold`, and a free slot exists. On success, `reproduce_cost` is deducted from the parent and an offspring is spawned in the first free slot with energy equal to `reproduce_cost`, a new unique ID, and a position adjacent to the parent.
**Decisions:**
- Action 5 = reproduce; agent stays in place (same as action 0 for movement).
- Reproduction is processed sequentially via `jax.lax.scan` over all agent indices — this handles slot allocation correctly (each birth fills a slot, reducing availability for subsequent agents in the same step).
- Reproduction check happens after energy drain and death (section 7), so agents that just died cannot reproduce, and energy threshold is checked against post-drain energy.
- Offspring position: random offset of (-1, 0, or 1) in each axis from parent, clipped to grid bounds.
- Offspring gets energy = `reproduce_cost` (the energy transferred from parent).
- `births_this_step` added to info dict.
- PRNG key is split from `state.key` for reproduction randomness.
- Note: US-008 (offspring spawning details) is effectively handled here too since the scan approach naturally spawns offspring during reproduction. US-008 may only need tests.
**Files changed:**
- `src/agents/network.py` — Changed default `num_actions` from 5 to 6
- `src/environment/env.py` — Added action 5 to action_deltas, added reproduction logic (section 7) with `jax.lax.scan`, updated state construction to use reproduction outputs, added `births_this_step` to info dict
- `tests/test_reproduction.py` — Created with 11 tests covering: action exists, energy deduction, failure below threshold, failure with no free slots, offspring spawning, offspring near parent, dead agents can't reproduce, no action 5 = no birth, multiple agents reproducing, JIT compatibility, interaction with energy drain
**Issues:** None. All 90 tests pass, mypy clean.

---

### US-008: Implement Offspring Spawning ✓
**What:** Verified offspring spawning implementation (already completed in US-007) and added dedicated `TestSpawn` test class with 10 tests covering all US-008 acceptance criteria.
**Decisions:**
- The spawning logic was already fully implemented in US-007's `_process_reproductions` scan function — it finds the first empty slot, sets offspring position (random adjacent to parent, clipped to grid), energy (`reproduce_cost`), alive flag, unique ID, and parent ID.
- Added `TestSpawn` class to `tests/test_reproduction.py` with 10 focused tests: first empty slot selection, adjacent position, grid boundary clipping, energy equals reproduce_cost, alive flag set, unique ID assignment, parent ID tracking, dead slot reuse, multi-generation spawning, and JIT compatibility.
**Files changed:**
- `tests/test_reproduction.py` — Added `TestSpawn` class with 10 tests for offspring spawning
**Issues:** None. All 100 tests pass, mypy clean.

---

### US-009: Implement Weight Inheritance ✓
**What:** Created `src/agents/reproduction.py` with weight mutation utilities. Added `agent_params` field to `EnvState` for per-agent network parameters. Integrated weight inheritance into the reproduction scan in `step()` — when `agent_params` is present, child params are copied from parent with Gaussian mutation.
**Decisions:**
- Created three functions in `reproduction.py`: `mutate_params` (adds Gaussian noise to all leaves of a param pytree), `copy_agent_params` (copies one agent slot to another in per-agent params), and `mutate_agent_params` (combined copy+mutate in one operation).
- Per-agent params are stored as a pytree where each leaf has shape `(max_agents, ...)`. This is stored in `EnvState.agent_params` (defaults to `None` when not using per-agent params).
- The reproduction scan in `env.py` has two branches: one with params mutation (when `agent_params is not None`) and one without (backwards-compatible with existing tests). This is a Python-level `if`, not a JAX conditional, so the branch is resolved at trace time.
- Mutation uses `jax.random.normal` scaled by `config.evolution.mutation_std` (default 0.01), applied independently to each leaf of the param pytree.
- `reset()` returns `agent_params=None`; the training code (US-011) will be responsible for initializing per-agent params by replicating single-agent params to `(max_agents, ...)`.
- Updated `_replace_field` in `ablation.py` to pass through `agent_params`.
**Files changed:**
- `src/agents/reproduction.py` — Created with `mutate_params`, `copy_agent_params`, `mutate_agent_params`
- `src/environment/state.py` — Added `agent_params: Any = None` field to `EnvState`
- `src/environment/env.py` — Imported `mutate_agent_params`, split reproduction scan into two branches (with/without params), integrated weight mutation on reproduction
- `src/analysis/ablation.py` — Updated `_replace_field` to include `agent_params`
- `tests/test_reproduction.py` — Added `TestInheritance` class with 9 tests: mutate_params noise, zero-std no change, copy_agent_params, mutate_agent_params, reproduction inherits params via step(), backward compat without params, JIT compatibility, multi-generation inheritance, mutate_params JIT
**Issues:** None. All 109 tests pass, mypy clean.

---

### US-010: Update Observations for Variable Population ✓
**What:** Updated `get_observations` to return `(max_agents, obs_dim)` with energy observation, dead agent masking, and full `max_agents`-width arrays throughout the pipeline.
**Decisions:**
- `obs_dim` increased by 1 to include normalized agent energy (position 3 in obs vector, index 2).
- `get_observations` now computes observations for all `max_agents` slots and zeros out dead agents via `jnp.where(alive_mask, obs, 0.0)`.
- `_compute_food_obs` updated to operate on all `max_agents` (dead agents get zeroed by the outer alive mask).
- Energy normalized as `clip(energy / max_energy, 0, 1)` — always in [0, 1].
- Rewards changed from `(num_agents,)` to `(max_agents,)` — dead agents get 0 reward via `jnp.where(agent_alive, energy_gained, 0.0)`. This keeps obs/actions/rewards dimensions consistent.
- Updated downstream test assertions for new reward and observation shapes (`max_agents` instead of `num_agents`).
- Created `tests/test_obs.py` with 20 tests: obs_dim includes energy, phase1 comparison, max_agents shape, different max_agents, energy normalized/max/zero/clamped, dead agents zero obs, killed agent zero, alive nonzero, mixed alive/dead, all obs in range, position normalized, energy nonneg, dead food obs zero, JIT compat, obs_dim consistency, after step, after death.
**Files changed:**
- `src/environment/obs.py` — Updated `obs_dim` (+1 for energy), `get_observations` (max_agents, energy, alive mask), `_compute_food_obs` (max_agents)
- `src/environment/env.py` — Changed rewards from `energy_gained[:num_agents]` to `jnp.where(agent_alive, energy_gained, 0.0)` for `(max_agents,)` shape
- `tests/test_obs.py` — Created with 20 tests across 7 test classes
- `tests/test_env.py` — Updated shape assertions for rewards and observations to use `max_agents`
- `tests/test_training.py` — Updated rollout batch shape assertions to use `max_agents`
**Issues:** None. All 129 tests pass, mypy clean.

---

### US-011: Update Training for Variable Population ✓
**What:** Updated the training pipeline (rollout, PPO loss, GAE, train_step) to handle variable population with alive mask. Added per-agent params initialization and synchronization. Fixed `num_actions` from 5 to 6 across training, integration tests, ablation, and video utilities.
**Decisions:**
- **Alive mask in rollout**: Collect `agent_alive` from env state at each step alongside other transition data. Shape: `(num_steps, num_envs, max_agents)`.
- **Masked PPO loss**: Added `_masked_mean` helper in `ppo.py` that computes `sum(x * mask) / max(sum(mask), 1)`. All PPO loss terms (policy loss, value loss, entropy, KL, clip fraction) use masked means when `alive_mask` is present in the batch. Backward-compatible: works without mask too.
- **Masked GAE**: Rather than modifying `gae.py`, mask rewards and values to zero for dead agents before passing to GAE. This produces zero advantages/returns for dead agents naturally. Bootstrap values also masked by final alive state.
- **Advantage normalization**: Updated minibatch advantage normalization to use masked mean and variance when alive mask is present.
- **Per-agent params**: Initialized per-agent params by replicating shared params to `(num_envs, max_agents, ...)` in `create_train_state`. After each PPO update, sync updated shared params to all alive agent slots (dead slots keep their existing params for potential reuse on spawn). Uses `flax.struct.dataclass.replace()` for env state updates.
- **num_actions=6**: Fixed all remaining hardcoded `num_actions=5` in `train.py` (both `create_train_state` and `train_step`), `test_integration.py`, `ablation.py` CLI, and `scripts/run_ablation.py`.
- **Rollout metrics**: `mean_reward`, `mean_value`, `mean_advantage`, `mean_return` now computed as masked averages over alive agents only. Added `mean_population` metric.
- **Steps per iteration**: Updated to use `max_agents` instead of `num_agents` since batch has `(max_agents,)` agent dimension.
- **Training banner**: Updated to Phase 2 with evolution info.
**Files changed:**
- `src/training/rollout.py` — Added `alive_mask` to transition dict in rollout collection
- `src/training/ppo.py` — Added `_masked_mean` helper, updated all loss/metric computations to use alive mask when present
- `src/training/train.py` — Fixed `num_actions` to 6, added per-agent params init/sync, added alive mask to GAE inputs and flat batch, masked advantage normalization, masked rollout metrics, added `mean_population` metric, updated banner/steps_per_iter
- `src/analysis/ablation.py` — Fixed `num_actions` from 5 to 6 in CLI main
- `scripts/run_ablation.py` — Fixed `num_actions` from 5 to 6
- `tests/test_integration.py` — Fixed `num_actions` from 5 to 6 in integration tests
**Issues:** None. All 129 tests pass, mypy clean.

---
