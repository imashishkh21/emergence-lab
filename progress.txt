# Ralph Progress Log

## Completed Tasks

### US-001: Create project structure [x]
- Created src/ folder with submodules: environment, agents, training, field, analysis
- All __init__.py files in place
- `python -c "import src"` works

### US-003: Create base config dataclasses [x]
- Created src/configs.py with all config dataclasses
- EnvConfig, FieldConfig, AgentConfig, TrainConfig, LogConfig, AnalysisConfig, Config
- Config.from_yaml() and to_yaml() implemented
- Typecheck passes

### US-002: Set up dependencies and virtual environment [x]
- Verified scripts/setup.sh creates venv, installs deps, verifies JAX
- Fixed mypy errors in src/configs.py: renamed `field` import to `dataclass_field` to avoid name collision with Config.field attribute
- Added types-PyYAML to dev dependencies in pyproject.toml
- Fixed `to_dict` type annotation and AgentConfig construction for mypy compliance
- All acceptance criteria pass: setup.sh runs clean, pip install -e . works, jax.devices() returns [CpuDevice(id=0)], mypy passes

### US-004: Implement FieldState dataclass [x]
- Created src/field/field.py with FieldState as @flax.struct.dataclass
- FieldState contains `values: jnp.ndarray` with shape (H, W, C)
- Implemented `create_field(height, width, channels)` factory function returning zero-initialized FieldState
- All TestFieldState tests pass (2/2)
- Typecheck passes

### US-005: Implement field dynamics (diffusion + decay) [x]
- Created src/field/dynamics.py with three functions: diffuse(), decay(), step_field()
- diffuse() uses 3x3 Gaussian kernel (1,2,1/2,4,2/1,2,1 normalized to sum=1), edge-padded, blended by rate
- decay() multiplies field values by (1 - rate)
- step_field() chains diffuse then decay
- All functions JIT-compatible (static loop over 3x3 kernel unrolled at trace time)
- Used FieldState constructor instead of .replace() to satisfy mypy
- All 5 TestFieldDynamics tests pass
- Typecheck passes

### US-006: Implement field read/write operations [x]
- Created src/field/ops.py with two functions: write_local() and read_local()
- write_local(field, positions, values): uses jnp.ndarray.at[].add() to write values at agent (row, col) positions; shape (N, 2) positions, (N, C) values
- read_local(field, positions, radius): reads local field values; radius=0 returns (N, C) at exact positions, radius>0 returns flattened (2r+1)x(2r+1) neighborhood as (N, (2r+1)^2 * C)
- Out-of-bounds reads are clamped to field edges via jnp.clip
- Both functions are JIT-compatible (no Python control flow for radius>0 case; radius is a static int)
- All 4 TestFieldOps tests pass, all 11 field tests pass
- Typecheck passes

### US-007: Implement EnvState dataclass [x]
- Created src/environment/state.py with EnvState as @flax.struct.dataclass
- EnvState fields: agent_positions (num_agents, 2), food_positions (num_food, 2), food_collected (num_food,) bool, field_state (FieldState), step (scalar int), key (PRNG key)
- Implemented create_env_state(key, config) factory function that randomly places agents and food on the grid, initializes field to zeros
- All TestEnvState tests pass (1/1)
- Typecheck passes

### US-008: Implement environment reset [x]
- Created src/environment/env.py with `reset(key, config) -> EnvState`
- Used `jax.random.permutation` over flattened grid indices to guarantee non-overlapping agent positions
- Food positions are random via `jax.random.randint` (may overlap each other, which is fine)
- Field initialized to zeros via `create_field()`
- Added 2 extra tests: `test_reset_field_initialized_fresh` and `test_reset_food_not_collected`
- All 5 TestEnvReset tests pass, typecheck passes
- Files changed: src/environment/env.py (new), tests/test_env.py (added tests)

### US-009: Implement environment step [x]
- Implemented `step(state, actions, config) -> (EnvState, rewards, dones, info)` in src/environment/env.py
- Actions map: 0=stay, 1=up(-row), 2=down(+row), 3=left(-col), 4=right(+col) using action_deltas lookup table
- Agent positions clipped to grid boundaries after movement
- Food collection uses Chebyshev distance <= 1 (within 1 cell in any direction)
- Reward: shared +1 per food collected this step, broadcast to all agents
- Field dynamics: step_field (diffuse + decay), then agents write presence (write_strength * ones for all channels)
- Done when step >= max_steps
- Info dict includes food_collected_this_step and total_food_collected
- All 3 TestEnvStep tests pass, mypy clean
- Files changed: src/environment/env.py (added step function + imports for step_field, write_local)

### US-010: Implement observation function [x]
- Created src/environment/obs.py with get_observations() and obs_dim() functions
- Observation components per agent:
  1. Own position (2 values): normalized to [-1, 1] via (pos / (grid_size-1)) * 2 - 1
  2. Local field values ((2*radius+1)^2 * channels values): read via read_local(), clipped to [-1, 1]
  3. K nearest uncollected food (K*3 = 15 values): relative (dx, dy) normalized by grid_size + availability flag
- K_NEAREST_FOOD = 5 (hardcoded constant)
- Food outside observation_radius or already collected is masked out
- Uses Chebyshev distance to determine food visibility range
- Total obs_dim with defaults: 2 + 484 + 15 = 501
- All TestObservations tests pass (2/2), mypy clean
- Files changed: src/environment/obs.py (new)

### US-011: Vectorize environment with vmap [x]
- Created src/environment/vec_env.py with VecEnv class
- VecEnv.__init__ takes Config, creates vmapped versions of reset() and step()
- reset(key) splits the key into num_envs sub-keys, vmaps over them
- step(states, actions) vmaps over (state, actions) pairs, config is shared
- Uses jax.vmap lambdas that close over self.config so config is not mapped
- Batch shapes: (num_envs, num_agents, ...) for positions, rewards, etc.
- All 3 TestVecEnv tests pass (reset, step, JIT-compatible)
- Typecheck passes (mypy clean)
- Files changed: src/environment/vec_env.py (new)

### US-REVIEW-01: Review Environment Epic [x]
- All 25 environment tests pass (14 test_env.py + 11 test_field.py)
- Mypy typecheck clean: no issues found in 15 source files
- Manual verification:
  - Created env with default config (8 agents, 20x20 grid, 10 food, 4 field channels)
  - Reset produces correct shapes: agent_positions (8,2), food_positions (10,2), field (20,20,4), obs (8,501)
  - Stepped 10 times: step counter increments, rewards fire on food collection, done=False before max_steps
- Field verification:
  - Initial field sum = 0.0 (correct)
  - After 1 step: field sum = 32.0, 32 nonzero cells (8 agents × 4 channels = 32 writes)
  - After 6 steps: field sum = 169.54, 572 nonzero cells (diffusion spreading values)
  - Field values show clear gradient around agent positions (diffusion working)
- Food collection verification:
  - 8 out of 10 food collected in 50 random-action steps
  - food_collected mask correctly tracks which food items are consumed
  - Rewards are shared across all agents (+1 per food item)
- No issues found. Environment epic is solid and ready for training integration.

### US-012: Implement actor-critic network [x]
- Created src/agents/network.py with ActorCritic as a Flax nn.Module
- Architecture: shared MLP backbone -> actor head (logits) + critic head (scalar value)
- Backbone: Dense -> LayerNorm -> Tanh for each hidden layer, configurable hidden_dims
- Actor head: Dense(num_actions) with orthogonal init scale 0.01 (small logits initially)
- Critic head: Dense(1) with orthogonal init scale 1.0, squeezed to scalar
- Hidden layers use orthogonal init with scale sqrt(2) (standard for ReLU/Tanh networks)
- All bias initializers are zeros
- Works with single obs (obs_dim,) and batched via vmap (batch, obs_dim)
- All 4 TestNetwork tests pass, mypy clean (16 source files, no issues)
- Files changed: src/agents/network.py (new)

### US-013: Implement action sampling [x]
- Created src/agents/policy.py with two functions: sample_actions() and get_deterministic_actions()
- sample_actions(network, params, obs, key): flattens (num_envs, num_agents, obs_dim) into batch, vmaps forward pass with shared params, samples from categorical distribution, returns (actions, log_probs, values, entropy) all reshaped to (num_envs, num_agents)
- get_deterministic_actions(network, params, obs): same forward pass but uses argmax instead of sampling
- Used jnp.asarray() on values output to satisfy mypy (flax apply return type is a union)
- All 4 TestActionSampling tests pass, all 8 agent tests pass
- Mypy clean (17 source files, no issues)
- Files changed: src/agents/policy.py (new)

### US-014: Implement GAE calculation [x]
- Created src/training/gae.py with compute_gae() function
- Implements Generalized Advantage Estimation using jax.lax.scan with reverse=True
- TD residuals: delta_t = r_t + gamma * V(s_{t+1}) * (1 - done_t) - V(s_t)
- Reverse scan accumulates: gae_t = delta_t + gamma * lambda * (1 - done_t) * gae_{t+1}
- Returns (advantages, returns) where returns = advantages + values[:T]
- Handles episode boundaries by zeroing out future contributions on done flags
- JIT-compatible (uses jax.lax.scan, no Python control flow)
- All 4 TestGAE tests pass, mypy clean (18 source files, no issues)
- Files changed: src/training/gae.py (new)

### US-015: Implement PPO loss function [x]
- Created src/training/ppo.py with ppo_loss() function
- Implements standard PPO clipped surrogate objective:
  - Forward pass through ActorCritic network to get logits and values
  - Computes new log probabilities from categorical distribution over logits
  - Policy loss: clipped surrogate objective using min(ratio * adv, clip(ratio) * adv)
  - Value loss: MSE between predicted values and target returns
  - Entropy bonus: -sum(p * log(p)) averaged over batch for exploration
  - Total loss: policy_loss + vf_coef * value_loss - ent_coef * entropy
- Metrics returned: policy_loss, value_loss, entropy, approx_kl, clip_fraction
- Used jnp.asarray() on network.apply output to satisfy mypy (Flax union return type)
- All 3 TestPPOLoss tests pass, mypy clean (19 source files, no issues)
- Files changed: src/training/ppo.py (new)

### US-016: Implement rollout collection [x]
- Created src/training/rollout.py with RunnerState and collect_rollout
- RunnerState is a flax.struct.dataclass holding: params, opt_state, env_state, last_obs, key
- collect_rollout uses jax.lax.scan over num_steps for efficient trajectory collection
- Each step: sample_actions → vec_env.step → get_observations → store transition
- Batch output contains: obs, actions, rewards, dones, values, log_probs
- Shapes: (num_steps, num_envs, num_agents, ...) for all batch entries
- collect_rollout takes (runner_state, network, vec_env, config) — config provides num_steps
- Fixed mypy error: scan carry type annotation was tuple[RunnerState] instead of RunnerState
- All 2 TestRollout tests pass, mypy clean (20 source files, no issues)
- Files changed: src/training/rollout.py (new)

### US-017: Implement training step [x]
- Created src/training/train.py with create_train_state() and train_step() functions
- create_train_state(config, key): initializes network, optimizer (adam + grad clipping), vectorized env, and returns RunnerState
- train_step(runner_state, config): full PPO iteration:
  1. Collects rollout via collect_rollout (num_steps transitions)
  2. Computes bootstrap value from last observation
  3. Computes GAE advantages/returns via vmap over env*agent pairs
  4. Flattens rollout into training batch (num_steps * num_envs * num_agents samples)
  5. Runs num_epochs of minibatch PPO updates using jax.lax.scan
  6. Each minibatch: shuffles, normalizes advantages, computes PPO loss + gradients, applies optimizer update
- Gradient clipping via optax.clip_by_global_norm(max_grad_norm)
- Advantage normalization per minibatch: (adv - mean) / (std + 1e-8)
- Metrics returned: total_loss, policy_loss, value_loss, entropy, approx_kl, clip_fraction, mean_reward, mean_value, mean_advantage, mean_return
- All 1 TestTrainStep test passes, all 10 training tests pass
- Mypy clean (21 source files, no issues)
- Files changed: src/training/train.py (new)

### US-018: Implement full training loop [x]
- Added `train(config)` function to src/training/train.py as the main training entry point
- Initializes: env, network, optimizer, runner_state via `create_train_state()`
- JIT-compiles `train_step` by wrapping in a closure that captures `config` (avoids hashability issues with static args)
- Loops for `total_steps` with tqdm progress bar (stderr)
- Logs metrics (reward, loss, entropy, value) at `log_interval` step intervals
- Prints training summary with final metrics at completion
- NaN/Inf detection halts training early with warning
- Added `main()` CLI entry point using `tyro.cli(Config)` for argument parsing
- `python -m src.training.train --train.total-steps 10000` runs successfully
- Warm-up JIT compilation with first iteration, then efficient loop
- All 10 training tests pass, mypy clean (21 source files, no issues)
- Files changed: src/training/train.py (added train(), main(), if __name__ block)

## Current Task
US-REVIEW-02: Review Training Epic

## Decisions Made
- Using JAX/Flax/Optax stack (not PyTorch)
- Dataclasses + YAML for config (not Hydra)
- W&B for experiment tracking
- Field has 4 channels by default
- 8 agents, 20x20 grid as starting point
- Shared rewards (team-based)

## Files Changed
- src/__init__.py
- src/configs.py (fixed mypy errors: renamed field import, fixed to_dict, fixed AgentConfig construction)
- src/environment/__init__.py
- src/agents/__init__.py
- src/training/__init__.py
- src/field/__init__.py
- src/analysis/__init__.py
- configs/default.yaml
- scripts/setup.sh
- pyproject.toml (added types-PyYAML to dev deps)
- tests/*.py (test stubs)
- src/field/field.py (FieldState dataclass and create_field)
- src/field/dynamics.py (diffuse, decay, step_field)
- src/field/ops.py (read_local, write_local)
- src/environment/state.py (EnvState dataclass and create_env_state)
- src/environment/env.py (added step function)
- src/environment/obs.py (get_observations, obs_dim)
- src/agents/network.py (ActorCritic module)
- src/agents/policy.py (sample_actions, get_deterministic_actions)
- src/training/rollout.py (RunnerState, collect_rollout)
- src/training/train.py (create_train_state, train_step)

## Key Design Notes
- The FIELD is the key innovation (shared medium between agents)
- Field has its own dynamics (diffusion + decay)
- Agents read/write to field locally
- We test if field develops structure encoding collective knowledge
- Ablation test will show if field matters

## Notes for Future Tasks
- US-004 through US-006: Field implementation is critical — this is our novel contribution
- US-009: Environment step must update field each timestep
- US-022-024: Analysis is how we prove emergence — metrics must be meaningful
