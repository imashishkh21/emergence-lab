{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emergence Lab - Autonomous Field OFF Training\n",
    "\n",
    "This notebook runs **ALL 30 Field OFF seeds automatically** in a single execution.\n",
    "\n",
    "**No manual intervention needed** - just run all cells and let it complete.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "- **FIELD_ENABLED = False** (hardcoded for ablation study)\n",
    "- **10 batches x 3 seeds = 30 seeds total**\n",
    "- **PROVEN 64-agent config** (grid=32, food=40, max_agents=64)\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. Open this notebook in Google Colab\n",
    "2. Runtime > Change runtime type > **TPU v5e** (or v6e) + **High-RAM**\n",
    "3. Run all cells (Ctrl+F9 or Runtime > Run all)\n",
    "4. Come back when complete (~10+ hours for 30 seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup\n",
    "\n",
    "Mount Google Drive, clone the repo, and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for checkpoint storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create checkpoint directory\n",
    "import os\n",
    "CHECKPOINT_BASE = '/content/drive/MyDrive/emergence-lab'\n",
    "os.makedirs(CHECKPOINT_BASE, exist_ok=True)\n",
    "print(f\"Checkpoint base: {CHECKPOINT_BASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repo if not already present\n",
    "REPO_DIR = '/content/emergence-lab'\n",
    "\n",
    "# CHANGE THIS to your GitHub username\n",
    "GITHUB_USERNAME = \"imashishkh\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone https://github.com/{GITHUB_USERNAME}/emergence-lab.git {REPO_DIR}\n",
    "else:\n",
    "    print(f\"Repo already exists at {REPO_DIR}\")\n",
    "    # Pull latest changes\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "# Change to repo directory\n",
    "os.chdir(REPO_DIR)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -e \".[dev]\" -q\n",
    "\n",
    "# Verify JAX sees TPU\n",
    "import jax\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Devices: {jax.devices()}\")\n",
    "print(f\"Device count: {jax.device_count()}\")\n",
    "\n",
    "# Check if TPU is available\n",
    "if 'tpu' in str(jax.devices()[0]).lower():\n",
    "    print(\"TPU detected!\")\n",
    "else:\n",
    "    print(\"WARNING: TPU not detected. Training will be slower on GPU/CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Autonomous Field OFF Training\n",
    "\n",
    "This cell runs ALL 10 batches (30 seeds) automatically.\n",
    "\n",
    "**FIELD_ENABLED = False** is hardcoded - this is the ablation condition."
   ]
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# TEST RUN - Verify parallel training works before full experiment\n# =============================================================================\n# This runs 2 seeds for 100K steps (~1-2 minutes) to catch errors early.\n# If this succeeds, the full 30-seed run should work.\n# =============================================================================\n\nprint(\"=\"*70)\nprint(\"TEST MODE: Running quick verification (Field OFF)...\")\nprint(\"=\"*70)\n\nimport time\nfrom src.configs import Config\nfrom src.training.parallel_train import ParallelTrainer\n\n# Minimal test config - MATCHES PROVEN 64-AGENT CONFIG\ntest_config = Config()\ntest_config.env.grid_size = 32\ntest_config.env.num_agents = 16\ntest_config.env.num_food = 40\ntest_config.evolution.enabled = True\ntest_config.evolution.max_agents = 64\ntest_config.evolution.starting_energy = 200\ntest_config.evolution.food_energy = 100\ntest_config.evolution.reproduce_threshold = 120\ntest_config.evolution.reproduce_cost = 40\ntest_config.train.num_envs = 32\ntest_config.train.num_steps = 128\ntest_config.log.wandb = False\n\n# FIELD OFF settings for test\ntest_config.field.decay_rate = 1.0\ntest_config.field.diffusion_rate = 0.0\ntest_config.field.write_strength = 0.0\n\n# Test parameters: 2 seeds, 100K steps\nTEST_STEPS = 100_000\nTEST_SEEDS = 2\nTEST_CHECKPOINT_DIR = f'{CHECKPOINT_BASE}/test_field_off'\n\nsteps_per_iter = 32 * 128 * 64\ntest_iterations = max(1, TEST_STEPS // steps_per_iter)\n\nprint(f\"Test config (Field OFF):\")\nprint(f\"  Seeds: {TEST_SEEDS}\")\nprint(f\"  Steps: {TEST_STEPS:,}\")\nprint(f\"  Iterations: {test_iterations}\")\nprint(f\"  Field: DISABLED (decay=1.0, diffusion=0.0, write=0.0)\")\nprint()\n\ntry:\n    test_trainer = ParallelTrainer(\n        config=test_config,\n        num_seeds=TEST_SEEDS,\n        seed_ids=[100, 101],\n        checkpoint_dir=TEST_CHECKPOINT_DIR,\n        master_seed=9999,\n    )\n    \n    t0 = time.time()\n    test_metrics = test_trainer.train(\n        num_iterations=test_iterations,\n        checkpoint_interval_minutes=60,\n        resume=False,\n        print_interval=5,\n    )\n    elapsed = time.time() - t0\n    \n    print()\n    print(\"=\"*70)\n    print(\"TEST PASSED!\")\n    print(\"=\"*70)\n    print(f\"Time: {elapsed:.1f}s\")\n    print(f\"Final rewards: {test_metrics.get('mean_reward', 'N/A')}\")\n    print(f\"Final alive: {test_metrics.get('alive_count', 'N/A')}\")\n    print()\n    print(\"Proceed to run the main autonomous training cell below.\")\n    print(\"=\"*70)\n    \nexcept Exception as e:\n    print()\n    print(\"=\"*70)\n    print(\"TEST FAILED!\")\n    print(\"=\"*70)\n    print(f\"Error: {e}\")\n    print()\n    print(\"DO NOT proceed with full training until this is fixed.\")\n    raise e",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# AUTONOMOUS FIELD OFF TRAINING - ALL 10 BATCHES\n",
    "# =============================================================================\n",
    "# This cell runs ALL 30 Field OFF seeds automatically.\n",
    "# No manual intervention needed - just run and wait.\n",
    "#\n",
    "# FIELD_ENABLED = False (hardcoded for ablation study)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from src.configs import Config\n",
    "from src.training.parallel_train import ParallelTrainer\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION (HARDCODED FOR FIELD OFF ABLATION)\n",
    "# =============================================================================\n",
    "\n",
    "# FIELD ABLATION - THIS IS THE KEY SETTING\n",
    "FIELD_ENABLED = False  # HARDCODED: Field OFF for ablation study\n",
    "\n",
    "# Training parameters\n",
    "TOTAL_STEPS = 10_000_000  # 10M steps per seed\n",
    "NUM_ENVS = 32\n",
    "NUM_STEPS = 128\n",
    "SEEDS_PER_BATCH = 3  # 3 seeds per batch for 64-agent config\n",
    "TOTAL_BATCHES = 10   # 10 batches = 30 seeds total\n",
    "\n",
    "# Checkpoint settings\n",
    "CHECKPOINT_DIR_BASE = f'{CHECKPOINT_BASE}/field_off'\n",
    "CHECKPOINT_INTERVAL_MINUTES = 30\n",
    "RESUME = True  # Resume from existing checkpoints if available\n",
    "\n",
    "# =============================================================================\n",
    "# BUILD CONFIG (PROVEN 64-AGENT CONFIG)\n",
    "# =============================================================================\n",
    "\n",
    "def build_config():\n",
    "    \"\"\"Build the PROVEN 64-agent config with Field OFF.\"\"\"\n",
    "    config = Config()\n",
    "    \n",
    "    # PROVEN 64-AGENT CONFIG\n",
    "    config.env.grid_size = 32             # Larger grid for 64 agents\n",
    "    config.env.num_agents = 16            # Starting population\n",
    "    config.env.num_food = 40              # More food for 64 agents\n",
    "    \n",
    "    # Evolution settings\n",
    "    config.evolution.enabled = True\n",
    "    config.evolution.max_agents = 64      # PROVEN: 64 achieved UTOPIA\n",
    "    config.evolution.starting_energy = 200\n",
    "    config.evolution.food_energy = 100\n",
    "    config.evolution.energy_per_step = 1\n",
    "    config.evolution.reproduce_threshold = 120\n",
    "    config.evolution.reproduce_cost = 40  # PROVEN: 40 not 50!\n",
    "    config.evolution.mutation_std = 0.01\n",
    "    \n",
    "    # Training parameters\n",
    "    config.train.total_steps = TOTAL_STEPS\n",
    "    config.train.num_envs = NUM_ENVS\n",
    "    config.train.num_steps = NUM_STEPS\n",
    "    config.train.seed = 42\n",
    "    \n",
    "    # FIELD OFF ABLATION - Zero out the field\n",
    "    config.field.decay_rate = 1.0         # Field decays completely each step\n",
    "    config.field.diffusion_rate = 0.0     # No diffusion\n",
    "    config.field.write_strength = 0.0     # Agents can't write to field\n",
    "    \n",
    "    # Logging\n",
    "    config.log.wandb = False\n",
    "    config.log.save_interval = 0\n",
    "    \n",
    "    return config\n",
    "\n",
    "# =============================================================================\n",
    "# RUN ALL BATCHES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AUTONOMOUS FIELD OFF TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Field Enabled: {FIELD_ENABLED} (ABLATION STUDY)\")\n",
    "print(f\"Total batches: {TOTAL_BATCHES}\")\n",
    "print(f\"Seeds per batch: {SEEDS_PER_BATCH}\")\n",
    "print(f\"Total seeds: {TOTAL_BATCHES * SEEDS_PER_BATCH}\")\n",
    "print(f\"Steps per seed: {TOTAL_STEPS:,}\")\n",
    "print(f\"Checkpoint base: {CHECKPOINT_DIR_BASE}\")\n",
    "print(f\"Resume: {RESUME}\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Build config once\n",
    "config = build_config()\n",
    "\n",
    "# Calculate iterations\n",
    "steps_per_iter = NUM_ENVS * NUM_STEPS * config.evolution.max_agents\n",
    "num_iterations = TOTAL_STEPS // steps_per_iter\n",
    "\n",
    "print(f\"Steps per iteration: {steps_per_iter:,}\")\n",
    "print(f\"Iterations per seed: {num_iterations:,}\")\n",
    "print()\n",
    "\n",
    "# Track results across all batches\n",
    "all_results = []\n",
    "start_time = time.time()\n",
    "\n",
    "# Run all 10 batches\n",
    "for batch_number in range(TOTAL_BATCHES):\n",
    "    batch_start = time.time()\n",
    "    \n",
    "    # Compute seed IDs for this batch\n",
    "    seed_ids = list(range(\n",
    "        batch_number * SEEDS_PER_BATCH,\n",
    "        (batch_number + 1) * SEEDS_PER_BATCH\n",
    "    ))\n",
    "    \n",
    "    # Checkpoint directory for this batch\n",
    "    checkpoint_dir = f'{CHECKPOINT_DIR_BASE}/batch_{batch_number}'\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"BATCH {batch_number + 1}/{TOTAL_BATCHES}\")\n",
    "    print(f\"Seeds: {seed_ids}\")\n",
    "    print(f\"Checkpoint dir: {checkpoint_dir}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        # Create trainer for this batch\n",
    "        trainer = ParallelTrainer(\n",
    "            config=config,\n",
    "            num_seeds=SEEDS_PER_BATCH,\n",
    "            seed_ids=seed_ids,\n",
    "            checkpoint_dir=checkpoint_dir,\n",
    "            master_seed=42 + batch_number * 1000,\n",
    "        )\n",
    "        \n",
    "        # Run training\n",
    "        metrics = trainer.train(\n",
    "            num_iterations=num_iterations,\n",
    "            checkpoint_interval_minutes=CHECKPOINT_INTERVAL_MINUTES,\n",
    "            resume=RESUME,\n",
    "            print_interval=100,  # Print every 100 iterations\n",
    "        )\n",
    "        \n",
    "        batch_elapsed = time.time() - batch_start\n",
    "        \n",
    "        # Store results\n",
    "        batch_result = {\n",
    "            'batch': batch_number,\n",
    "            'seed_ids': seed_ids,\n",
    "            'metrics': metrics,\n",
    "            'elapsed_seconds': batch_elapsed,\n",
    "            'success': True,\n",
    "        }\n",
    "        all_results.append(batch_result)\n",
    "        \n",
    "        # Print batch summary\n",
    "        print(f\"\\nBatch {batch_number} complete!\")\n",
    "        print(f\"  Time: {batch_elapsed/3600:.1f} hours\")\n",
    "        if 'mean_reward' in metrics:\n",
    "            print(f\"  Final rewards: {metrics['mean_reward']}\")\n",
    "        if 'alive_count' in metrics:\n",
    "            print(f\"  Final alive: {metrics['alive_count']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR in batch {batch_number}: {e}\")\n",
    "        batch_result = {\n",
    "            'batch': batch_number,\n",
    "            'seed_ids': seed_ids,\n",
    "            'error': str(e),\n",
    "            'success': False,\n",
    "        }\n",
    "        all_results.append(batch_result)\n",
    "        # Continue to next batch instead of stopping\n",
    "        continue\n",
    "    \n",
    "    # Progress estimate\n",
    "    total_elapsed = time.time() - start_time\n",
    "    batches_done = batch_number + 1\n",
    "    batches_remaining = TOTAL_BATCHES - batches_done\n",
    "    avg_time_per_batch = total_elapsed / batches_done\n",
    "    estimated_remaining = avg_time_per_batch * batches_remaining\n",
    "    \n",
    "    print(f\"\\nProgress: {batches_done}/{TOTAL_BATCHES} batches\")\n",
    "    print(f\"Elapsed: {total_elapsed/3600:.1f} hours\")\n",
    "    print(f\"Estimated remaining: {estimated_remaining/3600:.1f} hours\")\n",
    "    eta = datetime.now() + timedelta(seconds=estimated_remaining)\n",
    "    print(f\"ETA: {eta.strftime('%Y-%m-%d %H:%M')}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL BATCHES COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total time: {total_time/3600:.1f} hours\")\n",
    "print(f\"Successful batches: {sum(1 for r in all_results if r['success'])}/{TOTAL_BATCHES}\")\n",
    "print()\n",
    "\n",
    "# Save results summary\n",
    "results_path = f'{CHECKPOINT_DIR_BASE}/training_summary.pkl'\n",
    "with open(results_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'all_results': all_results,\n",
    "        'total_time_seconds': total_time,\n",
    "        'field_enabled': FIELD_ENABLED,\n",
    "        'total_steps': TOTAL_STEPS,\n",
    "        'config': {\n",
    "            'grid_size': config.env.grid_size,\n",
    "            'num_food': config.env.num_food,\n",
    "            'max_agents': config.evolution.max_agents,\n",
    "            'reproduce_threshold': config.evolution.reproduce_threshold,\n",
    "            'reproduce_cost': config.evolution.reproduce_cost,\n",
    "        },\n",
    "    }, f)\n",
    "print(f\"Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Final Summary\n",
    "\n",
    "Print detailed summary of all 30 seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL SUMMARY OF ALL 30 FIELD OFF SEEDS\n",
    "# =============================================================================\n",
    "\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FIELD OFF TRAINING - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Collect results from all seeds\n",
    "all_rewards = []\n",
    "all_alive = []\n",
    "seed_status = []\n",
    "\n",
    "for batch_number in range(10):\n",
    "    batch_dir = f'{CHECKPOINT_DIR_BASE}/batch_{batch_number}'\n",
    "    seed_ids = list(range(batch_number * 3, (batch_number + 1) * 3))\n",
    "    \n",
    "    print(f\"Batch {batch_number} (seeds {seed_ids[0]}-{seed_ids[-1]}):\")\n",
    "    \n",
    "    for seed_id in seed_ids:\n",
    "        seed_dir = os.path.join(batch_dir, f'seed_{seed_id}')\n",
    "        latest_path = os.path.join(seed_dir, 'latest.pkl')\n",
    "        \n",
    "        if os.path.exists(latest_path):\n",
    "            try:\n",
    "                with open(latest_path, 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                \n",
    "                step = data.get('step', 0)\n",
    "                progress = (step / TOTAL_STEPS) * 100\n",
    "                \n",
    "                # Get final metrics if available\n",
    "                reward = data.get('mean_reward', 'N/A')\n",
    "                alive = data.get('alive_count', 'N/A')\n",
    "                \n",
    "                status = 'COMPLETE' if progress >= 99 else f'{progress:.0f}%'\n",
    "                print(f\"  Seed {seed_id}: {status}, step {step:,}\")\n",
    "                \n",
    "                if progress >= 99:\n",
    "                    if isinstance(reward, (int, float)):\n",
    "                        all_rewards.append(reward)\n",
    "                    if isinstance(alive, (int, float)):\n",
    "                        all_alive.append(alive)\n",
    "                    seed_status.append((seed_id, 'complete'))\n",
    "                else:\n",
    "                    seed_status.append((seed_id, 'partial'))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Seed {seed_id}: ERROR - {e}\")\n",
    "                seed_status.append((seed_id, 'error'))\n",
    "        else:\n",
    "            print(f\"  Seed {seed_id}: NOT FOUND\")\n",
    "            seed_status.append((seed_id, 'missing'))\n",
    "    print()\n",
    "\n",
    "# Print aggregate statistics\n",
    "print(\"=\"*70)\n",
    "print(\"AGGREGATE STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "complete_count = sum(1 for _, status in seed_status if status == 'complete')\n",
    "partial_count = sum(1 for _, status in seed_status if status == 'partial')\n",
    "missing_count = sum(1 for _, status in seed_status if status == 'missing')\n",
    "error_count = sum(1 for _, status in seed_status if status == 'error')\n",
    "\n",
    "print(f\"Complete: {complete_count}/30 seeds\")\n",
    "print(f\"Partial: {partial_count}/30 seeds\")\n",
    "print(f\"Missing: {missing_count}/30 seeds\")\n",
    "print(f\"Errors: {error_count}/30 seeds\")\n",
    "print()\n",
    "\n",
    "if all_rewards:\n",
    "    print(f\"Mean reward (complete seeds): {np.mean(all_rewards):.4f} +/- {np.std(all_rewards):.4f}\")\n",
    "    print(f\"  Min: {np.min(all_rewards):.4f}\")\n",
    "    print(f\"  Max: {np.max(all_rewards):.4f}\")\n",
    "\n",
    "if all_alive:\n",
    "    print(f\"Mean alive count: {np.mean(all_alive):.1f} +/- {np.std(all_alive):.1f}\")\n",
    "    print(f\"  Min: {np.min(all_alive):.0f}\")\n",
    "    print(f\"  Max: {np.max(all_alive):.0f}\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. Ensure Field ON training is also complete (30 seeds)\")\n",
    "print(\"2. Download results to local machine\")\n",
    "print(\"3. Run analysis:\")\n",
    "print(\"   python scripts/generate_multi_seed_report.py \\\\\")\n",
    "print(\"       --checkpoint-dir /path/to/field_on \\\\\")\n",
    "print(\"       --compare-dir /path/to/field_off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick comparison with Field ON (if available)\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARISON WITH FIELD ON (if available)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "field_on_dir = f'{CHECKPOINT_BASE}/field_on'\n",
    "\n",
    "if os.path.exists(field_on_dir):\n",
    "    field_on_rewards = []\n",
    "    field_on_alive = []\n",
    "    field_on_complete = 0\n",
    "    \n",
    "    for batch_number in range(10):\n",
    "        batch_dir = f'{field_on_dir}/batch_{batch_number}'\n",
    "        for seed_id in range(batch_number * 3, (batch_number + 1) * 3):\n",
    "            latest_path = os.path.join(batch_dir, f'seed_{seed_id}', 'latest.pkl')\n",
    "            if os.path.exists(latest_path):\n",
    "                try:\n",
    "                    with open(latest_path, 'rb') as f:\n",
    "                        data = pickle.load(f)\n",
    "                    step = data.get('step', 0)\n",
    "                    if step >= TOTAL_STEPS * 0.99:\n",
    "                        field_on_complete += 1\n",
    "                        if 'mean_reward' in data:\n",
    "                            field_on_rewards.append(data['mean_reward'])\n",
    "                        if 'alive_count' in data:\n",
    "                            field_on_alive.append(data['alive_count'])\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    print(f\"Field ON complete seeds: {field_on_complete}/30\")\n",
    "    print(f\"Field OFF complete seeds: {complete_count}/30\")\n",
    "    print()\n",
    "    \n",
    "    if field_on_rewards and all_rewards:\n",
    "        print(\"REWARD COMPARISON:\")\n",
    "        print(f\"  Field ON:  {np.mean(field_on_rewards):.4f} +/- {np.std(field_on_rewards):.4f}\")\n",
    "        print(f\"  Field OFF: {np.mean(all_rewards):.4f} +/- {np.std(all_rewards):.4f}\")\n",
    "        diff = np.mean(field_on_rewards) - np.mean(all_rewards)\n",
    "        print(f\"  Difference: {diff:+.4f} (Field ON {'better' if diff > 0 else 'worse'})\")\n",
    "    \n",
    "    if field_on_alive and all_alive:\n",
    "        print(\"\\nALIVE COUNT COMPARISON:\")\n",
    "        print(f\"  Field ON:  {np.mean(field_on_alive):.1f} +/- {np.std(field_on_alive):.1f}\")\n",
    "        print(f\"  Field OFF: {np.mean(all_alive):.1f} +/- {np.std(all_alive):.1f}\")\n",
    "else:\n",
    "    print(\"Field ON results not found.\")\n",
    "    print(f\"Expected location: {field_on_dir}\")\n",
    "    print(\"Run Field ON training using colab_parallel_training.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V5E",
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}