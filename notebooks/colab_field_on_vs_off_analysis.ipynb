{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Field ON vs Field OFF: Statistical Comparison (v2 \u2014 Fresh Training)\n\n**v2 experiment**: Trains both conditions from scratch using the post-energy-fix codebase\nwith the biological pheromone system (Phase 6). Config loaded from sweep YAML if available,\notherwise falls back to v1 sweep values.\n\n30 seeds per condition (10 batches x 3 seeds), 10M steps each, 5 eval episodes per seed.\nMetrics: reward, population, trail_strength, survival_rate.\n\n**Key changes from v1:**\n- No hardcoded data \u2014 all metrics from fresh training + eval episodes\n- Field ON config uses sweep-optimized pheromone parameters\n- Field OFF inherits ALL params from Field ON (controlled experiment)\n- Energy fixes: crop refuel + free write steps\n- New metrics: trail_strength, survival_rate\n\n## Setup\n1. Runtime > Change runtime type > **TPU v6e** + **High-RAM**\n2. Run all cells (Ctrl+F9)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import drive\ndrive.mount('/content/drive')\n\nimport os\nREPO_DIR = '/content/emergence-lab'\nGITHUB_USERNAME = \"imashishkh21\"\n\nif not os.path.exists(REPO_DIR):\n    !git clone https://github.com/{GITHUB_USERNAME}/emergence-lab.git {REPO_DIR}\nelse:\n    !cd {REPO_DIR} && git pull\n\nos.chdir(REPO_DIR)\n!pip install -e \".[dev,phase5]\" -q\nprint(f\"Working directory: {os.getcwd()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "import gc\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import traceback\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from src.configs import Config, TrainingMode\n",
    "from src.training.parallel_train import ParallelTrainer\n",
    "from src.agents.network import ActorCritic\n",
    "from src.agents.policy import sample_actions\n",
    "from src.environment.env import reset, step\n",
    "from src.environment.obs import get_observations\n",
    "from src.analysis.statistics import (\n",
    "    compute_iqm, compare_methods, welch_t_test, mann_whitney_test,\n",
    "    probability_of_improvement,\n",
    ")\n",
    "from src.analysis.paper_figures import (\n",
    "    setup_publication_style, plot_performance_profiles,\n",
    "    save_figure,\n",
    ")\n",
    "\n",
    "# paper_figures.py sets matplotlib backend to Agg at import time.\n",
    "# Override to inline for Colab display.\n",
    "%matplotlib inline\n",
    "\n",
    "# --- Paths ---\n",
    "DRIVE_BASE = '/content/drive/MyDrive/emergence-lab/field_on_vs_off_v2'\n",
    "FIELD_ON_DIR = os.path.join(DRIVE_BASE, 'field_on')\n",
    "FIELD_OFF_DIR = os.path.join(DRIVE_BASE, 'field_off')\n",
    "OUTPUT_DIR = os.path.join(DRIVE_BASE, 'analysis_results')\n",
    "BEST_CONFIG_PATH = '/content/drive/MyDrive/emergence-lab/pheromone_best_config_v2.yaml'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(FIELD_ON_DIR, exist_ok=True)\n",
    "os.makedirs(FIELD_OFF_DIR, exist_ok=True)\n",
    "\n",
    "# --- Training constants ---\n",
    "NUM_ENVS = 32\n",
    "NUM_STEPS = 128\n",
    "MAX_AGENTS = 64\n",
    "TOTAL_STEPS = 10_000_000\n",
    "SEEDS_PER_BATCH = 3\n",
    "TOTAL_BATCHES = 10  # 30 seeds total\n",
    "NUM_EVAL_EPISODES = 5\n",
    "STEPS_PER_ITER = NUM_ENVS * NUM_STEPS * MAX_AGENTS  # 262,144\n",
    "NUM_ITERATIONS = math.ceil(TOTAL_STEPS / STEPS_PER_ITER)  # ~39\n",
    "\n",
    "print(\"Imports loaded.\")\n",
    "print(f\"Drive base:    {DRIVE_BASE}\")\n",
    "print(f\"Field ON dir:  {FIELD_ON_DIR}\")\n",
    "print(f\"Field OFF dir: {FIELD_OFF_DIR}\")\n",
    "print(f\"Output dir:    {OUTPUT_DIR}\")\n",
    "print(f\"Best config:   {BEST_CONFIG_PATH} (exists: {os.path.exists(BEST_CONFIG_PATH)})\")\n",
    "print(f\"\\nTraining plan: {TOTAL_BATCHES} batches x {SEEDS_PER_BATCH} seeds = \"\n",
    "      f\"{TOTAL_BATCHES * SEEDS_PER_BATCH} seeds per condition\")\n",
    "print(f\"Steps per seed: {TOTAL_STEPS:,} ({NUM_ITERATIONS} iterations)\")\n",
    "print(f\"Eval episodes per seed: {NUM_EVAL_EPISODES}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Config Builders + Eval Functions + Helpers\n# =============================================================================\n\n# --- Part A: Config builder with YAML fallback ---\n\ndef build_field_on_config() -> Config:\n    \"\"\"Build Field ON config: sweep-optimized field + nest params.\n\n    Loads from pheromone_best_config.yaml if the v2 sweep has completed.\n    Falls back to v1 sweep best values with a warning otherwise.\n    \"\"\"\n    cfg = Config()\n    # Environment\n    cfg.env.grid_size = 40\n    cfg.env.num_agents = 16\n    cfg.env.num_food = 25\n    cfg.env.max_steps = 500\n    # Evolution (survival-friendly)\n    cfg.evolution.enabled = True\n    cfg.evolution.food_energy = 100\n    cfg.evolution.starting_energy = 200\n    cfg.evolution.max_energy = 300\n    cfg.evolution.reproduce_threshold = 180\n    cfg.evolution.reproduce_cost = 80\n    cfg.evolution.energy_per_step = 1\n    cfg.evolution.max_agents = MAX_AGENTS\n    cfg.evolution.mutation_std = 0.01\n    # Training\n    cfg.train.training_mode = TrainingMode.GRADIENT\n    cfg.train.num_envs = NUM_ENVS\n    cfg.train.num_steps = NUM_STEPS\n    cfg.train.total_steps = TOTAL_STEPS\n    cfg.log.wandb = False\n    cfg.log.save_interval = 0\n    # Field base\n    cfg.field.num_channels = 4\n    cfg.field.field_value_cap = 1.0\n\n    # Field + Nest: load from sweep YAML or fall back to v1 values\n    if os.path.exists(BEST_CONFIG_PATH):\n        print(f\"Loading best sweep config from {BEST_CONFIG_PATH}\")\n        with open(BEST_CONFIG_PATH, 'r') as f:\n            best = yaml.safe_load(f)\n        if 'field' in best:\n            for k, v in best['field'].items():\n                if isinstance(v, list):\n                    v = tuple(v)\n                if hasattr(cfg.field, k):\n                    setattr(cfg.field, k, v)\n        if 'nest' in best:\n            for k, v in best['nest'].items():\n                if hasattr(cfg.nest, k):\n                    setattr(cfg.nest, k, v)\n        print(\"  Loaded sweep-optimized field + nest config\")\n    else:\n        print(f\"WARNING: {BEST_CONFIG_PATH} not found.\")\n        print(\"  Using v1 sweep values. Re-run after v2 sweep completes for best results.\")\n        cfg.field.channel_diffusion_rates = (0.7, 0.01, 0.0, 0.0)\n        cfg.field.channel_decay_rates = (0.08, 0.0001, 0.0, 0.0)\n        cfg.field.territory_write_strength = 0.05\n        cfg.nest.radius = 4\n        cfg.nest.compass_noise_rate = 0.15\n\n    return cfg\n\n\ndef build_field_off_config() -> Config:\n    \"\"\"Same as Field ON but with field channels disabled.\n\n    CRITICAL: inherits ALL params from build_field_on_config() including\n    nest radius, compass noise, etc. The ONLY difference is that pheromone\n    channels are zeroed out. This ensures a controlled experiment where\n    the field is the sole independent variable.\n    \"\"\"\n    cfg = build_field_on_config()\n    cfg.field.channel_diffusion_rates = (0.0, 0.0, 0.0, 0.0)\n    cfg.field.channel_decay_rates = (1.0, 1.0, 1.0, 1.0)\n    cfg.field.territory_write_strength = 0.0\n    return cfg\n\n\n# Print config summary showing both conditions share non-field params\nprint(\"=\" * 70)\nprint(\"CONFIG SUMMARY\")\nprint(\"=\" * 70)\ncfg_on = build_field_on_config()\ncfg_off = build_field_off_config()\n\nprint(f\"\\n{'Parameter':<40} {'Field ON':>15} {'Field OFF':>15} {'Match?':>8}\")\nprint(\"-\" * 80)\n# Shared params (should match)\nfor label, on_val, off_val in [\n    (\"env.grid_size\", cfg_on.env.grid_size, cfg_off.env.grid_size),\n    (\"env.num_agents\", cfg_on.env.num_agents, cfg_off.env.num_agents),\n    (\"env.num_food\", cfg_on.env.num_food, cfg_off.env.num_food),\n    (\"evolution.max_agents\", cfg_on.evolution.max_agents, cfg_off.evolution.max_agents),\n    (\"evolution.food_energy\", cfg_on.evolution.food_energy, cfg_off.evolution.food_energy),\n    (\"evolution.reproduce_threshold\", cfg_on.evolution.reproduce_threshold, cfg_off.evolution.reproduce_threshold),\n    (\"nest.radius\", cfg_on.nest.radius, cfg_off.nest.radius),\n    (\"nest.compass_noise_rate\", cfg_on.nest.compass_noise_rate, cfg_off.nest.compass_noise_rate),\n    (\"nest.food_sip_fraction\", cfg_on.nest.food_sip_fraction, cfg_off.nest.food_sip_fraction),\n]:\n    match = \"YES\" if on_val == off_val else \"NO!\"\n    print(f\"  {label:<38} {str(on_val):>15} {str(off_val):>15} {match:>8}\")\n\n# Field params (should differ)\nprint(\"\\nField-specific params (should differ):\")\nfor label, on_val, off_val in [\n    (\"field.channel_diffusion_rates\", cfg_on.field.channel_diffusion_rates, cfg_off.field.channel_diffusion_rates),\n    (\"field.channel_decay_rates\", cfg_on.field.channel_decay_rates, cfg_off.field.channel_decay_rates),\n    (\"field.territory_write_strength\", cfg_on.field.territory_write_strength, cfg_off.field.territory_write_strength),\n]:\n    print(f\"  {label:<38} {str(on_val):>15} {str(off_val):>15}\")\n\ndel cfg_on, cfg_off  # Free memory\n\n\n# --- Part B: run_eval() function ---\n\ndef run_eval(network, params, config, key, num_steps=500):\n    \"\"\"Run a single eval episode using lax.scan.\"\"\"\n    key, reset_key = jax.random.split(key)\n    init_state = reset(reset_key, config)\n\n    def _eval_step(carry, _unused):\n        state, rng, total_reward = carry\n        obs = get_observations(state, config)\n        obs_batched = obs[None, :, :]\n        rng, act_key = jax.random.split(rng)\n        actions, _, _, _ = sample_actions(network, params, obs_batched, act_key)\n        actions = actions[0]\n        state, rewards, done, info = step(state, actions, config)\n        alive = state.agent_alive.astype(jnp.float32)\n        total_reward = total_reward + jnp.sum(rewards * alive)\n        return (state, rng, total_reward), None\n\n    (final_state, _, total_reward), _ = jax.lax.scan(\n        _eval_step, (init_state, key, jnp.float32(0.0)), None, length=num_steps,\n    )\n\n    ch0 = jnp.asarray(final_state.field_state.values[:, :, 0])\n    nonzero_mask = ch0 > 0.01\n    trail_strength = jnp.where(\n        jnp.any(nonzero_mask),\n        jnp.sum(jnp.where(nonzero_mask, ch0, 0.0)) /\n          jnp.maximum(jnp.sum(nonzero_mask.astype(jnp.float32)), 1.0),\n        0.0,\n    )\n    final_pop = jnp.sum(final_state.agent_alive.astype(jnp.int32))\n\n    return {\n        'total_reward': float(total_reward),\n        'final_population': int(final_pop),\n        'trail_strength': float(trail_strength),\n        'survival_rate': float(final_pop) / config.env.num_agents,\n    }\n\n\n# --- Part C: eval_after_batch() helper ---\n\ndef eval_after_batch(trainer, config, seed_ids, num_episodes=NUM_EVAL_EPISODES):\n    \"\"\"Run eval episodes for each seed after a training batch completes.\n\n    Extracts per-seed params from trainer._parallel_state (internal API --\n    no public accessor exists; save_checkpoints uses the same pattern).\n    \"\"\"\n    network = ActorCritic(\n        hidden_dims=tuple(config.agent.hidden_dims),\n        num_actions=config.agent.num_actions,\n    )\n    seed_evals = []\n    for i, seed_id in enumerate(seed_ids):\n        # Internal access: ParallelTrainer has no public API for per-seed params\n        seed_params = jax.tree.map(lambda x: x[i], trainer._parallel_state.params)\n        ep_results = []\n        for ep in range(num_episodes):\n            key = jax.random.PRNGKey(seed_id * 1000 + ep)\n            result = run_eval(network, seed_params, config, key)\n            ep_results.append(result)\n        seed_evals.append({\n            'seed_id': seed_id,\n            'total_reward': float(np.mean([r['total_reward'] for r in ep_results])),\n            'final_population': float(np.mean([r['final_population'] for r in ep_results])),\n            'trail_strength': float(np.mean([r['trail_strength'] for r in ep_results])),\n            'survival_rate': float(np.mean([r['survival_rate'] for r in ep_results])),\n            'all_episodes': ep_results,\n        })\n        print(f\"    Seed {seed_id}: reward={seed_evals[-1]['total_reward']:.1f}, \"\n              f\"pop={seed_evals[-1]['final_population']:.1f}, \"\n              f\"trail={seed_evals[-1]['trail_strength']:.3f}\")\n    return seed_evals\n\n\n# --- Part D: Early sanity check helper ---\n\ndef sanity_check_batch(seed_evals, condition_name, batch_number):\n    \"\"\"Print loud warning if batch 0 looks broken (zero pop or zero reward).\"\"\"\n    if batch_number != 0:\n        return\n    for s in seed_evals:\n        if s['final_population'] == 0 and s['total_reward'] == 0:\n            print(\"\\n\" + \"!\" * 70)\n            print(f\"WARNING: {condition_name} seed {s['seed_id']} has 0 population AND 0 reward!\")\n            print(\"Training may be broken. Check config and environment setup.\")\n            print(\"!\" * 70 + \"\\n\")\n    avg_pop = np.mean([s['final_population'] for s in seed_evals])\n    avg_rew = np.mean([s['total_reward'] for s in seed_evals])\n    if avg_pop < 2:\n        print(\"\\n\" + \"!\" * 70)\n        print(f\"WARNING: {condition_name} batch 0 avg population = {avg_pop:.1f}\")\n        print(\"Population near-zero. Consider: increase food_energy, decrease energy_per_step\")\n        print(\"!\" * 70 + \"\\n\")\n    else:\n        print(f\"  Sanity check OK: batch 0 avg pop={avg_pop:.1f}, avg reward={avg_rew:.1f}\")\n\n\nprint(\"\\nConfig builders + eval functions ready.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Phase 1: Train Field ON (30 seeds, 10M steps) + Eval\n\n10 batches x 3 seeds. Each batch trains to completion, then runs 5 eval episodes per seed.\nResume-safe: skips batches that already have results in pickle."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== FIELD ON: TRAINING + EVAL ==========\nRESULTS_PATH_ON = os.path.join(FIELD_ON_DIR, 'eval_results.pkl')\nif os.path.exists(RESULTS_PATH_ON):\n    with open(RESULTS_PATH_ON, 'rb') as f:\n        all_results_on = pickle.load(f)\nelse:\n    all_results_on = []\n\ncompleted_batches_on = {r['batch'] for r in all_results_on if r.get('success')}\nconfig_on = build_field_on_config()\n\nfor batch_number in range(TOTAL_BATCHES):\n    if batch_number in completed_batches_on:\n        print(f\"[Batch {batch_number}] SKIPPED (already completed)\")\n        continue\n\n    seed_ids = list(range(batch_number * SEEDS_PER_BATCH,\n                          (batch_number + 1) * SEEDS_PER_BATCH))\n    checkpoint_dir = os.path.join(FIELD_ON_DIR, f'batch_{batch_number}')\n\n    print(f\"\\n{'='*60}\")\n    print(f\"[Batch {batch_number}] Training Field ON seeds {seed_ids}\")\n    print(f\"{'='*60}\")\n\n    try:\n        trainer = ParallelTrainer(\n            config=config_on, num_seeds=SEEDS_PER_BATCH,\n            seed_ids=seed_ids, checkpoint_dir=checkpoint_dir,\n            master_seed=42 + batch_number * 1000,\n        )\n        train_metrics = trainer.train(\n            num_iterations=NUM_ITERATIONS,\n            checkpoint_interval_minutes=30,\n            resume=True, print_interval=5,\n        )\n        # Run proper eval episodes (not training metrics)\n        print(f\"  Running {NUM_EVAL_EPISODES} eval episodes per seed...\")\n        seed_evals = eval_after_batch(trainer, config_on, seed_ids)\n        sanity_check_batch(seed_evals, \"Field ON\", batch_number)\n\n        all_results_on.append({\n            'batch': batch_number, 'seed_ids': seed_ids,\n            'eval': seed_evals,\n            'train_metrics': train_metrics,\n            'success': True,\n        })\n    except Exception as e:\n        traceback.print_exc()\n        all_results_on.append({\n            'batch': batch_number, 'seed_ids': seed_ids,\n            'error': str(e), 'success': False,\n        })\n    finally:\n        try:\n            del trainer\n        except NameError:\n            pass\n        gc.collect()\n        jax.clear_caches()\n\n    # Atomic save after each batch\n    tmp = RESULTS_PATH_ON + '.tmp'\n    with open(tmp, 'wb') as f:\n        pickle.dump(all_results_on, f, protocol=pickle.HIGHEST_PROTOCOL)\n    os.replace(tmp, RESULTS_PATH_ON)\n\nprint(f\"\\nField ON: {len([r for r in all_results_on if r.get('success')])} / \"\n      f\"{TOTAL_BATCHES} batches completed\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Phase 2: Train Field OFF (30 seeds, 10M steps) + Eval\n\nSame structure as Phase 1 but with field channels disabled.\nAll other params (env, evolution, nest) are identical to Field ON.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ========== FIELD OFF: TRAINING + EVAL ==========\nRESULTS_PATH_OFF = os.path.join(FIELD_OFF_DIR, 'eval_results.pkl')\nif os.path.exists(RESULTS_PATH_OFF):\n    with open(RESULTS_PATH_OFF, 'rb') as f:\n        all_results_off = pickle.load(f)\nelse:\n    all_results_off = []\n\ncompleted_batches_off = {r['batch'] for r in all_results_off if r.get('success')}\nconfig_off = build_field_off_config()\n\nfor batch_number in range(TOTAL_BATCHES):\n    if batch_number in completed_batches_off:\n        print(f\"[Batch {batch_number}] SKIPPED (already completed)\")\n        continue\n\n    seed_ids = list(range(batch_number * SEEDS_PER_BATCH,\n                          (batch_number + 1) * SEEDS_PER_BATCH))\n    checkpoint_dir = os.path.join(FIELD_OFF_DIR, f'batch_{batch_number}')\n\n    print(f\"\\n{'='*60}\")\n    print(f\"[Batch {batch_number}] Training Field OFF seeds {seed_ids}\")\n    print(f\"{'='*60}\")\n\n    try:\n        trainer = ParallelTrainer(\n            config=config_off, num_seeds=SEEDS_PER_BATCH,\n            seed_ids=seed_ids, checkpoint_dir=checkpoint_dir,\n            master_seed=42 + batch_number * 1000,\n        )\n        train_metrics = trainer.train(\n            num_iterations=NUM_ITERATIONS,\n            checkpoint_interval_minutes=30,\n            resume=True, print_interval=5,\n        )\n        # Run proper eval episodes (not training metrics)\n        print(f\"  Running {NUM_EVAL_EPISODES} eval episodes per seed...\")\n        seed_evals = eval_after_batch(trainer, config_off, seed_ids)\n        sanity_check_batch(seed_evals, \"Field OFF\", batch_number)\n\n        all_results_off.append({\n            'batch': batch_number, 'seed_ids': seed_ids,\n            'eval': seed_evals,\n            'train_metrics': train_metrics,\n            'success': True,\n        })\n    except Exception as e:\n        traceback.print_exc()\n        all_results_off.append({\n            'batch': batch_number, 'seed_ids': seed_ids,\n            'error': str(e), 'success': False,\n        })\n    finally:\n        try:\n            del trainer\n        except NameError:\n            pass\n        gc.collect()\n        jax.clear_caches()\n\n    # Atomic save after each batch\n    tmp = RESULTS_PATH_OFF + '.tmp'\n    with open(tmp, 'wb') as f:\n        pickle.dump(all_results_off, f, protocol=pickle.HIGHEST_PROTOCOL)\n    os.replace(tmp, RESULTS_PATH_OFF)\n\nprint(f\"\\nField OFF: {len([r for r in all_results_off if r.get('success')])} / \"\n      f\"{TOTAL_BATCHES} batches completed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ========== EXTRACT EVAL METRICS INTO ARRAYS ==========\nFAILED_THRESHOLD = 1.0  # Seeds with reward below this are considered \"failed\"\n\ndef extract_eval_arrays(all_results):\n    \"\"\"Extract per-seed metric arrays from batch eval results.\"\"\"\n    seeds = []\n    for r in sorted(all_results, key=lambda x: x['batch']):\n        if not r.get('success'):\n            continue\n        for s in r['eval']:\n            seeds.append(s)\n    rewards = np.array([s['total_reward'] for s in seeds])\n    populations = np.array([s['final_population'] for s in seeds])\n    trail_strengths = np.array([s['trail_strength'] for s in seeds])\n    survival_rates = np.array([s['survival_rate'] for s in seeds])\n    return rewards, populations, trail_strengths, survival_rates\n\nfield_on_rewards, field_on_populations, field_on_trails, field_on_survival = \\\n    extract_eval_arrays(all_results_on)\nfield_off_rewards, field_off_populations, field_off_trails, field_off_survival = \\\n    extract_eval_arrays(all_results_off)\n\n# Alias for downstream analysis cells\nfield_off_populations_train = field_off_populations\n\nassert len(field_on_rewards) == 30, f\"Expected 30 Field ON seeds, got {len(field_on_rewards)}\"\nassert len(field_off_rewards) == 30, f\"Expected 30 Field OFF seeds, got {len(field_off_rewards)}\"\n\n# Print summary table\nprint(\"=\" * 80)\nprint(\"EVAL METRICS SUMMARY\")\nprint(\"=\" * 80)\nprint(f\"\\n{'Metric':<25} {'Field ON':>25} {'Field OFF':>25}\")\nprint(\"-\" * 80)\nfor label, on_arr, off_arr in [\n    (\"Reward\", field_on_rewards, field_off_rewards),\n    (\"Population\", field_on_populations, field_off_populations),\n    (\"Trail Strength\", field_on_trails, field_off_trails),\n    (\"Survival Rate\", field_on_survival, field_off_survival),\n]:\n    print(f\"  {label:<23} {on_arr.mean():>10.3f} +/- {on_arr.std():<10.3f} \"\n          f\"{off_arr.mean():>10.3f} +/- {off_arr.std():<10.3f}\")\n\nprint(f\"\\nTotal seeds: {len(field_on_rewards)} ON, {len(field_off_rewards)} OFF\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== DESCRIPTIVE STATISTICS ==========\nprint(\"=\"*60)\nprint(\"DESCRIPTIVE STATISTICS\")\nprint(\"=\"*60)\n\nfor name, rewards in [(\"Field ON\", field_on_rewards), (\"Field OFF\", field_off_rewards)]:\n    iqm = compute_iqm(rewards, n_bootstrap=10000, seed=42)\n    print(f\"\\n{name} (n={len(rewards)}):\")\n    print(f\"  Mean:   {rewards.mean():.4f} +/- {rewards.std(ddof=1):.4f}\")\n    print(f\"  Median: {np.median(rewards):.4f}\")\n    print(f\"  IQM:    {iqm.iqm:.4f} [{iqm.ci_lower:.4f}, {iqm.ci_upper:.4f}]\")\n    print(f\"  Min:    {rewards.min():.4f}\")\n    print(f\"  Max:    {rewards.max():.4f}\")\n    print(f\"  CoV:    {rewards.std(ddof=1)/rewards.mean():.4f}\")\n\n# Compute and store IQMs for later\niqm_on = compute_iqm(field_on_rewards, n_bootstrap=10000, seed=42)\niqm_off = compute_iqm(field_off_rewards, n_bootstrap=10000, seed=42)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== HYPOTHESIS TESTS ==========\nprint(\"=\"*60)\nprint(\"HYPOTHESIS TESTS\")\nprint(\"=\"*60)\n\n# Welch's t-test\nwelch = welch_t_test(field_off_rewards, field_on_rewards)\nprint(f\"\\n1. Welch's t-test:\")\nprint(f\"   t = {welch.statistic:.4f}, p = {welch.p_value:.6f}\")\nprint(f\"   Cohen's d = {welch.effect_size:.4f}\", end=\"\")\nd = abs(welch.effect_size)\nif d < 0.2: print(\" (negligible)\")\nelif d < 0.5: print(\" (small)\")\nelif d < 0.8: print(\" (MEDIUM)\")\nelse: print(\" (LARGE)\")\nprint(f\"   Significant at alpha=0.05: {welch.significant}\")\n\n# Mann-Whitney U\nmw = mann_whitney_test(field_off_rewards, field_on_rewards)\nprint(f\"\\n2. Mann-Whitney U test:\")\nprint(f\"   U = {mw.statistic:.1f}, p = {mw.p_value:.6f}\")\nprint(f\"   Rank-biserial r = {mw.effect_size:.4f}\")\nprint(f\"   Significant at alpha=0.05: {mw.significant}\")\n\n# Probability of Improvement\npoi = probability_of_improvement(field_off_rewards, field_on_rewards, n_bootstrap=5000, seed=42)\nprint(f\"\\n3. Probability of Improvement:\")\nprint(f\"   P(Field OFF > Field ON) = {poi['prob_x_better']:.4f}\")\nprint(f\"   P(Field ON > Field OFF) = {poi['prob_y_better']:.4f}\")\nprint(f\"   95% CI: [{poi['ci_lower']:.4f}, {poi['ci_upper']:.4f}]\")\n\n# Direction\nprint(f\"\\n4. Direction:\")\nprint(f\"   Field OFF mean: {field_off_rewards.mean():.4f}\")\nprint(f\"   Field ON mean:  {field_on_rewards.mean():.4f}\")\nprint(f\"   Gap: {field_off_rewards.mean() - field_on_rewards.mean():+.4f} (Field OFF {'higher' if field_off_rewards.mean() > field_on_rewards.mean() else 'lower'})\")\n\n# Sensitivity: exclude failed seeds (using consistent threshold)\nmask_on = field_on_rewards >= FAILED_THRESHOLD\non_filtered = field_on_rewards[mask_on]\nwelch_f = welch_t_test(field_off_rewards, on_filtered)\nprint(f\"\\n5. Sensitivity (excluding {np.sum(~mask_on)} failed Field ON seeds, threshold={FAILED_THRESHOLD}):\")\nprint(f\"   Field ON filtered: n={len(on_filtered)}, mean={on_filtered.mean():.4f}\")\nprint(f\"   Welch p = {welch_f.p_value:.6f}, Cohen's d = {welch_f.effect_size:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== FULL METHOD COMPARISON ==========\n",
    "print(\"=\"*60)\n",
    "print(\"FULL METHOD COMPARISON (rliable-style)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison = compare_methods(\n",
    "    {\"Field ON\": field_on_rewards, \"Field OFF\": field_off_rewards},\n",
    "    n_bootstrap=10000, seed=42,\n",
    ")\n",
    "print(comparison.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Trail Strength & Survival Analysis\n\nTrail strength: Field ON should show actual pheromone trails; OFF should be ~0.\nSurvival rate: final population / starting agents.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ========== TRAIL STRENGTH & SURVIVAL ANALYSIS ==========\nsetup_publication_style()\ncolors = ['#009988', '#BBBBBB']\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# (a) Trail strength distribution\nax = axes[0]\nax.hist(field_on_trails, bins=15, alpha=0.6, color=colors[0],\n        label='Field ON', edgecolor='black')\nax.hist(field_off_trails, bins=15, alpha=0.6, color=colors[1],\n        label='Field OFF', edgecolor='black')\nax.axvline(field_on_trails.mean(), color=colors[0], linestyle='--', linewidth=2)\nax.axvline(field_off_trails.mean(), color=colors[1], linestyle='--', linewidth=2)\nax.set_xlabel('Trail Strength (mean Ch0 value in non-zero cells)')\nax.set_ylabel('Count')\nax.set_title('(a) Trail Strength Distribution')\nax.legend()\n\n# (b) Survival rate distribution\nax = axes[1]\nax.hist(field_on_survival, bins=15, alpha=0.6, color=colors[0],\n        label='Field ON', edgecolor='black')\nax.hist(field_off_survival, bins=15, alpha=0.6, color=colors[1],\n        label='Field OFF', edgecolor='black')\nax.axvline(field_on_survival.mean(), color=colors[0], linestyle='--', linewidth=2)\nax.axvline(field_off_survival.mean(), color=colors[1], linestyle='--', linewidth=2)\nax.set_xlabel('Survival Rate (final pop / starting agents)')\nax.set_ylabel('Count')\nax.set_title('(b) Survival Rate Distribution')\nax.legend()\n\nplt.suptitle('Trail Strength & Survival: Field ON vs OFF', fontsize=14, y=1.02)\nplt.tight_layout()\nsave_figure(fig, os.path.join(OUTPUT_DIR, 'trail_survival_comparison'))\nplt.show()\n\n# Statistical tests\ntrail_welch = welch_t_test(field_on_trails, field_off_trails)\nsurv_welch = welch_t_test(field_on_survival, field_off_survival)\n\nprint(\"Trail Strength Comparison:\")\nprint(f\"  Field ON:  {field_on_trails.mean():.4f} +/- {field_on_trails.std():.4f}\")\nprint(f\"  Field OFF: {field_off_trails.mean():.4f} +/- {field_off_trails.std():.4f}\")\nprint(f\"  Welch t={trail_welch.statistic:.4f}, p={trail_welch.p_value:.6f}, \"\n      f\"d={trail_welch.effect_size:.4f}\")\n\nprint(f\"\\nSurvival Rate Comparison:\")\nprint(f\"  Field ON:  {field_on_survival.mean():.4f} +/- {field_on_survival.std():.4f}\")\nprint(f\"  Field OFF: {field_off_survival.mean():.4f} +/- {field_off_survival.std():.4f}\")\nprint(f\"  Welch t={surv_welch.statistic:.4f}, p={surv_welch.p_value:.6f}, \"\n      f\"d={surv_welch.effect_size:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Comparison Plots"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== COMPARISON PLOTS ==========\nsetup_publication_style()\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# 1. Bar chart: IQM with CI\nax = axes[0]\nmethods = ['Field ON\\n(Stigmergy)', 'Field OFF\\n(No Field)']\niqm_vals = [iqm_on.iqm, iqm_off.iqm]\niqm_lo = [iqm_on.iqm - iqm_on.ci_lower, iqm_off.iqm - iqm_off.ci_lower]\niqm_hi = [iqm_on.ci_upper - iqm_on.iqm, iqm_off.ci_upper - iqm_off.iqm]\ncolors = ['#009988', '#BBBBBB']\n\nbars = ax.bar(methods, iqm_vals, yerr=[iqm_lo, iqm_hi], color=colors,\n              edgecolor='black', linewidth=0.5, capsize=8)\nfor bar, val in zip(bars, iqm_vals):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(iqm_hi) + 0.05,\n            f'{val:.3f}', ha='center', fontsize=10)\nax.set_ylabel('IQM Reward')\nax.set_title('(a) IQM + 95% CI')\nsig = f\"p = {welch.p_value:.4f}\"\nif welch.p_value < 0.001: sig += \" ***\"\nelif welch.p_value < 0.01: sig += \" **\"\nelif welch.p_value < 0.05: sig += \" *\"\nax.text(0.5, max(iqm_vals) + max(iqm_hi) + 0.3, sig, ha='center', fontsize=11,\n        transform=ax.get_xaxis_transform())\n\n# 2. Violin + swarm\nax = axes[1]\nparts = ax.violinplot([field_on_rewards, field_off_rewards], positions=[1, 2],\n                       showmeans=True, showmedians=True, showextrema=False)\nfor i, body in enumerate(parts['bodies']):\n    body.set_facecolor(colors[i])\n    body.set_alpha(0.4)\n\nrng = np.random.default_rng(42)\nfor i, (data, pos) in enumerate(zip([field_on_rewards, field_off_rewards], [1, 2])):\n    jitter = rng.normal(0, 0.05, size=len(data))\n    ax.scatter(np.full_like(data, pos) + jitter, data, alpha=0.6, s=25,\n               color=colors[i], edgecolor='black', linewidth=0.3, zorder=3)\n\nax.scatter([1], [iqm_on.iqm], marker='D', s=80, color='red', zorder=5, label='IQM')\nax.scatter([2], [iqm_off.iqm], marker='D', s=80, color='red', zorder=5)\nax.set_xticks([1, 2])\nax.set_xticklabels(['Field ON', 'Field OFF'])\nax.set_ylabel('Mean Reward')\nax.set_title('(b) Distribution (all 60 seeds)')\nax.legend(fontsize=9)\n\n# 3. Population comparison (training-time)\nax = axes[2]\nax.hist(field_on_populations, bins=15, alpha=0.6, color=colors[0],\n        label='Field ON', edgecolor='black')\nif field_off_populations_train is not None:\n    ax.hist(field_off_populations_train, bins=15, alpha=0.6, color=colors[1],\n            label='Field OFF', edgecolor='black')\nax.set_xlabel('Final Population (training)')\nax.set_ylabel('Count')\nax.set_title('(c) Population Distribution')\nax.axvline(x=64, color='red', linestyle='--', alpha=0.5, label='Max capacity')\nax.legend(fontsize=9)\n\nplt.suptitle('Field ON vs Field OFF: 30-Seed Comparison', fontsize=14, y=1.02)\nplt.tight_layout()\nsave_figure(fig, os.path.join(OUTPUT_DIR, 'main_comparison'))\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== PERFORMANCE PROFILES ==========\nsetup_publication_style()\nfig = plot_performance_profiles(\n    {\"Field ON (Stigmergy)\": field_on_rewards, \"Field OFF (No Field)\": field_off_rewards},\n    output_path=os.path.join(OUTPUT_DIR, 'performance_profiles'),\n    tau_range=(0, 1.05),\n)\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== REWARD vs POPULATION ==========\nsetup_publication_style()\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Field ON\nax = axes[0]\nax.scatter(field_on_populations, field_on_rewards, color='#009988', label='Field ON',\n           s=50, alpha=0.7, edgecolor='black', linewidth=0.5)\nax.set_xlabel('Final Population')\nax.set_ylabel('Mean Reward')\nax.set_title('(a) Field ON: Reward vs Population')\n\n# Annotate failed seeds\nfailed_idx = np.where(field_on_rewards < FAILED_THRESHOLD)[0]\nfor idx in failed_idx:\n    ax.annotate(f'Seed {idx}\\n(died)', xy=(field_on_populations[idx], field_on_rewards[idx]),\n                fontsize=8, color='red', arrowprops=dict(arrowstyle='->', color='red'),\n                xytext=(field_on_populations[idx]+5, field_on_rewards[idx]+1))\n\n# Field OFF\nax = axes[1]\nif field_off_populations_train is not None:\n    ax.scatter(field_off_populations_train, field_off_rewards, color='#BBBBBB',\n               label='Field OFF', s=50, alpha=0.7, edgecolor='black', linewidth=0.5)\n    ax.set_xlabel('Final Population')\n    ax.set_ylabel('Mean Reward')\n    ax.set_title('(b) Field OFF: Reward vs Population')\nelse:\n    ax.text(0.5, 0.5, 'Field OFF populations\\navailable after eval (Phase 3)',\n            ha='center', va='center', transform=ax.transAxes, fontsize=12)\n    ax.set_title('(b) Field OFF: Reward vs Population')\n\nplt.tight_layout()\nsave_figure(fig, os.path.join(OUTPUT_DIR, 'reward_vs_population'))\nplt.show()\n\n# Correlation for Field ON (excluding failed seeds)\nfrom scipy import stats as scipy_stats\nmask = field_on_rewards >= FAILED_THRESHOLD\nr, p = scipy_stats.pearsonr(field_on_populations[mask], field_on_rewards[mask])\nprint(f\"Field ON correlation (pop vs reward, excl. failed): r={r:.3f}, p={p:.6f}\")\n\nif field_off_populations_train is not None:\n    r2, p2 = scipy_stats.pearsonr(field_off_populations_train, field_off_rewards)\n    print(f\"Field OFF correlation (pop vs reward): r={r2:.3f}, p={p2:.6f}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Phase 3: Checkpoint Analysis (BOTH Conditions)\n\nLoad ALL 60 checkpoints from Drive, run eval episodes and compute weight divergence for both Field ON and Field OFF.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ========== LOAD ALL CHECKPOINTS (BOTH CONDITIONS) ==========\nimport glob as glob_mod\nfrom src.training.checkpointing import load_checkpoint\nfrom src.analysis.ablation import _run_episode_full\n\ndef discover_checkpoints(drive_dir, condition_name):\n    \"\"\"Discover all checkpoint paths in a Drive directory.\"\"\"\n    paths = []\n    for batch_idx in range(10):\n        batch_dir = os.path.join(drive_dir, f'batch_{batch_idx}')\n        if not os.path.exists(batch_dir):\n            continue\n        for seed_dir in sorted(os.listdir(batch_dir)):\n            seed_path = os.path.join(batch_dir, seed_dir)\n            if not os.path.isdir(seed_path):\n                continue\n            pkl_files = glob_mod.glob(os.path.join(seed_path, 'step_*.pkl'))\n            if pkl_files:\n                paths.append(max(pkl_files, key=lambda p: int(re.search(r'step_(\\d+)', p).group(1))))\n    print(f\"  {condition_name}: Found {len(paths)} checkpoints on Drive\")\n    return paths\n\nfield_on_ckpt_paths = discover_checkpoints(FIELD_ON_DIR, \"Field ON\")\nfield_off_ckpt_paths = discover_checkpoints(FIELD_OFF_DIR, \"Field OFF\")\n\ndef load_seed_data(ckpt_path):\n    \"\"\"Load a checkpoint and extract seed data for eval + analysis.\"\"\"\n    ckpt = load_checkpoint(ckpt_path)\n    config = ckpt['config']\n    agent_params = jax.tree_util.tree_map(lambda x: x[0], ckpt['agent_params'])\n    network = ActorCritic(\n        hidden_dims=tuple(config.agent.hidden_dims),\n        num_actions=config.agent.num_actions,\n    )\n    return {\n        'params': ckpt['params'],\n        'agent_params': agent_params,\n        'config': config,\n        'network': network,\n        'seed_id': ckpt.get('seed_id', -1),\n    }\n\n# Verify checkpoint counts\nprint(f\"Field ON checkpoints:  {len(field_on_ckpt_paths)}\")\nprint(f\"Field OFF checkpoints: {len(field_off_ckpt_paths)}\")\nassert len(field_on_ckpt_paths) == 30, f\"Expected 30 Field ON checkpoints, got {len(field_on_ckpt_paths)}\"\nassert len(field_off_ckpt_paths) == 30, f\"Expected 30 Field OFF checkpoints, got {len(field_off_ckpt_paths)}\"\n\n# Test load one from each condition\nfor name, paths in [(\"Field ON\", field_on_ckpt_paths), (\"Field OFF\", field_off_ckpt_paths)]:\n    test_data = load_seed_data(paths[0])\n    cfg = test_data['config']\n    print(f\"\\n{name} config verification:\")\n    print(f\"  seed={test_data['seed_id']}, grid={cfg.env.grid_size}, max_agents={cfg.evolution.max_agents}\")\n    print(f\"  channel_diffusion={cfg.field.channel_diffusion_rates}, \"\n          f\"channel_decay={cfg.field.channel_decay_rates}, \"\n          f\"territory_write={cfg.field.territory_write_strength}\")\n\nprint(f\"\\nAll checkpoints verified. Ready for eval episodes.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ========== EVAL EPISODES + WEIGHT DIVERGENCE (BOTH CONDITIONS) ==========\n# Single pass: load each checkpoint once, run eval AND compute divergence\n# This avoids loading 60 checkpoints twice from Drive.\nfrom src.analysis.specialization import compute_weight_divergence\n\nNUM_EVAL_EPISODES = 5\neval_results_on = []\neval_results_off = []\ndivergence_on = []\ndivergence_off = []\n\nfor cond_name, ckpt_paths, eval_list, div_list in [\n    (\"Field ON\", field_on_ckpt_paths, eval_results_on, divergence_on),\n    (\"Field OFF\", field_off_ckpt_paths, eval_results_off, divergence_off),\n]:\n    print(f\"\\n{'='*60}\")\n    print(f\"ANALYZING {cond_name} ({len(ckpt_paths)} seeds)\")\n    print(f\"{'='*60}\")\n\n    for i, ckpt_path in enumerate(ckpt_paths):\n        seed_data = load_seed_data(ckpt_path)\n        config = seed_data['config']\n\n        # --- Eval episodes ---\n        seed_pops = []\n        seed_rewards = []\n        seed_births = []\n        seed_deaths = []\n\n        for ep in range(NUM_EVAL_EPISODES):\n            key = jax.random.PRNGKey(ep * 1000 + i)\n            stats = _run_episode_full(\n                network=seed_data['network'],\n                params=seed_data['params'],\n                config=config,\n                key=key,\n                condition=\"normal\",\n                evolution=True,\n            )\n            seed_pops.append(stats.final_population)\n            seed_rewards.append(stats.total_reward)\n            seed_births.append(stats.total_births)\n            seed_deaths.append(stats.total_deaths)\n\n        eval_list.append({\n            'seed_id': seed_data['seed_id'],\n            'ckpt_path': ckpt_path,\n            'mean_total_reward': np.mean(seed_rewards),\n            'std_total_reward': np.std(seed_rewards),\n            'mean_population': np.mean(seed_pops),\n            'mean_births': np.mean(seed_births),\n            'mean_deaths': np.mean(seed_deaths),\n            'survival_rate': np.mean(seed_pops) / config.env.num_agents,\n            'all_rewards': seed_rewards,\n            'all_populations': seed_pops,\n        })\n\n        # --- Weight divergence ---\n        div = compute_weight_divergence(seed_data['agent_params'])\n        div_list.append({\n            'seed_id': seed_data['seed_id'],\n            'mean_divergence': float(div['mean_divergence']),\n            'max_divergence': float(div['max_divergence']),\n            'n_agents': len(div['agent_indices']),\n        })\n\n        # Free checkpoint memory\n        del seed_data\n        gc.collect()\n\n        if (i + 1) % 5 == 0 or i == 0:\n            print(f\"  [{i+1}/{len(ckpt_paths)}] seed {eval_list[-1]['seed_id']}: \"\n                  f\"reward={np.mean(seed_rewards):.1f}, pop={np.mean(seed_pops):.1f}, \"\n                  f\"div={div['mean_divergence']:.4f}\")\n\n# Extract arrays\nfield_on_eval_populations = np.array([r['mean_population'] for r in eval_results_on])\nfield_off_eval_populations = np.array([r['mean_population'] for r in eval_results_off])\nfield_on_eval_rewards = np.array([r['mean_total_reward'] for r in eval_results_on])\nfield_off_eval_rewards = np.array([r['mean_total_reward'] for r in eval_results_off])\n\non_mean_divs = np.array([r['mean_divergence'] for r in divergence_on])\noff_mean_divs = np.array([r['mean_divergence'] for r in divergence_off])\non_max_divs = np.array([r['max_divergence'] for r in divergence_on])\noff_max_divs = np.array([r['max_divergence'] for r in divergence_off])\n\n# Summary\nfor cond_name, evals, pops, rewards, mean_d, max_d in [\n    (\"Field ON\", eval_results_on, field_on_eval_populations, field_on_eval_rewards, on_mean_divs, on_max_divs),\n    (\"Field OFF\", eval_results_off, field_off_eval_populations, field_off_eval_rewards, off_mean_divs, off_max_divs),\n]:\n    print(f\"\\n{'='*60}\")\n    print(f\"{cond_name} SUMMARY ({len(evals)} seeds x {NUM_EVAL_EPISODES} episodes)\")\n    print(f\"  Eval reward:       {rewards.mean():.1f} +/- {rewards.std():.1f}\")\n    print(f\"  Eval population:   {pops.mean():.1f} +/- {pops.std():.1f}\")\n    print(f\"  At max capacity:   {np.sum(pops >= 60)}/{len(pops)}\")\n    print(f\"  Mean births:       {np.mean([r['mean_births'] for r in evals]):.1f}\")\n    print(f\"  Mean deaths:       {np.mean([r['mean_deaths'] for r in evals]):.1f}\")\n    print(f\"  Mean divergence:   {mean_d.mean():.4f} +/- {mean_d.std():.4f}\")\n    print(f\"  Max divergence:    {max_d.mean():.4f} +/- {max_d.std():.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ========== PHASE 3 PLOTS + STATISTICAL COMPARISON ==========\nsetup_publication_style()\ncolors = ['#009988', '#BBBBBB']\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# (a) Weight divergence comparison\nax = axes[0, 0]\nax.hist(on_mean_divs, bins=12, alpha=0.6, color=colors[0], label='Field ON', edgecolor='black')\nax.hist(off_mean_divs, bins=12, alpha=0.6, color=colors[1], label='Field OFF', edgecolor='black')\nax.axvline(on_mean_divs.mean(), color=colors[0], linestyle='--', linewidth=2)\nax.axvline(off_mean_divs.mean(), color=colors[1], linestyle='--', linewidth=2)\nax.set_xlabel('Mean Pairwise Weight Divergence (cosine)')\nax.set_ylabel('Count')\nax.set_title('(a) Weight Divergence Distribution')\nax.legend()\n\n# (b) Eval population comparison\nax = axes[0, 1]\nax.hist(field_on_eval_populations, bins=15, alpha=0.6, color=colors[0],\n        label='Field ON', edgecolor='black')\nax.hist(field_off_eval_populations, bins=15, alpha=0.6, color=colors[1],\n        label='Field OFF', edgecolor='black')\nax.axvline(x=64, color='red', linestyle='--', alpha=0.5, label='Max capacity')\nax.set_xlabel('Eval Population')\nax.set_ylabel('Count')\nax.set_title('(b) Eval Population Distribution')\nax.legend()\n\n# (c) Eval reward vs population (both conditions)\nax = axes[1, 0]\nax.scatter(field_on_eval_populations, field_on_eval_rewards, color=colors[0],\n           s=50, alpha=0.7, edgecolor='black', linewidth=0.5, label='Field ON')\nax.scatter(field_off_eval_populations, field_off_eval_rewards, color=colors[1],\n           s=50, alpha=0.7, edgecolor='black', linewidth=0.5, label='Field OFF')\nax.set_xlabel('Eval Population')\nax.set_ylabel('Total Eval Reward')\nax.set_title('(c) Eval: Reward vs Population')\nax.legend()\n\n# (d) Divergence vs training reward\nax = axes[1, 1]\nax.scatter(on_mean_divs, field_on_rewards, color=colors[0],\n           s=50, alpha=0.7, edgecolor='black', linewidth=0.5, label='Field ON')\nax.scatter(off_mean_divs, field_off_rewards, color=colors[1],\n           s=50, alpha=0.7, edgecolor='black', linewidth=0.5, label='Field OFF')\nax.set_xlabel('Mean Weight Divergence')\nax.set_ylabel('Training Reward')\nax.set_title('(d) Divergence vs Training Reward')\nax.legend()\n\nplt.suptitle('Phase 3: Checkpoint Analysis (Both Conditions)', fontsize=14, y=1.01)\nplt.tight_layout()\nsave_figure(fig, os.path.join(OUTPUT_DIR, 'checkpoint_analysis'))\nplt.show()\n\n# Divergence statistical comparison\ndiv_welch = welch_t_test(on_mean_divs, off_mean_divs)\ndiv_mw = mann_whitney_test(on_mean_divs, off_mean_divs)\nprint(f\"\\nDivergence comparison:\")\nprint(f\"  Welch t-test: t={div_welch.statistic:.4f}, p={div_welch.p_value:.6f}, d={div_welch.effect_size:.4f}\")\nprint(f\"  Mann-Whitney: U={div_mw.statistic:.1f}, p={div_mw.p_value:.6f}, r={div_mw.effect_size:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Phase 4: Report & Save\n\nGenerate formatted comparison report and save all results to Drive.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# ========== COMPARISON REPORT ==========\n",
    "from datetime import datetime\n",
    "\n",
    "# Determine significance level\n",
    "if welch.p_value < 0.001:\n",
    "    sig_str = \"p < 0.001 (***)\"\n",
    "elif welch.p_value < 0.01:\n",
    "    sig_str = f\"p = {welch.p_value:.4f} (**)\"\n",
    "elif welch.p_value < 0.05:\n",
    "    sig_str = f\"p = {welch.p_value:.4f} (*)\"\n",
    "else:\n",
    "    sig_str = f\"p = {welch.p_value:.4f} (not significant)\"\n",
    "\n",
    "d_val = abs(welch.effect_size)\n",
    "if d_val < 0.2: d_str = \"negligible\"\n",
    "elif d_val < 0.5: d_str = \"small\"\n",
    "elif d_val < 0.8: d_str = \"medium\"\n",
    "else: d_str = \"large\"\n",
    "\n",
    "winner = \"Field OFF\" if field_off_rewards.mean() > field_on_rewards.mean() else \"Field ON\"\n",
    "\n",
    "# Sample std for CoV\n",
    "cov_on = field_on_rewards.std(ddof=1) / field_on_rewards.mean() if field_on_rewards.mean() != 0 else float('inf')\n",
    "cov_off = field_off_rewards.std(ddof=1) / field_off_rewards.mean() if field_off_rewards.mean() != 0 else float('inf')\n",
    "\n",
    "n_failed = int(np.sum(field_on_rewards < FAILED_THRESHOLD))\n",
    "on_nonfailed = field_on_rewards[field_on_rewards >= FAILED_THRESHOLD]\n",
    "\n",
    "# Config source description\n",
    "config_source = f\"sweep-optimized ({os.path.basename(BEST_CONFIG_PATH)})\" if os.path.exists(BEST_CONFIG_PATH) else \"v1 sweep fallback values\"\n",
    "\n",
    "report = f\"\"\"# Field ON vs Field OFF: 30-Seed Comparison Report (v2 \u2014 Fresh Training)\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Experiment Setup\n",
    "- **Conditions**: Field ON (biological pheromone system) vs Field OFF (no shared field)\n",
    "- **Seeds per condition**: 30 (10 batches x 3 seeds)\n",
    "- **Training steps**: 10M per seed\n",
    "- **Eval episodes**: {NUM_EVAL_EPISODES} per seed (post-training)\n",
    "- **Config**: grid=40, num_agents=16 (start), max_agents=64, num_food=25\n",
    "- **Energy**: starting=200, food=100, reproduce_threshold=180, cost=80\n",
    "- **Config source**: {config_source}\n",
    "- **Codebase**: post-energy-fix (crop refuel + free write steps)\n",
    "- **Key design**: Field OFF inherits ALL params from Field ON (controlled experiment)\n",
    "  - Same nest radius, compass noise, evolution params\n",
    "  - ONLY difference: field channels zeroed out\n",
    "\n",
    "## Key Results\n",
    "\n",
    "### Eval Reward Comparison (from {NUM_EVAL_EPISODES} eval episodes per seed)\n",
    "| Metric | Field ON | Field OFF |\n",
    "|--------|----------|-----------|\n",
    "| Mean | {field_on_rewards.mean():.4f} +/- {field_on_rewards.std(ddof=1):.4f} | {field_off_rewards.mean():.4f} +/- {field_off_rewards.std(ddof=1):.4f} |\n",
    "| Median | {np.median(field_on_rewards):.4f} | {np.median(field_off_rewards):.4f} |\n",
    "| IQM | {iqm_on.iqm:.4f} [{iqm_on.ci_lower:.4f}, {iqm_on.ci_upper:.4f}] | {iqm_off.iqm:.4f} [{iqm_off.ci_lower:.4f}, {iqm_off.ci_upper:.4f}] |\n",
    "| Min | {field_on_rewards.min():.4f} | {field_off_rewards.min():.4f} |\n",
    "| Max | {field_on_rewards.max():.4f} | {field_off_rewards.max():.4f} |\n",
    "| CoV | {cov_on:.4f} | {cov_off:.4f} |\n",
    "\n",
    "### Trail Strength & Survival\n",
    "| Metric | Field ON | Field OFF |\n",
    "|--------|----------|-----------|\n",
    "| Trail strength | {field_on_trails.mean():.4f} +/- {field_on_trails.std():.4f} | {field_off_trails.mean():.4f} +/- {field_off_trails.std():.4f} |\n",
    "| Survival rate | {field_on_survival.mean():.4f} +/- {field_on_survival.std():.4f} | {field_off_survival.mean():.4f} +/- {field_off_survival.std():.4f} |\n",
    "| Trail Welch p | {trail_welch.p_value:.6f} | d = {trail_welch.effect_size:.4f} |\n",
    "| Survival Welch p | {surv_welch.p_value:.6f} | d = {surv_welch.effect_size:.4f} |\n",
    "\n",
    "### Statistical Tests (Reward)\n",
    "- **Welch's t-test**: {sig_str}, Cohen's d = {welch.effect_size:.4f} ({d_str})\n",
    "- **Mann-Whitney U**: U = {mw.statistic:.1f}, p = {mw.p_value:.6f}, rank-biserial r = {mw.effect_size:.4f}\n",
    "- **P(Field OFF > Field ON)**: {poi['prob_x_better']:.4f}\n",
    "\n",
    "### Winner: **{winner}** (by mean eval reward)\n",
    "\n",
    "### Sensitivity Analysis (threshold = {FAILED_THRESHOLD})\n",
    "- Excluding {n_failed} failed Field ON seeds:\n",
    "  Field ON filtered mean = {on_nonfailed.mean():.4f} (n={len(on_nonfailed)})\n",
    "  Welch p = {welch_f.p_value:.6f}, Cohen's d = {welch_f.effect_size:.4f}\n",
    "\n",
    "### Population Comparison\n",
    "| Metric | Field ON | Field OFF |\n",
    "|--------|----------|-----------|\n",
    "| Mean population | {field_on_populations.mean():.1f} +/- {field_on_populations.std():.1f} | {field_off_populations.mean():.1f} +/- {field_off_populations.std():.1f} |\n",
    "| At max (64) | {np.sum(field_on_populations >= 60)}/30 | {np.sum(field_off_populations >= 60)}/30 |\n",
    "| Failed seeds | {n_failed} | {int(np.sum(field_off_rewards < FAILED_THRESHOLD))} |\n",
    "\n",
    "### Checkpoint Analysis ({NUM_EVAL_EPISODES} eval episodes/seed)\n",
    "| Metric | Field ON | Field OFF |\n",
    "|--------|----------|-----------|\n",
    "| Eval total reward | {field_on_eval_rewards.mean():.1f} +/- {field_on_eval_rewards.std():.1f} | {field_off_eval_rewards.mean():.1f} +/- {field_off_eval_rewards.std():.1f} |\n",
    "| Eval population | {field_on_eval_populations.mean():.1f} +/- {field_on_eval_populations.std():.1f} | {field_off_eval_populations.mean():.1f} +/- {field_off_eval_populations.std():.1f} |\n",
    "\n",
    "### Weight Divergence\n",
    "| Metric | Field ON | Field OFF |\n",
    "|--------|----------|-----------|\n",
    "| Mean divergence | {on_mean_divs.mean():.4f} +/- {on_mean_divs.std():.4f} | {off_mean_divs.mean():.4f} +/- {off_mean_divs.std():.4f} |\n",
    "| Max divergence | {on_max_divs.mean():.4f} +/- {on_max_divs.std():.4f} | {off_max_divs.mean():.4f} +/- {off_max_divs.std():.4f} |\n",
    "| Divergence Welch p | {div_welch.p_value:.6f} | Cohen's d = {div_welch.effect_size:.4f} |\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "**Direction**: Field {'OFF' if field_off_rewards.mean() > field_on_rewards.mean() else 'ON'} achieves {'higher' if field_off_rewards.mean() > field_on_rewards.mean() else 'lower'} mean reward.\n",
    "\n",
    "Key observations:\n",
    "1. Trail strength: Field ON = {field_on_trails.mean():.4f}, Field OFF = {field_off_trails.mean():.4f} (p={trail_welch.p_value:.4f})\n",
    "2. Survival rate: Field ON = {field_on_survival.mean():.3f}, Field OFF = {field_off_survival.mean():.3f}\n",
    "3. Variance: Field ON CoV = {cov_on:.3f}, Field OFF CoV = {cov_off:.3f}\n",
    "4. Failed seeds: {n_failed} Field ON seeds with reward < {FAILED_THRESHOLD}\n",
    "5. Weight divergence: {'Field ON higher' if on_mean_divs.mean() > off_mean_divs.mean() else 'Field OFF higher'} (p={div_welch.p_value:.4f})\n",
    "\n",
    "## Next Steps\n",
    "- If Field ON > OFF: biological pheromone system enables collective intelligence\n",
    "- If Field OFF > ON: pheromone overhead may need longer training or better hyperparameters\n",
    "- Investigate trail formation patterns in successful Field ON seeds\n",
    "- Test with diversity_bonus and niche_pressure enabled\n",
    "\"\"\"\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(report))\n",
    "print(\"\\nReport generated successfully.\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ========== SAVE RESULTS ==========\nresults = {\n    'metadata': {\n        'generated': datetime.now().isoformat(),\n        'field_on_seeds': 30,\n        'field_off_seeds': 30,\n        'steps_per_seed': 10_000_000,\n        'eval_episodes_per_seed': NUM_EVAL_EPISODES,\n        'data_source': 'Fresh training + eval episodes (v2)',\n        'config_source': config_source,\n    },\n    'field_on': {\n        'eval_rewards': field_on_rewards.tolist(),\n        'eval_populations': field_on_populations.tolist(),\n        'eval_trail_strengths': field_on_trails.tolist(),\n        'eval_survival_rates': field_on_survival.tolist(),\n        'mean_reward': float(field_on_rewards.mean()),\n        'std_reward': float(field_on_rewards.std(ddof=1)),\n        'iqm': float(iqm_on.iqm),\n        'iqm_ci': [float(iqm_on.ci_lower), float(iqm_on.ci_upper)],\n        'checkpoint_eval': {\n            'populations': field_on_eval_populations.tolist(),\n            'total_rewards': field_on_eval_rewards.tolist(),\n            'per_seed': [{k: v for k, v in r.items() if k != 'ckpt_path'}\n                         for r in eval_results_on],\n        },\n        'weight_divergence': {\n            'mean_divergences': on_mean_divs.tolist(),\n            'max_divergences': on_max_divs.tolist(),\n            'per_seed': divergence_on,\n        },\n    },\n    'field_off': {\n        'eval_rewards': field_off_rewards.tolist(),\n        'eval_populations': field_off_populations.tolist(),\n        'eval_trail_strengths': field_off_trails.tolist(),\n        'eval_survival_rates': field_off_survival.tolist(),\n        'mean_reward': float(field_off_rewards.mean()),\n        'std_reward': float(field_off_rewards.std(ddof=1)),\n        'iqm': float(iqm_off.iqm),\n        'iqm_ci': [float(iqm_off.ci_lower), float(iqm_off.ci_upper)],\n        'checkpoint_eval': {\n            'populations': field_off_eval_populations.tolist(),\n            'total_rewards': field_off_eval_rewards.tolist(),\n            'per_seed': [{k: v for k, v in r.items() if k != 'ckpt_path'}\n                         for r in eval_results_off],\n        },\n        'weight_divergence': {\n            'mean_divergences': off_mean_divs.tolist(),\n            'max_divergences': off_max_divs.tolist(),\n            'per_seed': divergence_off,\n        },\n    },\n    'tests': {\n        'welch_t': float(welch.statistic),\n        'welch_p': float(welch.p_value),\n        'cohens_d': float(welch.effect_size),\n        'mann_whitney_u': float(mw.statistic),\n        'mann_whitney_p': float(mw.p_value),\n        'rank_biserial_r': float(mw.effect_size),\n        'prob_off_better': float(poi['prob_x_better']),\n        'divergence_welch_p': float(div_welch.p_value),\n        'divergence_cohens_d': float(div_welch.effect_size),\n        'trail_welch_p': float(trail_welch.p_value),\n        'trail_cohens_d': float(trail_welch.effect_size),\n        'survival_welch_p': float(surv_welch.p_value),\n        'survival_cohens_d': float(surv_welch.effect_size),\n    },\n}\n\n# Save JSON\njson_path = os.path.join(OUTPUT_DIR, 'field_on_vs_off_results.json')\nwith open(json_path, 'w') as f:\n    json.dump(results, f, indent=2)\nprint(f\"JSON saved: {json_path}\")\n\n# Save pickle (preserves numpy arrays)\npkl_path = os.path.join(OUTPUT_DIR, 'field_on_vs_off_results.pkl')\nwith open(pkl_path, 'wb') as f:\n    pickle.dump(results, f)\nprint(f\"Pickle saved: {pkl_path}\")\n\n# Save report markdown\nmd_path = os.path.join(OUTPUT_DIR, 'comparison_report.md')\nwith open(md_path, 'w') as f:\n    f.write(report)\nprint(f\"Report saved: {md_path}\")\n\n# List all output files\nprint(f\"\\n{'='*60}\")\nprint(\"ALL OUTPUT FILES:\")\nfor fname in sorted(os.listdir(OUTPUT_DIR)):\n    fpath = os.path.join(OUTPUT_DIR, fname)\n    size_mb = os.path.getsize(fpath) / 1024 / 1024\n    print(f\"  {fname} ({size_mb:.2f} MB)\")\nprint(f\"\\nDone! All results saved to {OUTPUT_DIR}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}