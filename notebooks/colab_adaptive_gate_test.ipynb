{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Gate 3-Way Comparison\n",
    "\n",
    "**Purpose**: Compare three field conditions to determine if learned field gating improves performance:\n",
    "\n",
    "- **field_on**: Normal field (baseline) - agents always read full field\n",
    "- **field_off**: Instant decay = no field information\n",
    "- **adaptive**: Learned gate modulates field usage per channel\n",
    "\n",
    "**Key question**: Does letting agents *learn* when to use the field beat always-on or always-off?\n",
    "\n",
    "**Metrics tracked**:\n",
    "- Reward comparison (mean, 95% CI, statistical tests)\n",
    "- Gate evolution over training (do gates tend toward 0 or 1?)\n",
    "- Gate bimodality (do agents specialize into field-users vs field-ignorers?)\n",
    "\n",
    "## Setup\n",
    "1. Runtime > Change runtime type > **TPU v6e** + **High-RAM**\n",
    "2. Run all cells (Ctrl+F9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup - Mount Drive, clone repo, install\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "REPO_DIR = '/content/emergence-lab'\n",
    "GITHUB_USERNAME = \"imashishkh21\"  # Verified from git remote\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone https://github.com/{GITHUB_USERNAME}/emergence-lab.git {REPO_DIR}\n",
    "else:\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "!pip install -e \".[dev]\" -q\n",
    "!pip install scipy -q  # For statistical tests\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports + TPU/GPU Verification\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import traceback\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "\n",
    "from src.configs import Config, TrainingMode\n",
    "from src.training.parallel_train import ParallelTrainer\n",
    "from src.agents.network import ActorCritic\n",
    "from src.agents.policy import sample_actions\n",
    "from src.environment.env import reset, step\n",
    "from src.environment.obs import get_observations\n",
    "\n",
    "# TPU/GPU verification\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Devices: {jax.devices()}\")\n",
    "device_str = str(jax.devices()[0])\n",
    "assert 'TPU' in device_str or 'GPU' in device_str, f\"No accelerator! Found: {device_str}\"\n",
    "print(\"Accelerator check passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Constants & Paths\n",
    "DRIVE_BASE = '/content/drive/MyDrive/emergence-lab/adaptive_gate_test'\n",
    "os.makedirs(DRIVE_BASE, exist_ok=True)\n",
    "\n",
    "TOTAL_STEPS = 2_000_000  # Quick test (full = 10M)\n",
    "NUM_ENVS = 32\n",
    "NUM_STEPS = 128\n",
    "MAX_AGENTS = 64\n",
    "SEEDS = [42, 123, 456]\n",
    "CONDITIONS = ['field_on', 'field_off', 'adaptive']\n",
    "NUM_EVAL_EPISODES = 5\n",
    "\n",
    "STEPS_PER_ITER = NUM_ENVS * NUM_STEPS * MAX_AGENTS\n",
    "NUM_ITERATIONS = math.ceil(TOTAL_STEPS / STEPS_PER_ITER)\n",
    "\n",
    "print(f\"Drive base: {DRIVE_BASE}\")\n",
    "print(f\"Total steps: {TOTAL_STEPS:,}\")\n",
    "print(f\"Steps per iteration: {STEPS_PER_ITER:,}\")\n",
    "print(f\"Num iterations: {NUM_ITERATIONS}\")\n",
    "print(f\"Seeds: {SEEDS}\")\n",
    "print(f\"Conditions: {CONDITIONS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Config Builders\n",
    "# Base config with v2 sweep values - shared by all conditions\n",
    "\n",
    "def build_base_config() -> Config:\n",
    "    \"\"\"Build base config with v2 sweep values.\"\"\"\n",
    "    cfg = Config()\n",
    "    # Environment\n",
    "    cfg.env.grid_size = 40\n",
    "    cfg.env.num_agents = 16\n",
    "    cfg.env.num_food = 25\n",
    "    cfg.env.max_steps = 500\n",
    "    # Evolution (survival-friendly)\n",
    "    cfg.evolution.enabled = True\n",
    "    cfg.evolution.food_energy = 100\n",
    "    cfg.evolution.starting_energy = 200\n",
    "    cfg.evolution.max_energy = 300\n",
    "    cfg.evolution.reproduce_threshold = 180\n",
    "    cfg.evolution.reproduce_cost = 80\n",
    "    cfg.evolution.energy_per_step = 1\n",
    "    cfg.evolution.max_agents = MAX_AGENTS\n",
    "    # Field (v2 sweep values)\n",
    "    cfg.field.num_channels = 4\n",
    "    cfg.field.channel_diffusion_rates = (0.7, 0.01, 0.0, 0.0)\n",
    "    cfg.field.channel_decay_rates = (0.02, 0.0001, 0.0, 0.0)\n",
    "    cfg.field.territory_write_strength = 0.02\n",
    "    cfg.field.field_value_cap = 1.0\n",
    "    cfg.field.adaptive_gate = False  # Default: no gating\n",
    "    # Nest\n",
    "    cfg.nest.radius = 4\n",
    "    cfg.nest.compass_noise_rate = 0.2\n",
    "    # Training\n",
    "    cfg.train.training_mode = TrainingMode.GRADIENT\n",
    "    cfg.train.num_envs = NUM_ENVS\n",
    "    cfg.train.num_steps = NUM_STEPS\n",
    "    cfg.train.total_steps = TOTAL_STEPS\n",
    "    cfg.log.wandb = False\n",
    "    cfg.log.save_interval = 0\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def build_field_off_config() -> Config:\n",
    "    \"\"\"Build Field OFF config: instant decay = no field information.\"\"\"\n",
    "    cfg = build_base_config()\n",
    "    cfg.field.channel_diffusion_rates = (0.0, 0.0, 0.0, 0.0)\n",
    "    cfg.field.channel_decay_rates = (1.0, 1.0, 1.0, 1.0)\n",
    "    cfg.field.territory_write_strength = 0.0\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def build_adaptive_config() -> Config:\n",
    "    \"\"\"Build adaptive gate config: agents learn when to use field.\"\"\"\n",
    "    cfg = build_base_config()\n",
    "    cfg.field.adaptive_gate = True\n",
    "    return cfg\n",
    "\n",
    "\n",
    "# Print config summary\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIG SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for name, builder in [(\"field_on\", build_base_config), \n",
    "                       (\"field_off\", build_field_off_config), \n",
    "                       (\"adaptive\", build_adaptive_config)]:\n",
    "    cfg = builder()\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  adaptive_gate: {cfg.field.adaptive_gate}\")\n",
    "    print(f\"  channel_decay_rates: {cfg.field.channel_decay_rates}\")\n",
    "    print(f\"  territory_write_strength: {cfg.field.territory_write_strength}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Evaluation Function\n",
    "\n",
    "def run_eval(network, params, config, key, use_gate_bias=False, num_steps=500):\n",
    "    \"\"\"Run a single eval episode using lax.scan.\n",
    "\n",
    "    Args:\n",
    "        network: ActorCritic network.\n",
    "        params: Network parameters.\n",
    "        config: Environment config.\n",
    "        key: PRNG key.\n",
    "        use_gate_bias: If True and config.field.adaptive_gate is True,\n",
    "                       pass agent_gate_bias to sample_actions.\n",
    "        num_steps: Number of eval steps.\n",
    "\n",
    "    Returns:\n",
    "        Dict with total_reward, final_population, trail_strength, and\n",
    "        gate_bias_values (if adaptive and use_gate_bias=True).\n",
    "    \"\"\"\n",
    "    key, reset_key = jax.random.split(key)\n",
    "    init_state = reset(reset_key, config)\n",
    "\n",
    "    def _eval_step(carry, _unused):\n",
    "        state, rng, total_reward = carry\n",
    "        obs = get_observations(state, config)\n",
    "        obs_batched = obs[None, :, :]  # (1, max_agents, obs_dim)\n",
    "        rng, act_key = jax.random.split(rng)\n",
    "\n",
    "        # Pass gate_bias for adaptive config\n",
    "        gb = None\n",
    "        if use_gate_bias and config.field.adaptive_gate and state.agent_gate_bias is not None:\n",
    "            gb = state.agent_gate_bias[None, :, :]  # (1, max_agents, num_channels)\n",
    "\n",
    "        actions, _, _, _, _ = sample_actions(network, params, obs_batched, act_key, gb)\n",
    "        actions = actions[0]\n",
    "        state, rewards, done, info = step(state, actions, config)\n",
    "        alive = state.agent_alive.astype(jnp.float32)\n",
    "        total_reward = total_reward + jnp.sum(rewards * alive)\n",
    "        return (state, rng, total_reward), None\n",
    "\n",
    "    (final_state, _, total_reward), _ = jax.lax.scan(\n",
    "        _eval_step, (init_state, key, jnp.float32(0.0)), None, length=num_steps,\n",
    "    )\n",
    "\n",
    "    # Trail strength from Ch0\n",
    "    ch0 = jnp.asarray(final_state.field_state.values[:, :, 0])\n",
    "    nonzero_mask = ch0 > 0.01\n",
    "    trail_strength = jnp.where(\n",
    "        jnp.any(nonzero_mask),\n",
    "        jnp.sum(jnp.where(nonzero_mask, ch0, 0.0)) / jnp.maximum(jnp.sum(nonzero_mask.astype(jnp.float32)), 1.0),\n",
    "        0.0,\n",
    "    )\n",
    "\n",
    "    result = {\n",
    "        'total_reward': float(total_reward),\n",
    "        'final_population': int(jnp.sum(final_state.agent_alive)),\n",
    "        'trail_strength': float(trail_strength),\n",
    "    }\n",
    "\n",
    "    # For adaptive: capture final EVOLVED gate biases (genotype)\n",
    "    # Note: Actual gate output = sigmoid(learned_weights * hidden + bias)\n",
    "    # This captures the evolved preference, not momentary gate activation\n",
    "    if config.field.adaptive_gate and final_state.agent_gate_bias is not None:\n",
    "        alive_mask = final_state.agent_alive\n",
    "        gate_vals = final_state.agent_gate_bias[alive_mask]  # (n_alive, num_channels)\n",
    "        result['gate_bias_values'] = np.array(gate_vals)  # Renamed for clarity\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"Evaluation function ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Training Loop\n",
    "# Main training loop with resume safety\n",
    "\n",
    "RESULTS_PATH = os.path.join(DRIVE_BASE, 'all_results.pkl')\n",
    "if os.path.exists(RESULTS_PATH):\n",
    "    with open(RESULTS_PATH, 'rb') as f:\n",
    "        all_results = pickle.load(f)\n",
    "else:\n",
    "    all_results = []\n",
    "\n",
    "completed = {(r['condition'], r['seed_id']) for r in all_results if r.get('success')}\n",
    "\n",
    "CONFIG_BUILDERS = {\n",
    "    'field_on': build_base_config,\n",
    "    'field_off': build_field_off_config,\n",
    "    'adaptive': build_adaptive_config,\n",
    "}\n",
    "\n",
    "for condition in CONDITIONS:\n",
    "    for seed_id in SEEDS:\n",
    "        if (condition, seed_id) in completed:\n",
    "            print(f\"[{condition}|seed={seed_id}] SKIPPED (already completed)\")\n",
    "            continue\n",
    "\n",
    "        config = CONFIG_BUILDERS[condition]()\n",
    "        checkpoint_dir = os.path.join(DRIVE_BASE, f'{condition}_seed{seed_id}')\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training: {condition} | Seed: {seed_id}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        try:\n",
    "            trainer = ParallelTrainer(\n",
    "                config=config, num_seeds=1, seed_ids=[seed_id],\n",
    "                checkpoint_dir=checkpoint_dir, master_seed=seed_id,\n",
    "            )\n",
    "            train_metrics = trainer.train(\n",
    "                num_iterations=NUM_ITERATIONS,\n",
    "                checkpoint_interval_minutes=30,\n",
    "                resume=True, print_interval=5,\n",
    "            )\n",
    "\n",
    "            # Create network for eval\n",
    "            network = ActorCritic(\n",
    "                hidden_dims=tuple(config.agent.hidden_dims),\n",
    "                num_actions=config.agent.num_actions,\n",
    "                adaptive_gate=config.field.adaptive_gate,\n",
    "                num_field_channels=config.field.num_channels,\n",
    "            )\n",
    "            # Extract params for this seed (parallel trainer uses stacked params)\n",
    "            seed_params = jax.tree.map(lambda x: x[0], trainer._parallel_state.params)\n",
    "\n",
    "            # Run eval episodes\n",
    "            eval_results = []\n",
    "            all_gate_bias_values = []\n",
    "            for ep in range(NUM_EVAL_EPISODES):\n",
    "                key = jax.random.PRNGKey(seed_id * 1000 + ep)\n",
    "                res = run_eval(network, seed_params, config, key, use_gate_bias=True)\n",
    "                eval_results.append(res)\n",
    "                if 'gate_bias_values' in res:\n",
    "                    all_gate_bias_values.append(res['gate_bias_values'])\n",
    "\n",
    "            result = {\n",
    "                'condition': condition,\n",
    "                'seed_id': seed_id,\n",
    "                'success': True,\n",
    "                'total_reward': float(np.mean([r['total_reward'] for r in eval_results])),\n",
    "                'final_population': float(np.mean([r['final_population'] for r in eval_results])),\n",
    "                'trail_strength': float(np.mean([r['trail_strength'] for r in eval_results])),\n",
    "                'train_metrics': train_metrics,  # For learning curves\n",
    "            }\n",
    "\n",
    "            if all_gate_bias_values:\n",
    "                result['gate_bias_values'] = np.concatenate(all_gate_bias_values, axis=0)\n",
    "\n",
    "            all_results.append(result)\n",
    "            print(f\"  Eval: reward={result['total_reward']:.1f}, pop={result['final_population']:.1f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            all_results.append({\n",
    "                'condition': condition, 'seed_id': seed_id,\n",
    "                'error': str(e), 'success': False,\n",
    "            })\n",
    "        finally:\n",
    "            try:\n",
    "                del trainer\n",
    "            except NameError:\n",
    "                pass\n",
    "            gc.collect()\n",
    "            jax.clear_caches()\n",
    "\n",
    "        # Atomic save\n",
    "        tmp = RESULTS_PATH + '.tmp'\n",
    "        with open(tmp, 'wb') as f:\n",
    "            pickle.dump(all_results, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        os.replace(tmp, RESULTS_PATH)\n",
    "\n",
    "print(f\"\\nCompleted: {len([r for r in all_results if r.get('success')])} / {len(CONDITIONS) * len(SEEDS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Group Results by Condition + Summary\n\n# Note: ParallelTrainer.train() only returns FINAL metrics, not the full\n# training history. Learning curves would require modifying ParallelTrainer\n# to return all_metrics instead of final_metrics, or logging to a separate file.\n\ncolors = {'field_on': '#2ecc71', 'field_off': '#e74c3c', 'adaptive': '#3498db'}\n\nby_condition = defaultdict(list)\nfor r in all_results:\n    if r.get('success'):\n        by_condition[r['condition']].append(r)\n\nprint(\"=\" * 60)\nprint(\"RESULTS BY CONDITION\")\nprint(\"=\" * 60)\nfor condition, runs in by_condition.items():\n    print(f\"\\n{condition} ({len(runs)} successful runs):\")\n    for run in runs:\n        print(f\"  Seed {run['seed_id']}: reward={run['total_reward']:.1f}, pop={run['final_population']:.1f}\")\n\n# Learning curves note\nprint()\nprint(\"Note: Learning curves not available in this version.\")\nprint(\"ParallelTrainer returns only final metrics, not training history.\")\nprint(\"To enable curves, modify ParallelTrainer.train() to return all_metrics.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Aggregate Results & Summary Table\n",
    "\n",
    "summary = {}\n",
    "for cond, runs in by_condition.items():\n",
    "    rewards = [r['total_reward'] for r in runs]\n",
    "    pops = [r['final_population'] for r in runs]\n",
    "    n = len(rewards)\n",
    "    summary[cond] = {\n",
    "        'reward_mean': np.mean(rewards),\n",
    "        'reward_std': np.std(rewards, ddof=1) if n > 1 else 0,\n",
    "        'reward_ci': 1.96 * np.std(rewards, ddof=1) / np.sqrt(n) if n > 1 else 0,  # 95% CI\n",
    "        'pop_mean': np.mean(pops),\n",
    "        'pop_std': np.std(pops, ddof=1) if n > 1 else 0,\n",
    "        'n': n,\n",
    "    }\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Condition':<12} | {'Reward (mean +/- std)':<25} | {'95% CI':<20} | {'Population':<15}\")\n",
    "print(\"-\" * 70)\n",
    "for cond in CONDITIONS:\n",
    "    if cond not in summary:\n",
    "        print(f\"{cond:<12} | {'(no data)':<25} |\")\n",
    "        continue\n",
    "    s = summary[cond]\n",
    "    ci_lo = s['reward_mean'] - s['reward_ci']\n",
    "    ci_hi = s['reward_mean'] + s['reward_ci']\n",
    "    print(f\"{cond:<12} | {s['reward_mean']:>8.1f} +/- {s['reward_std']:<8.1f} | \"\n",
    "          f\"[{ci_lo:.1f}, {ci_hi:.1f}] | \"\n",
    "          f\"{s['pop_mean']:.1f} +/- {s['pop_std']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Statistical Tests\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STATISTICAL TESTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def cohens_d(x, y):\n",
    "    \"\"\"Compute Cohen's d effect size.\"\"\"\n",
    "    nx, ny = len(x), len(y)\n",
    "    if nx < 2 or ny < 2:\n",
    "        return 0.0\n",
    "    pooled_std = np.sqrt(((nx-1)*np.var(x, ddof=1) + (ny-1)*np.var(y, ddof=1)) / (nx+ny-2))\n",
    "    return (np.mean(x) - np.mean(y)) / pooled_std if pooled_std > 0 else 0\n",
    "\n",
    "adaptive_rewards = [r['total_reward'] for r in by_condition.get('adaptive', [])]\n",
    "on_rewards = [r['total_reward'] for r in by_condition.get('field_on', [])]\n",
    "off_rewards = [r['total_reward'] for r in by_condition.get('field_off', [])]\n",
    "\n",
    "# ADAPTIVE vs ON\n",
    "if len(adaptive_rewards) >= 2 and len(on_rewards) >= 2:\n",
    "    t_ao, p_ao = stats.ttest_ind(adaptive_rewards, on_rewards, equal_var=False)\n",
    "    d_ao = cohens_d(adaptive_rewards, on_rewards)\n",
    "    print(f\"\\nADAPTIVE vs FIELD_ON:\")\n",
    "    print(f\"  Welch's t = {t_ao:.4f}, p = {p_ao:.4f}\")\n",
    "    print(f\"  Cohen's d = {d_ao:.4f} ({'small' if abs(d_ao)<0.5 else 'medium' if abs(d_ao)<0.8 else 'large'})\")\n",
    "    print(f\"  Direction: {'ADAPTIVE wins' if np.mean(adaptive_rewards) > np.mean(on_rewards) else 'FIELD_ON wins'}\")\n",
    "else:\n",
    "    p_ao, d_ao = 1.0, 0.0\n",
    "    print(\"\\nADAPTIVE vs FIELD_ON: Not enough data\")\n",
    "\n",
    "# ADAPTIVE vs OFF\n",
    "if len(adaptive_rewards) >= 2 and len(off_rewards) >= 2:\n",
    "    t_af, p_af = stats.ttest_ind(adaptive_rewards, off_rewards, equal_var=False)\n",
    "    d_af = cohens_d(adaptive_rewards, off_rewards)\n",
    "    print(f\"\\nADAPTIVE vs FIELD_OFF:\")\n",
    "    print(f\"  Welch's t = {t_af:.4f}, p = {p_af:.4f}\")\n",
    "    print(f\"  Cohen's d = {d_af:.4f} ({'small' if abs(d_af)<0.5 else 'medium' if abs(d_af)<0.8 else 'large'})\")\n",
    "    print(f\"  Direction: {'ADAPTIVE wins' if np.mean(adaptive_rewards) > np.mean(off_rewards) else 'FIELD_OFF wins'}\")\n",
    "else:\n",
    "    p_af, d_af = 1.0, 0.0\n",
    "    print(\"\\nADAPTIVE vs FIELD_OFF: Not enough data\")\n",
    "\n",
    "# ON vs OFF\n",
    "if len(on_rewards) >= 2 and len(off_rewards) >= 2:\n",
    "    t_of, p_of = stats.ttest_ind(on_rewards, off_rewards, equal_var=False)\n",
    "    d_of = cohens_d(on_rewards, off_rewards)\n",
    "    print(f\"\\nFIELD_ON vs FIELD_OFF:\")\n",
    "    print(f\"  Welch's t = {t_of:.4f}, p = {p_of:.4f}\")\n",
    "    print(f\"  Cohen's d = {d_of:.4f}\")\n",
    "else:\n",
    "    print(\"\\nFIELD_ON vs FIELD_OFF: Not enough data\")\n",
    "\n",
    "# Winner\n",
    "all_means = []\n",
    "for cond in CONDITIONS:\n",
    "    rewards = [r['total_reward'] for r in by_condition.get(cond, [])]\n",
    "    if rewards:\n",
    "        all_means.append((np.mean(rewards), cond))\n",
    "if all_means:\n",
    "    winner = max(all_means, key=lambda x: x[0])\n",
    "    print(f\"\\n*** WINNER: {winner[1].upper()} (mean reward = {winner[0]:.1f}) ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Bar Chart Comparison\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "bar_colors = ['#2ecc71', '#e74c3c', '#3498db']  # green, red, blue\n",
    "\n",
    "# (a) Reward\n",
    "ax = axes[0]\n",
    "means = [summary.get(c, {}).get('reward_mean', 0) for c in CONDITIONS]\n",
    "cis = [summary.get(c, {}).get('reward_ci', 0) for c in CONDITIONS]\n",
    "bars = ax.bar(CONDITIONS, means, yerr=cis, color=bar_colors, capsize=8, edgecolor='black')\n",
    "for bar, m in zip(bars, means):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(cis) + 0.5,\n",
    "            f'{m:.1f}', ha='center', fontsize=10)\n",
    "ax.set_ylabel('Total Reward')\n",
    "ax.set_title('(a) Reward Comparison (mean +/- 95% CI)')\n",
    "\n",
    "# (b) Population\n",
    "ax = axes[1]\n",
    "means = [summary.get(c, {}).get('pop_mean', 0) for c in CONDITIONS]\n",
    "stds = [summary.get(c, {}).get('pop_std', 0) for c in CONDITIONS]\n",
    "bars = ax.bar(CONDITIONS, means, yerr=stds, color=bar_colors, capsize=8, edgecolor='black')\n",
    "ax.set_ylabel('Final Population')\n",
    "ax.set_title('(b) Population Comparison (mean +/- std)')\n",
    "ax.axhline(64, color='red', linestyle='--', alpha=0.5, label='Max capacity')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(DRIVE_BASE, 'comparison.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Gate Analysis Over Training (Adaptive Only)\n",
    "# Note: Gate metrics during training are not captured by ParallelTrainer.\n",
    "# This cell shows placeholder / future analysis.\n",
    "\n",
    "adaptive_runs = by_condition.get('adaptive', [])\n",
    "if not adaptive_runs:\n",
    "    print(\"No adaptive runs completed yet.\")\n",
    "else:\n",
    "    print(f\"Adaptive runs: {len(adaptive_runs)}\")\n",
    "    print(\"\")\n",
    "    print(\"Note: ParallelTrainer doesn't currently log gate metrics during training.\")\n",
    "    print(\"Gate behavior is analyzed via final gate_bias_values in Cell 12.\")\n",
    "    print(\"\")\n",
    "    print(\"To analyze gate evolution over training, you would need to:\")\n",
    "    print(\"1. Add gate metric logging to parallel_train.py single_seed_train_step()\")\n",
    "    print(\"2. Capture 'gate/mean' and 'gate/channel_X_mean' in all_metrics dict\")\n",
    "    print(\"3. Re-run training with updated code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Gate Distribution Histogram (Bimodality Check) - CRITICAL\n",
    "\n",
    "# Initialize for scope safety (used in Cell 13)\n",
    "gate_arr = None\n",
    "all_gate_bias_values = []\n",
    "\n",
    "# Collect final gate bias values from all adaptive runs\n",
    "for run in by_condition.get('adaptive', []):\n",
    "    if 'gate_bias_values' in run:\n",
    "        all_gate_bias_values.append(run['gate_bias_values'])\n",
    "\n",
    "if not all_gate_bias_values:\n",
    "    print(\"No gate bias values collected. Skipping histogram.\")\n",
    "    print(\"(This is expected if adaptive runs haven't completed yet)\")\n",
    "else:\n",
    "    gate_arr = np.concatenate(all_gate_bias_values, axis=0)  # (n_agents_total, num_channels)\n",
    "    print(f\"Collected gate biases from {len(all_gate_bias_values)} runs, {gate_arr.shape[0]} agents total\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    for ch in range(4):\n",
    "        ax = axes[ch // 2, ch % 2]\n",
    "        gate_ch = gate_arr[:, ch]\n",
    "        ax.hist(gate_ch, bins=20, edgecolor='black', alpha=0.7)\n",
    "        ax.axvline(0.0, color='red', linestyle='--', alpha=0.5, label='Bias=0 (neutral)')\n",
    "        ax.set_xlabel('Gate Bias Value')\n",
    "        ax.set_ylabel('Count (agents)')\n",
    "        ax.set_title(f'Channel {ch} Final Gate Bias Distribution')\n",
    "\n",
    "        # Check for bimodality: peaks at extremes\n",
    "        low_count = np.sum(gate_ch < -0.5)\n",
    "        high_count = np.sum(gate_ch > 0.5)\n",
    "        if low_count > len(gate_ch)*0.2 and high_count > len(gate_ch)*0.2:\n",
    "            ax.text(0.5, 0.95, 'BIMODAL (specialization!)',\n",
    "                    transform=ax.transAxes, ha='center', fontsize=10, color='green', weight='bold')\n",
    "        ax.legend()\n",
    "\n",
    "    plt.suptitle('Final Gate Bias Distribution (Bimodal = Agents Specialize!)', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(DRIVE_BASE, 'gate_bias_distribution.png'), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    # Inter-agent variance analysis\n",
    "    print(\"=\" * 60)\n",
    "    print(\"INTER-AGENT GATE BIAS VARIANCE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    for ch in range(4):\n",
    "        gate_ch = gate_arr[:, ch]\n",
    "        var = np.var(gate_ch)\n",
    "        print(f\"Channel {ch}: mean={np.mean(gate_ch):.3f}, var={var:.4f} \"\n",
    "              f\"({'high variance = differentiation!' if var > 0.05 else 'low variance = uniform'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Conclusion\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Key questions answered:\")\n",
    "print()\n",
    "\n",
    "# 1. Winner determination\n",
    "print(\"1. Does ADAPTIVE beat both ON and OFF?\")\n",
    "all_means = []\n",
    "for cond in CONDITIONS:\n",
    "    rewards = [r['total_reward'] for r in by_condition.get(cond, [])]\n",
    "    if rewards:\n",
    "        all_means.append((np.mean(rewards), cond))\n",
    "if all_means:\n",
    "    winner_name = max(all_means, key=lambda x: x[0])[1]\n",
    "    print(f\"   WINNER: {winner_name.upper()}\")\n",
    "    if 'p_ao' in dir() and 'p_af' in dir():\n",
    "        print(f\"   ADAPTIVE vs ON: p={p_ao:.4f}, d={d_ao:.4f}\")\n",
    "        print(f\"   ADAPTIVE vs OFF: p={p_af:.4f}, d={d_af:.4f}\")\n",
    "else:\n",
    "    print(\"   (No data available)\")\n",
    "print()\n",
    "\n",
    "# 2. Gate bias direction\n",
    "print(\"2. Do gate biases evolve toward negative (ignore) or positive (use)?\")\n",
    "if gate_arr is not None:\n",
    "    overall_mean = np.mean(gate_arr)\n",
    "    direction = (\"leaning toward IGNORE field\" if overall_mean < -0.2 else\n",
    "                 \"leaning toward USE field\" if overall_mean > 0.2 else \"neutral\")\n",
    "    print(f\"   Overall mean gate bias = {overall_mean:.3f} ({direction})\")\n",
    "else:\n",
    "    print(\"   (No gate data available)\")\n",
    "print()\n",
    "\n",
    "# 3. Per-channel patterns\n",
    "print(\"3. Different patterns per channel?\")\n",
    "print(\"   See per-channel histograms above.\")\n",
    "print()\n",
    "\n",
    "# 4. Specialization evidence\n",
    "print(\"4. Evidence of specialization (bimodal gate distribution)?\")\n",
    "if gate_arr is not None:\n",
    "    for ch in range(4):\n",
    "        gate_ch = gate_arr[:, ch]\n",
    "        low = np.sum(gate_ch < -0.5) / len(gate_ch)\n",
    "        high = np.sum(gate_ch > 0.5) / len(gate_ch)\n",
    "        if low > 0.2 and high > 0.2:\n",
    "            print(f\"   Channel {ch}: BIMODAL ({low:.0%} low, {high:.0%} high)\")\n",
    "        else:\n",
    "            print(f\"   Channel {ch}: Unimodal\")\n",
    "else:\n",
    "    print(\"   (No gate data available)\")\n",
    "print()\n",
    "\n",
    "print(\"Next steps:\")\n",
    "print(\"- If ADAPTIVE wins: run full 30-seed experiment\")\n",
    "print(\"- If gates are bimodal: agents are specializing!\")\n",
    "print(\"- If gate biases are negative: field may be noise\")\n",
    "print(\"- If gate biases are positive: field is useful\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}