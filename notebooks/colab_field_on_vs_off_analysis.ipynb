{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Field ON vs Field OFF: Statistical Comparison\n",
    "\n",
    "30 seeds per condition, 10M steps each, PROVEN 64-agent config.\n",
    "\n",
    "## Setup\n",
    "1. Runtime > Change runtime type > **TPU v6e** + **High-RAM**\n",
    "2. Run all cells (Ctrl+F9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "REPO_DIR = '/content/emergence-lab'\n",
    "GITHUB_USERNAME = \"imashishkh\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone https://github.com/{GITHUB_USERNAME}/emergence-lab.git {REPO_DIR}\n",
    "else:\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "!pip install -e \".[dev]\" -q\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport json\nimport pickle\nfrom pathlib import Path\n\nfrom src.analysis.statistics import (\n    compute_iqm, compare_methods, welch_t_test, mann_whitney_test,\n    probability_of_improvement,\n)\nfrom src.analysis.paper_figures import (\n    setup_publication_style, plot_performance_profiles,\n    save_figure,\n)\n\nFIELD_OFF_DIR = '/content/drive/MyDrive/emergence-lab/field_off/'\nOUTPUT_DIR = '/content/drive/MyDrive/emergence-lab/analysis_results/'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint(\"Imports loaded. Output dir:\", OUTPUT_DIR)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Training-Time Data\n",
    "\n",
    "All 60 reward values hardcoded from training logs (no checkpoint loading needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== TRAINING-TIME DATA ==========\n",
    "# Source: EXPERIMENT_LOG.md (Field ON) and training output (Field OFF)\n",
    "\n",
    "field_on_rewards = np.array([\n",
    "    5.19, 5.38, 4.70, 3.09, 4.84, 5.50, 2.54, 0.00, 5.18, 4.52,\n",
    "    5.09, 5.33, 5.38, 3.70, 5.24, 4.61, 3.46, 4.56, 5.48, 4.42,\n",
    "    5.43, 4.56, 5.30, 4.99, 4.22, 4.33, 5.51, 4.19, 4.67, 5.20,\n",
    "])\n",
    "\n",
    "field_on_populations = np.array([\n",
    "    64, 64, 6, 22, 64, 64, 11, 0, 64, 20,\n",
    "    64, 64, 40, 30, 48, 62, 39, 58, 64, 64,\n",
    "    64, 58, 64, 30, 28, 50, 62, 52, 60, 58,\n",
    "])\n",
    "\n",
    "field_off_rewards = np.array([\n",
    "    5.527, 5.614, 5.378, 5.728, 5.547, 5.624, 5.618, 5.386, 5.685, 5.600,\n",
    "    5.430, 4.785, 5.542, 5.556, 5.610, 5.709, 5.494, 5.487, 5.695, 5.661,\n",
    "    5.457, 5.605, 5.180, 5.588, 5.428, 5.399, 5.476, 5.297, 5.712, 5.670,\n",
    "])\n",
    "\n",
    "# Field OFF populations not available from training output - will get from eval\n",
    "field_off_populations = None  # Will be populated in Phase 3\n",
    "\n",
    "print(f\"Field ON:  {len(field_on_rewards)} seeds, mean={field_on_rewards.mean():.3f} +/- {field_on_rewards.std():.3f}\")\n",
    "print(f\"Field OFF: {len(field_off_rewards)} seeds, mean={field_off_rewards.mean():.3f} +/- {field_off_rewards.std():.3f}\")\n",
    "print(f\"\\nField ON population: mean={field_on_populations.mean():.1f}, at max(64): {np.sum(field_on_populations==64)}/30\")\n",
    "print(f\"Field ON failed seeds (reward < 1.0): {np.sum(field_on_rewards < 1.0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== DESCRIPTIVE STATISTICS ==========\nprint(\"=\"*60)\nprint(\"DESCRIPTIVE STATISTICS\")\nprint(\"=\"*60)\n\nfor name, rewards in [(\"Field ON\", field_on_rewards), (\"Field OFF\", field_off_rewards)]:\n    iqm = compute_iqm(rewards, n_bootstrap=10000, seed=42)\n    print(f\"\\n{name} (n={len(rewards)}):\")\n    print(f\"  Mean:   {rewards.mean():.4f} +/- {rewards.std(ddof=1):.4f}\")\n    print(f\"  Median: {np.median(rewards):.4f}\")\n    print(f\"  IQM:    {iqm.iqm:.4f} [{iqm.ci_lower:.4f}, {iqm.ci_upper:.4f}]\")\n    print(f\"  Min:    {rewards.min():.4f}\")\n    print(f\"  Max:    {rewards.max():.4f}\")\n    print(f\"  CoV:    {rewards.std(ddof=1)/rewards.mean():.4f}\")\n\n# Compute and store IQMs for later\niqm_on = compute_iqm(field_on_rewards, n_bootstrap=10000, seed=42)\niqm_off = compute_iqm(field_off_rewards, n_bootstrap=10000, seed=42)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== HYPOTHESIS TESTS ==========\n",
    "print(\"=\"*60)\n",
    "print(\"HYPOTHESIS TESTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Welch's t-test\n",
    "welch = welch_t_test(field_off_rewards, field_on_rewards)\n",
    "print(f\"\\n1. Welch's t-test:\")\n",
    "print(f\"   t = {welch.statistic:.4f}, p = {welch.p_value:.6f}\")\n",
    "print(f\"   Cohen's d = {welch.effect_size:.4f}\", end=\"\")\n",
    "d = abs(welch.effect_size)\n",
    "if d < 0.2: print(\" (negligible)\")\n",
    "elif d < 0.5: print(\" (small)\")\n",
    "elif d < 0.8: print(\" (MEDIUM)\")\n",
    "else: print(\" (LARGE)\")\n",
    "print(f\"   Significant at alpha=0.05: {welch.significant}\")\n",
    "\n",
    "# Mann-Whitney U\n",
    "mw = mann_whitney_test(field_off_rewards, field_on_rewards)\n",
    "print(f\"\\n2. Mann-Whitney U test:\")\n",
    "print(f\"   U = {mw.statistic:.1f}, p = {mw.p_value:.6f}\")\n",
    "print(f\"   Rank-biserial r = {mw.effect_size:.4f}\")\n",
    "print(f\"   Significant at alpha=0.05: {mw.significant}\")\n",
    "\n",
    "# Probability of Improvement\n",
    "poi = probability_of_improvement(field_off_rewards, field_on_rewards, n_bootstrap=5000, seed=42)\n",
    "print(f\"\\n3. Probability of Improvement:\")\n",
    "print(f\"   P(Field OFF > Field ON) = {poi['prob_x_better']:.4f}\")\n",
    "print(f\"   P(Field ON > Field OFF) = {poi['prob_y_better']:.4f}\")\n",
    "print(f\"   95% CI: [{poi['ci_lower']:.4f}, {poi['ci_upper']:.4f}]\")\n",
    "\n",
    "# Direction\n",
    "print(f\"\\n4. Direction:\")\n",
    "print(f\"   Field OFF mean: {field_off_rewards.mean():.4f}\")\n",
    "print(f\"   Field ON mean:  {field_on_rewards.mean():.4f}\")\n",
    "print(f\"   Gap: {field_off_rewards.mean() - field_on_rewards.mean():+.4f} (Field OFF {'higher' if field_off_rewards.mean() > field_on_rewards.mean() else 'lower'})\")\n",
    "\n",
    "# Sensitivity: exclude failed seeds\n",
    "mask_on = field_on_rewards > 0.5\n",
    "on_filtered = field_on_rewards[mask_on]\n",
    "welch_f = welch_t_test(field_off_rewards, on_filtered)\n",
    "print(f\"\\n5. Sensitivity (excluding {np.sum(~mask_on)} failed Field ON seeds):\")\n",
    "print(f\"   Field ON filtered: n={len(on_filtered)}, mean={on_filtered.mean():.4f}\")\n",
    "print(f\"   Welch p = {welch_f.p_value:.6f}, Cohen's d = {welch_f.effect_size:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== FULL METHOD COMPARISON ==========\n",
    "print(\"=\"*60)\n",
    "print(\"FULL METHOD COMPARISON (rliable-style)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison = compare_methods(\n",
    "    {\"Field ON\": field_on_rewards, \"Field OFF\": field_off_rewards},\n",
    "    n_bootstrap=10000, seed=42,\n",
    ")\n",
    "print(comparison.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== COMPARISON PLOTS ==========\n",
    "setup_publication_style()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Bar chart: IQM with CI\n",
    "ax = axes[0]\n",
    "methods = ['Field ON\\n(Stigmergy)', 'Field OFF\\n(No Field)']\n",
    "iqm_vals = [iqm_on.iqm, iqm_off.iqm]\n",
    "iqm_lo = [iqm_on.iqm - iqm_on.ci_lower, iqm_off.iqm - iqm_off.ci_lower]\n",
    "iqm_hi = [iqm_on.ci_upper - iqm_on.iqm, iqm_off.ci_upper - iqm_off.iqm]\n",
    "colors = ['#009988', '#BBBBBB']\n",
    "\n",
    "bars = ax.bar(methods, iqm_vals, yerr=[iqm_lo, iqm_hi], color=colors,\n",
    "              edgecolor='black', linewidth=0.5, capsize=8)\n",
    "for bar, val in zip(bars, iqm_vals):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(iqm_hi) + 0.05,\n",
    "            f'{val:.3f}', ha='center', fontsize=10)\n",
    "ax.set_ylabel('IQM Reward')\n",
    "ax.set_title('(a) IQM + 95% CI')\n",
    "sig = f\"p = {welch.p_value:.4f}\"\n",
    "if welch.p_value < 0.001: sig += \" ***\"\n",
    "elif welch.p_value < 0.01: sig += \" **\"\n",
    "elif welch.p_value < 0.05: sig += \" *\"\n",
    "ax.text(0.5, max(iqm_vals) + max(iqm_hi) + 0.3, sig, ha='center', fontsize=11,\n",
    "        transform=ax.get_xaxis_transform())\n",
    "\n",
    "# 2. Violin + swarm\n",
    "ax = axes[1]\n",
    "parts = ax.violinplot([field_on_rewards, field_off_rewards], positions=[1, 2],\n",
    "                       showmeans=True, showmedians=True, showextrema=False)\n",
    "for i, body in enumerate(parts['bodies']):\n",
    "    body.set_facecolor(colors[i])\n",
    "    body.set_alpha(0.4)\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "for i, (data, pos) in enumerate(zip([field_on_rewards, field_off_rewards], [1, 2])):\n",
    "    jitter = rng.normal(0, 0.05, size=len(data))\n",
    "    ax.scatter(np.full_like(data, pos) + jitter, data, alpha=0.6, s=25,\n",
    "               color=colors[i], edgecolor='black', linewidth=0.3, zorder=3)\n",
    "\n",
    "ax.scatter([1], [iqm_on.iqm], marker='D', s=80, color='red', zorder=5, label='IQM')\n",
    "ax.scatter([2], [iqm_off.iqm], marker='D', s=80, color='red', zorder=5)\n",
    "ax.set_xticks([1, 2])\n",
    "ax.set_xticklabels(['Field ON', 'Field OFF'])\n",
    "ax.set_ylabel('Mean Reward')\n",
    "ax.set_title('(b) Distribution (all 60 seeds)')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# 3. Population\n",
    "ax = axes[2]\n",
    "ax.hist(field_on_populations, bins=15, alpha=0.7, color=colors[0], label='Field ON', edgecolor='black')\n",
    "ax.set_xlabel('Final Population')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('(c) Field ON Population Distribution')\n",
    "ax.axvline(x=64, color='red', linestyle='--', alpha=0.5, label='Max capacity')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "plt.suptitle('Field ON vs Field OFF: 30-Seed Comparison', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "save_figure(fig, os.path.join(OUTPUT_DIR, 'main_comparison'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== PERFORMANCE PROFILES ==========\nsetup_publication_style()\nfig = plot_performance_profiles(\n    {\"Field ON (Stigmergy)\": field_on_rewards, \"Field OFF (No Field)\": field_off_rewards},\n    output_path=os.path.join(OUTPUT_DIR, 'performance_profiles'),\n    tau_range=(0, 1.05),\n)\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== REWARD vs POPULATION ==========\n",
    "setup_publication_style()\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.scatter(field_on_populations, field_on_rewards, color='#009988', label='Field ON',\n",
    "            s=50, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel('Final Population')\n",
    "ax.set_ylabel('Mean Reward')\n",
    "ax.set_title('Field ON: Reward vs Population (per seed)')\n",
    "ax.legend()\n",
    "\n",
    "# Annotate failed seed\n",
    "failed_idx = np.where(field_on_rewards < 1.0)[0]\n",
    "for idx in failed_idx:\n",
    "    ax.annotate(f'Seed {idx}\\n(died)', xy=(field_on_populations[idx], field_on_rewards[idx]),\n",
    "                fontsize=8, color='red', arrowprops=dict(arrowstyle='->', color='red'),\n",
    "                xytext=(field_on_populations[idx]+5, field_on_rewards[idx]+1))\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, os.path.join(OUTPUT_DIR, 'reward_vs_population'))\n",
    "plt.show()\n",
    "\n",
    "# Correlation\n",
    "from scipy import stats as scipy_stats\n",
    "mask = field_on_rewards > 0.5  # exclude dead seed\n",
    "r, p = scipy_stats.pearsonr(field_on_populations[mask], field_on_rewards[mask])\n",
    "print(f\"Correlation (pop vs reward, excluding failed): r={r:.3f}, p={p:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Phase 3: Checkpoint Analysis (Field OFF)\n\nLoad Field OFF checkpoints from Drive, run eval episodes, compute weight divergence.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ========== LOAD FIELD OFF CHECKPOINTS ==========\nimport glob as glob_mod\nimport jax\nimport jax.numpy as jnp\nfrom src.training.checkpointing import load_checkpoint\nfrom src.agents.network import ActorCritic\nfrom src.environment.obs import obs_dim\nfrom src.analysis.ablation import _run_episode_full\n\n# Discover checkpoints\ncheckpoint_paths = []\nfor batch_idx in range(10):\n    batch_dir = os.path.join(FIELD_OFF_DIR, f'batch_{batch_idx}')\n    if not os.path.exists(batch_dir):\n        continue\n    for seed_dir in sorted(os.listdir(batch_dir)):\n        seed_path = os.path.join(batch_dir, seed_dir)\n        if not os.path.isdir(seed_path):\n            continue\n        pkl_files = glob_mod.glob(os.path.join(seed_path, 'step_*.pkl'))\n        if pkl_files:\n            checkpoint_paths.append(sorted(pkl_files)[-1])\n\nprint(f\"Found {len(checkpoint_paths)} Field OFF checkpoints\")\n\n# Load helper\ndef load_seed_data(ckpt_path):\n    ckpt = load_checkpoint(ckpt_path)\n    config = ckpt['config']\n    agent_params = jax.tree_util.tree_map(lambda x: x[0], ckpt['agent_params'])\n    network = ActorCritic(hidden_dims=tuple(config.agent.hidden_dims), num_actions=6)\n    return {\n        'params': ckpt['params'],\n        'agent_params': agent_params,\n        'config': config,\n        'network': network,\n        'seed_id': ckpt.get('seed_id', -1),\n    }\n\n# Test load\ntest_data = load_seed_data(checkpoint_paths[0])\nprint(f\"Test load OK: seed {test_data['seed_id']}, grid={test_data['config'].env.grid_size}\")\nprint(f\"Field OFF config: decay={test_data['config'].field.decay_rate}, diffusion={test_data['config'].field.diffusion_rate}, write={test_data['config'].field.write_strength}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ========== EVAL EPISODES (Field OFF) ==========\n# Run 5 eval episodes per seed to get population dynamics + survival stats\n# Uses shared params (not per-agent); \"normal\" condition since field already off in config\n\nNUM_EVAL_EPISODES = 5\neval_results = []\n\nfor i, ckpt_path in enumerate(checkpoint_paths):\n    seed_data = load_seed_data(ckpt_path)\n    config = seed_data['config']\n\n    seed_pops = []\n    seed_rewards = []\n    seed_births = []\n    seed_deaths = []\n\n    for ep in range(NUM_EVAL_EPISODES):\n        key = jax.random.PRNGKey(ep * 1000 + i)\n        stats = _run_episode_full(\n            network=seed_data['network'],\n            params=seed_data['params'],\n            config=config,\n            key=key,\n            condition=\"normal\",  # field already disabled in config\n            evolution=True,\n        )\n        seed_pops.append(stats.final_population)\n        seed_rewards.append(stats.total_reward)\n        seed_births.append(stats.total_births)\n        seed_deaths.append(stats.total_deaths)\n\n    eval_results.append({\n        'seed_id': seed_data['seed_id'],\n        'ckpt_path': ckpt_path,\n        'mean_total_reward': np.mean(seed_rewards),\n        'std_total_reward': np.std(seed_rewards),\n        'mean_population': np.mean(seed_pops),\n        'mean_births': np.mean(seed_births),\n        'mean_deaths': np.mean(seed_deaths),\n        'survival_rate': np.mean(seed_pops) / config.evolution.max_agents,\n        'all_rewards': seed_rewards,\n        'all_populations': seed_pops,\n    })\n\n    if (i + 1) % 5 == 0 or i == 0:\n        print(f\"  [{i+1}/{len(checkpoint_paths)}] seed {seed_data['seed_id']}: \"\n              f\"total_reward={np.mean(seed_rewards):.1f}, pop={np.mean(seed_pops):.1f}, \"\n              f\"births={np.mean(seed_births):.0f}, deaths={np.mean(seed_deaths):.0f}\")\n\n# Extract Field OFF populations from eval\nfield_off_populations = np.array([r['mean_population'] for r in eval_results])\nfield_off_eval_total_rewards = np.array([r['mean_total_reward'] for r in eval_results])\n\nprint(f\"\\n{'='*60}\")\nprint(f\"FIELD OFF EVAL SUMMARY ({len(eval_results)} seeds x {NUM_EVAL_EPISODES} episodes)\")\nprint(f\"  Mean total reward: {field_off_eval_total_rewards.mean():.1f} +/- {field_off_eval_total_rewards.std():.1f}\")\nprint(f\"  Mean population:   {field_off_populations.mean():.1f} +/- {field_off_populations.std():.1f}\")\nprint(f\"  At max capacity:   {np.sum(field_off_populations >= 60)}/{len(field_off_populations)}\")\nprint(f\"  Mean births:       {np.mean([r['mean_births'] for r in eval_results]):.1f}\")\nprint(f\"  Mean deaths:       {np.mean([r['mean_deaths'] for r in eval_results]):.1f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ========== WEIGHT DIVERGENCE (Field OFF) ==========\nfrom src.analysis.specialization import compute_weight_divergence\n\ndivergence_results = []\n\nfor i, ckpt_path in enumerate(checkpoint_paths):\n    seed_data = load_seed_data(ckpt_path)\n    agent_params = seed_data['agent_params']\n\n    # Compute weight divergence across all agents in this seed\n    div = compute_weight_divergence(agent_params)\n    divergence_results.append({\n        'seed_id': seed_data['seed_id'],\n        'mean_divergence': float(div['mean_divergence']),\n        'max_divergence': float(div['max_divergence']),\n        'n_agents': len(div['agent_indices']),\n    })\n\n    if (i + 1) % 10 == 0 or i == 0:\n        print(f\"  [{i+1}/{len(checkpoint_paths)}] seed {seed_data['seed_id']}: \"\n              f\"mean_div={div['mean_divergence']:.4f}, max_div={div['max_divergence']:.4f}, \"\n              f\"n_agents={len(div['agent_indices'])}\")\n\nfield_off_divergences = np.array([r['mean_divergence'] for r in divergence_results])\nfield_off_max_divs = np.array([r['max_divergence'] for r in divergence_results])\n\nprint(f\"\\n{'='*60}\")\nprint(f\"FIELD OFF WEIGHT DIVERGENCE SUMMARY ({len(divergence_results)} seeds)\")\nprint(f\"  Mean divergence:  {field_off_divergences.mean():.4f} +/- {field_off_divergences.std():.4f}\")\nprint(f\"  Max divergence:   {field_off_max_divs.mean():.4f} +/- {field_off_max_divs.std():.4f}\")\nprint(f\"  Range: [{field_off_divergences.min():.4f}, {field_off_divergences.max():.4f}]\")\n\n# Plot weight divergence distribution\nsetup_publication_style()\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nax = axes[0]\nax.hist(field_off_divergences, bins=15, color='#BBBBBB', edgecolor='black', alpha=0.7)\nax.axvline(field_off_divergences.mean(), color='red', linestyle='--', label=f'Mean={field_off_divergences.mean():.4f}')\nax.set_xlabel('Mean Pairwise Weight Divergence (cosine)')\nax.set_ylabel('Count')\nax.set_title('(a) Field OFF: Weight Divergence Distribution')\nax.legend()\n\n# Population vs reward scatter for Field OFF\nax = axes[1]\nif field_off_populations is not None and len(field_off_populations) == len(field_off_rewards):\n    ax.scatter(field_off_populations, field_off_rewards, color='#BBBBBB', s=50,\n               alpha=0.7, edgecolor='black', linewidth=0.5)\n    ax.set_xlabel('Eval Population')\n    ax.set_ylabel('Training Reward')\n    ax.set_title('(b) Field OFF: Reward vs Eval Population')\nelse:\n    ax.text(0.5, 0.5, 'Eval populations not yet available',\n            ha='center', va='center', transform=ax.transAxes)\n    ax.set_title('(b) Field OFF: Reward vs Population')\n\nplt.tight_layout()\nsave_figure(fig, os.path.join(OUTPUT_DIR, 'field_off_analysis'))\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Phase 4: Report & Save\n\nGenerate formatted comparison report and save all results to Drive.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ========== COMPARISON REPORT ==========\nfrom datetime import datetime\n\n# Determine significance level\nif welch.p_value < 0.001:\n    sig_str = \"p < 0.001 (***)\"\nelif welch.p_value < 0.01:\n    sig_str = f\"p = {welch.p_value:.4f} (**)\"\nelif welch.p_value < 0.05:\n    sig_str = f\"p = {welch.p_value:.4f} (*)\"\nelse:\n    sig_str = f\"p = {welch.p_value:.4f} (not significant)\"\n\nd_val = abs(welch.effect_size)\nif d_val < 0.2: d_str = \"negligible\"\nelif d_val < 0.5: d_str = \"small\"\nelif d_val < 0.8: d_str = \"medium\"\nelse: d_str = \"large\"\n\nwinner = \"Field OFF\" if field_off_rewards.mean() > field_on_rewards.mean() else \"Field ON\"\n\n# Use sample std (ddof=1) for CoV\ncov_on = field_on_rewards.std(ddof=1) / field_on_rewards.mean()\ncov_off = field_off_rewards.std(ddof=1) / field_off_rewards.mean()\n\nreport = f\"\"\"# Field ON vs Field OFF: 30-Seed Comparison Report\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n## Experiment Setup\n- **Conditions**: Field ON (stigmergy) vs Field OFF (no shared field)\n- **Seeds per condition**: 30\n- **Training steps**: 10M per seed\n- **Config**: 64-agent, grid=32, num_food=40, starting_energy=200\n- **Field ON**: diffusion=0.1, decay=0.05, write_strength=1.0\n- **Field OFF**: diffusion=0.0, decay=1.0, write_strength=0.0\n\n## Key Results\n\n### Reward Comparison\n| Metric | Field ON | Field OFF |\n|--------|----------|-----------|\n| Mean | {field_on_rewards.mean():.4f} +/- {field_on_rewards.std(ddof=1):.4f} | {field_off_rewards.mean():.4f} +/- {field_off_rewards.std(ddof=1):.4f} |\n| Median | {np.median(field_on_rewards):.4f} | {np.median(field_off_rewards):.4f} |\n| IQM | {iqm_on.iqm:.4f} [{iqm_on.ci_lower:.4f}, {iqm_on.ci_upper:.4f}] | {iqm_off.iqm:.4f} [{iqm_off.ci_lower:.4f}, {iqm_off.ci_upper:.4f}] |\n| Min | {field_on_rewards.min():.4f} | {field_off_rewards.min():.4f} |\n| Max | {field_on_rewards.max():.4f} | {field_off_rewards.max():.4f} |\n| CoV | {cov_on:.4f} | {cov_off:.4f} |\n\n### Statistical Tests\n- **Welch's t-test**: {sig_str}, Cohen's d = {welch.effect_size:.4f} ({d_str})\n- **Mann-Whitney U**: U = {mw.statistic:.1f}, p = {mw.p_value:.6f}, rank-biserial r = {mw.effect_size:.4f}\n- **P(Field OFF > Field ON)**: {poi['prob_x_better']:.4f}\n\n### Winner: **{winner}** (by mean reward)\n\n### Population Dynamics (Field ON)\n- Mean final population: {field_on_populations.mean():.1f}\n- At max capacity (64): {np.sum(field_on_populations == 64)}/30\n- Failed seeds (reward < 1.0): {np.sum(field_on_rewards < 1.0)}\n\n### Sensitivity Analysis\n- Excluding {np.sum(field_on_rewards < 0.5)} failed Field ON seeds:\n  Field ON filtered mean = {field_on_rewards[field_on_rewards > 0.5].mean():.4f} (n={np.sum(field_on_rewards > 0.5)})\n  Welch p = {welch_f.p_value:.6f}, Cohen's d = {welch_f.effect_size:.4f}\n\"\"\"\n\n# Add weight divergence if available\nif divergence_results:\n    report += f\"\"\"\n### Weight Divergence (Field OFF only)\n- Mean divergence: {field_off_divergences.mean():.4f} +/- {field_off_divergences.std():.4f}\n- Max divergence: {field_off_max_divs.mean():.4f} +/- {field_off_max_divs.std():.4f}\n\"\"\"\n\n# Add eval results if available\nif eval_results:\n    report += f\"\"\"\n### Eval Episodes (Field OFF, {NUM_EVAL_EPISODES} episodes/seed)\n- Mean total reward: {field_off_eval_total_rewards.mean():.1f} +/- {field_off_eval_total_rewards.std():.1f}\n- Mean population: {field_off_populations.mean():.1f} +/- {field_off_populations.std():.1f}\n- At max capacity: {np.sum(field_off_populations >= 60)}/{len(field_off_populations)}\n\"\"\"\n\nreport += f\"\"\"\n## Interpretation\n\n**Surprising finding**: Field OFF agents achieve {'higher' if field_off_rewards.mean() > field_on_rewards.mean() else 'lower'} mean reward than Field ON.\n\nKey observations:\n1. Field ON has MUCH higher variance (CoV {cov_on:.3f} vs {cov_off:.3f})\n2. Field ON has {np.sum(field_on_rewards < 1.0)} failed seed(s) with near-zero reward\n3. Field OFF is remarkably consistent across all 30 seeds\n4. When excluding failed seeds, the gap {'narrows' if abs(welch_f.effect_size) < abs(welch.effect_size) else 'remains'}\n\nPossible explanations:\n- The shared field may introduce a coordination overhead that hurts some seeds\n- Field ON populations are more variable (some collapse, some max out)\n- The field may be a harder optimization landscape requiring more training\n- Field OFF is simpler: agents just learn individual foraging without field-reading costs\n\n## Next Steps\n- Run Field ON checkpoints through eval episodes (need to upload to Drive)\n- Compare weight divergence between Field ON and Field OFF\n- Investigate why some Field ON seeds fail (population collapse)\n- Try longer training (20M+ steps) to see if Field ON catches up\n- Test with diversity_bonus and niche_pressure enabled\n\"\"\"\n\nfrom IPython.display import Markdown, display\ndisplay(Markdown(report))\nprint(\"\\nReport generated successfully.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ========== SAVE RESULTS ==========\nimport json\n\nresults = {\n    'metadata': {\n        'generated': datetime.now().isoformat(),\n        'field_on_seeds': 30,\n        'field_off_seeds': 30,\n        'steps_per_seed': 10_000_000,\n    },\n    'field_on': {\n        'rewards': field_on_rewards.tolist(),\n        'populations': field_on_populations.tolist(),\n        'mean_reward': float(field_on_rewards.mean()),\n        'std_reward': float(field_on_rewards.std()),\n        'iqm': float(iqm_on.iqm),\n        'iqm_ci': [float(iqm_on.ci_lower), float(iqm_on.ci_upper)],\n    },\n    'field_off': {\n        'rewards': field_off_rewards.tolist(),\n        'mean_reward': float(field_off_rewards.mean()),\n        'std_reward': float(field_off_rewards.std()),\n        'iqm': float(iqm_off.iqm),\n        'iqm_ci': [float(iqm_off.ci_lower), float(iqm_off.ci_upper)],\n    },\n    'tests': {\n        'welch_t': float(welch.statistic),\n        'welch_p': float(welch.p_value),\n        'cohens_d': float(welch.effect_size),\n        'mann_whitney_u': float(mw.statistic),\n        'mann_whitney_p': float(mw.p_value),\n        'prob_off_better': float(poi['prob_x_better']),\n    },\n}\n\n# Add eval results if available\nif eval_results:\n    results['field_off']['eval'] = {\n        'populations': field_off_populations.tolist(),\n        'total_rewards': field_off_eval_total_rewards.tolist(),\n        'per_seed': [{k: v for k, v in r.items() if k != 'ckpt_path'} for r in eval_results],\n    }\n\n# Add divergence results if available\nif divergence_results:\n    results['field_off']['weight_divergence'] = {\n        'mean_divergences': field_off_divergences.tolist(),\n        'max_divergences': field_off_max_divs.tolist(),\n        'per_seed': divergence_results,\n    }\n\n# Save JSON\njson_path = os.path.join(OUTPUT_DIR, 'field_on_vs_off_results.json')\nwith open(json_path, 'w') as f:\n    json.dump(results, f, indent=2)\nprint(f\"JSON saved: {json_path}\")\n\n# Save pickle (preserves numpy arrays)\npkl_path = os.path.join(OUTPUT_DIR, 'field_on_vs_off_results.pkl')\nwith open(pkl_path, 'wb') as f:\n    pickle.dump(results, f)\nprint(f\"Pickle saved: {pkl_path}\")\n\n# Save report markdown\nmd_path = os.path.join(OUTPUT_DIR, 'comparison_report.md')\nwith open(md_path, 'w') as f:\n    f.write(report)\nprint(f\"Report saved: {md_path}\")\n\n# List all output files\nprint(f\"\\n{'='*60}\")\nprint(f\"ALL OUTPUT FILES:\")\nfor f in sorted(os.listdir(OUTPUT_DIR)):\n    fpath = os.path.join(OUTPUT_DIR, f)\n    size_mb = os.path.getsize(fpath) / 1024 / 1024\n    print(f\"  {f} ({size_mb:.2f} MB)\")\nprint(f\"\\nDone! All results saved to {OUTPUT_DIR}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}