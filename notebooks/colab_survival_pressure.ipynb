{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survival Pressure: Does Environmental Harshness Make Stigmergy Essential?\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "| Prediction | Mechanism |\n",
    "|------------|----------|\n",
    "| Field ON agents survive better | Field encodes spatial memory of food locations, enabling efficient foraging under scarcity |\n",
    "| Field ON agents coordinate to reveal hidden food | Field gradients guide multiple agents to converge near hidden food sites |\n",
    "| Field OFF agents starve or fail to coordinate | Without shared medium, agents cannot communicate food locations or coordinate for hidden food |\n",
    "| The harsher the environment, the bigger the gap | Stigmergy becomes *essential*, not just *helpful*, when survival is at stake |\n",
    "\n",
    "## Experimental Design\n",
    "\n",
    "| Condition | Field | Seeds | Steps |\n",
    "|-----------|-------|-------|-------|\n",
    "| **Field ON** | diffusion=0.1, decay=0.05, write=1.0 | 30 | 10M |\n",
    "| **Field OFF** | diffusion=0.0, decay=1.0, write=0.0 | 30 | 10M |\n",
    "\n",
    "## Harsh Environment Config\n",
    "\n",
    "| Parameter | Value | Why |\n",
    "|-----------|-------|-----|\n",
    "| grid_size | 40 | Larger arena = harder to find food |\n",
    "| num_food | 10 | **SCARCE** (vs 40 in standard config) |\n",
    "| food_energy | 60 | **REDUCED** (vs 100 standard) |\n",
    "| num_hidden | 8 | Abundant hidden food — the coordination prize |\n",
    "| required_agents | 5 | Hard coordination requirement |\n",
    "| reveal_distance | 3 | Chebyshev distance for reveal |\n",
    "| hidden_food_value_multiplier | 10.0 | Each hidden food worth 600 energy (10x regular) |\n",
    "| max_agents | 64 | Population cap |\n",
    "| starting_energy | 200 | Starting energy per agent |\n",
    "| reproduce_threshold | 120 | Energy needed to reproduce |\n",
    "| reproduce_cost | 40 | Energy cost of reproduction |\n",
    "\n",
    "**The survival pressure**: Regular food alone may not sustain a large population. Agents that learn to coordinate and reveal hidden food get a massive energy windfall (600 per hidden food item). The field should be essential for this coordination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "if not os.path.exists('/content/emergence-lab'):\n",
    "    !git clone https://github.com/imashishkh21/emergence-lab.git /content/emergence-lab\n",
    "%cd /content/emergence-lab\n",
    "!git pull origin main\n",
    "\n",
    "!pip install -e \".[dev]\" -q\n",
    "!pip install rliable -q\n",
    "\n",
    "import jax\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"JAX backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1 — Quick Validation\n",
    "\n",
    "Before committing to 60 full training runs, verify:\n",
    "1. Populations survive the harsh environment (at least one condition has mean pop > 10)\n",
    "2. Check if hidden food is being revealed at all (may take longer than 2M steps)\n",
    "\n",
    "**Setup**: 3 seeds ON + 3 seeds OFF, ~2M steps (~8 iterations of 262K steps each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, gc\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from src.configs import Config, TrainingMode\n",
    "from src.training.parallel_train import ParallelTrainer\n",
    "from src.agents.network import ActorCritic\n",
    "from src.agents.policy import get_deterministic_actions\n",
    "from src.environment.env import reset, step\n",
    "from src.environment.obs import get_observations\n",
    "\n",
    "NUM_ENVS = 32\n",
    "NUM_STEPS = 128\n",
    "MAX_AGENTS = 64\n",
    "STEPS_PER_ITER = NUM_ENVS * NUM_STEPS * MAX_AGENTS  # 262,144\n",
    "VALIDATION_ITERS = 8  # ~2M steps\n",
    "\n",
    "\n",
    "def build_config(field_enabled: bool) -> Config:\n",
    "    \"\"\"Build harsh environment config for survival pressure experiment.\"\"\"\n",
    "    config = Config()\n",
    "    # 64-agent base on large grid\n",
    "    config.env.grid_size = 40\n",
    "    config.env.num_agents = 16\n",
    "    config.env.num_food = 10              # SCARCE\n",
    "    config.env.max_steps = 500            # Explicit episode length\n",
    "    config.evolution.enabled = True\n",
    "    config.evolution.max_agents = 64\n",
    "    config.evolution.starting_energy = 200\n",
    "    config.evolution.food_energy = 60     # REDUCED\n",
    "    config.evolution.energy_per_step = 1\n",
    "    config.evolution.reproduce_threshold = 120\n",
    "    config.evolution.reproduce_cost = 40\n",
    "    config.evolution.mutation_std = 0.01\n",
    "    # Hidden food\n",
    "    config.env.hidden_food.enabled = True\n",
    "    config.env.hidden_food.num_hidden = 8\n",
    "    config.env.hidden_food.required_agents = 5\n",
    "    config.env.hidden_food.reveal_distance = 3\n",
    "    config.env.hidden_food.hidden_food_value_multiplier = 10.0\n",
    "    # Training\n",
    "    config.train.training_mode = TrainingMode.GRADIENT\n",
    "    config.train.num_envs = NUM_ENVS\n",
    "    config.train.num_steps = NUM_STEPS\n",
    "    config.train.seed = 42\n",
    "    config.log.wandb = False\n",
    "    config.log.save_interval = 0\n",
    "    # Field\n",
    "    if not field_enabled:\n",
    "        config.field.diffusion_rate = 0.0\n",
    "        config.field.decay_rate = 1.0\n",
    "        config.field.write_strength = 0.0\n",
    "    return config\n",
    "\n",
    "\n",
    "def run_hidden_food_eval(network, params, config, key, num_episodes=1):\n",
    "    \"\"\"Run eval episodes tracking hidden food metrics.\"\"\"\n",
    "    all_results = []\n",
    "    for ep in range(num_episodes):\n",
    "        key, ep_key = jax.random.split(key)\n",
    "        state = reset(ep_key, config)\n",
    "        ep_reward = 0.0\n",
    "        ep_regular_food = 0.0\n",
    "        ep_hf_revealed = 0\n",
    "        ep_hf_collected = 0.0\n",
    "        for t in range(config.env.max_steps):\n",
    "            obs = get_observations(state, config)\n",
    "            obs_batched = obs[None, :, :]  # (1, max_agents, obs_dim)\n",
    "            actions = get_deterministic_actions(network, params, obs_batched)\n",
    "            actions = actions[0]  # (max_agents,)\n",
    "            pre_revealed = state.hidden_food_revealed\n",
    "            state, rewards, done, info = step(state, actions, config)\n",
    "            ep_reward += float(jnp.sum(rewards))\n",
    "            ep_regular_food += float(info['food_collected_this_step'])\n",
    "            ep_hf_collected += float(info['hidden_food_collected_this_step'])\n",
    "            if pre_revealed is not None and state.hidden_food_revealed is not None:\n",
    "                newly_revealed = (~pre_revealed) & state.hidden_food_revealed\n",
    "                ep_hf_revealed += int(jnp.sum(newly_revealed))\n",
    "            if bool(done):\n",
    "                break\n",
    "        food_energy = config.evolution.food_energy\n",
    "        hf_multiplier = config.env.hidden_food.hidden_food_value_multiplier\n",
    "        all_results.append({\n",
    "            'total_reward': ep_reward,\n",
    "            'regular_food_collected': ep_regular_food,\n",
    "            'hidden_food_revealed': ep_hf_revealed,\n",
    "            'hidden_food_collected': ep_hf_collected,\n",
    "            'regular_food_energy': ep_regular_food * food_energy,\n",
    "            'hidden_food_energy': ep_hf_collected * food_energy * hf_multiplier,\n",
    "            'final_population': int(jnp.sum(state.agent_alive.astype(jnp.int32))),\n",
    "        })\n",
    "    agg = {k: np.mean([r[k] for r in all_results]) for k in all_results[0]}\n",
    "    agg['per_episode'] = all_results\n",
    "    return agg\n",
    "\n",
    "\n",
    "# --- Validation training ---\n",
    "validation_results = {}\n",
    "for cond_name, field_on in [('field_on', True), ('field_off', False)]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"VALIDATION: {cond_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    config = build_config(field_on)\n",
    "    seed_ids = [100, 101, 102]\n",
    "    trainer = ParallelTrainer(\n",
    "        config=config, num_seeds=3, seed_ids=seed_ids,\n",
    "        checkpoint_dir=f'/tmp/validation_{cond_name}', master_seed=9999,\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    metrics = trainer.train(\n",
    "        num_iterations=VALIDATION_ITERS,\n",
    "        checkpoint_interval_minutes=999,   # No checkpoint writes\n",
    "        resume=False,\n",
    "        print_interval=2,\n",
    "    )\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    # Extract live population from trainer state\n",
    "    ps = trainer._parallel_state\n",
    "    # ps.env_state.agent_alive: shape (num_seeds, num_envs, max_agents)\n",
    "    alive = np.array(ps.env_state.agent_alive)       # (3, 32, 64)\n",
    "    pop_per_seed = alive.sum(axis=(1, 2)) / alive.shape[1]  # mean across envs\n",
    "    final_pops = [float(p) for p in pop_per_seed]\n",
    "\n",
    "    # Quick eval: 1 episode per condition with seed 0's params\n",
    "    network = ActorCritic(hidden_dims=(64, 64), num_actions=6)\n",
    "    eval_key = jax.random.PRNGKey(42)\n",
    "    # Use shared params from seed 0\n",
    "    seed0_params = jax.tree.map(lambda x: x[0], ps.params)\n",
    "    eval_result = run_hidden_food_eval(network, seed0_params, config, eval_key, num_episodes=1)\n",
    "\n",
    "    validation_results[cond_name] = {\n",
    "        'metrics': metrics, 'elapsed': elapsed,\n",
    "        'final_pops': final_pops, 'eval': eval_result,\n",
    "    }\n",
    "    print(f\"\\n{cond_name}: {elapsed:.0f}s\")\n",
    "    print(f\"  Final populations (per seed, mean across envs): {[f'{p:.1f}' for p in final_pops]}\")\n",
    "    print(f\"  Eval reward: {eval_result['total_reward']:.1f}\")\n",
    "    print(f\"  Eval HF revealed: {eval_result['hidden_food_revealed']}\")\n",
    "    print(f\"  Eval HF collected: {eval_result['hidden_food_collected']:.1f}\")\n",
    "    print(f\"  Eval final pop: {eval_result['final_population']}\")\n",
    "\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    jax.clear_caches()\n",
    "\n",
    "# --- Decision gate ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION DECISION GATE\")\n",
    "print(\"=\"*60)\n",
    "on_pops = validation_results['field_on']['final_pops']\n",
    "off_pops = validation_results['field_off']['final_pops']\n",
    "mean_on = np.mean(on_pops)\n",
    "mean_off = np.mean(off_pops)\n",
    "print(f\"Field ON  mean population: {mean_on:.1f}\")\n",
    "print(f\"Field OFF mean population: {mean_off:.1f}\")\n",
    "\n",
    "on_hfr = validation_results['field_on']['eval']['hidden_food_revealed']\n",
    "off_hfr = validation_results['field_off']['eval']['hidden_food_revealed']\n",
    "print(f\"Field ON  hidden food revealed (eval): {on_hfr}\")\n",
    "print(f\"Field OFF hidden food revealed (eval): {off_hfr}\")\n",
    "\n",
    "if mean_on > 10 or mean_off > 10:\n",
    "    print(\"\\nPASS: At least one condition sustains population > 10. Proceed to Phase 2.\")\n",
    "else:\n",
    "    print(\"\\nFAIL: Both conditions have dangerously low populations.\")\n",
    "    print(\"  Fallback 1: Increase num_food from 10 to 15 (slightly less harsh)\")\n",
    "    print(\"  Fallback 2: Increase food_energy from 60 to 80 (more reward per food)\")\n",
    "    print(\"  Fallback 3: Reduce required_agents from 5 to 4 (easier hidden food)\")\n",
    "    print(\"  Apply ONE change at a time and re-run validation.\")\n",
    "\n",
    "if on_hfr == 0 and off_hfr == 0:\n",
    "    print(\"\\nNOTE: No hidden food revealed in either condition at 2M steps.\")\n",
    "    print(\"  This is expected - coordination may take longer to emerge.\")\n",
    "    print(\"  If populations are healthy, proceed. HF coordination is the 10M-step question.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2 — Full Training\n",
    "\n",
    "30 seeds Field ON + 30 seeds Field OFF, 10M steps each.\n",
    "\n",
    "**Execution plan**:\n",
    "- 10 batches x 3 seeds per condition\n",
    "- Seed IDs: ON [0-29], OFF [50-79]\n",
    "- Resume-safe with checkpoint detection\n",
    "- Memory cleanup between every batch\n",
    "- Population crash detection after each batch\n",
    "- Checkpoints saved to Google Drive every 60 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, gc, pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "TOTAL_STEPS = 10_000_000\n",
    "SEEDS_PER_BATCH = 3\n",
    "TOTAL_BATCHES = 10\n",
    "CHECKPOINT_INTERVAL_MINUTES = 60\n",
    "RESUME = True\n",
    "\n",
    "steps_per_iter = NUM_ENVS * NUM_STEPS * MAX_AGENTS  # 262,144\n",
    "num_iterations = TOTAL_STEPS // steps_per_iter       # 38\n",
    "\n",
    "CHECKPOINT_BASE = '/content/drive/MyDrive/emergence-lab'\n",
    "CHECKPOINT_DIRS = {\n",
    "    'field_on': f'{CHECKPOINT_BASE}/survival_pressure_on',\n",
    "    'field_off': f'{CHECKPOINT_BASE}/survival_pressure_off',\n",
    "}\n",
    "\n",
    "print(f\"Steps per iteration: {steps_per_iter:,}\")\n",
    "print(f\"Total iterations: {num_iterations}\")\n",
    "print(f\"Total seeds per condition: {SEEDS_PER_BATCH * TOTAL_BATCHES}\")\n",
    "print(f\"Total training runs: {2 * SEEDS_PER_BATCH * TOTAL_BATCHES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for condition_name, field_enabled in [('field_on', True), ('field_off', False)]:\n",
    "    config = build_config(field_enabled)\n",
    "    config.train.total_steps = TOTAL_STEPS\n",
    "    checkpoint_dir_base = CHECKPOINT_DIRS[condition_name]\n",
    "    os.makedirs(checkpoint_dir_base, exist_ok=True)\n",
    "\n",
    "    all_results = []\n",
    "    cond_start = time.time()\n",
    "\n",
    "    for batch_number in range(TOTAL_BATCHES):\n",
    "        if condition_name == 'field_on':\n",
    "            seed_ids = list(range(\n",
    "                batch_number * SEEDS_PER_BATCH,\n",
    "                (batch_number + 1) * SEEDS_PER_BATCH\n",
    "            ))\n",
    "        else:\n",
    "            seed_ids = list(range(\n",
    "                50 + batch_number * SEEDS_PER_BATCH,\n",
    "                50 + (batch_number + 1) * SEEDS_PER_BATCH\n",
    "            ))\n",
    "\n",
    "        checkpoint_dir = f'{checkpoint_dir_base}/batch_{batch_number}'\n",
    "        batch_start = time.time()\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"{condition_name.upper()} - Batch {batch_number+1}/{TOTAL_BATCHES} - Seeds {seed_ids}\")\n",
    "        elapsed_total = time.time() - cond_start\n",
    "        if batch_number > 0:\n",
    "            avg_per_batch = elapsed_total / batch_number\n",
    "            remaining = avg_per_batch * (TOTAL_BATCHES - batch_number)\n",
    "            eta = datetime.now() + timedelta(seconds=remaining)\n",
    "            print(f\"ETA: {eta.strftime('%H:%M:%S')} ({remaining/60:.0f} min remaining)\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        try:\n",
    "            trainer = ParallelTrainer(\n",
    "                config=config, num_seeds=SEEDS_PER_BATCH,\n",
    "                seed_ids=seed_ids, checkpoint_dir=checkpoint_dir,\n",
    "                master_seed=42 + batch_number * 1000,\n",
    "            )\n",
    "            metrics = trainer.train(\n",
    "                num_iterations=num_iterations,\n",
    "                checkpoint_interval_minutes=CHECKPOINT_INTERVAL_MINUTES,\n",
    "                resume=RESUME,\n",
    "                print_interval=5,\n",
    "            )\n",
    "            batch_time = time.time() - batch_start\n",
    "\n",
    "            # Population crash detection\n",
    "            ps = trainer._parallel_state\n",
    "            alive = np.array(ps.env_state.agent_alive)  # (num_seeds, num_envs, max_agents)\n",
    "            pop_per_seed = alive.sum(axis=(1, 2)) / alive.shape[1]\n",
    "            batch_pops = [float(p) for p in pop_per_seed]\n",
    "            print(f\"  Final populations: {[f'{p:.0f}' for p in batch_pops]}\")\n",
    "            if all(p < 3 for p in batch_pops):\n",
    "                print(f\"  WARNING: All seeds in batch {batch_number} have near-zero population!\")\n",
    "                print(f\"  Population may have crashed. Check environment parameters.\")\n",
    "\n",
    "            all_results.append({\n",
    "                'batch': batch_number, 'seed_ids': seed_ids,\n",
    "                'metrics': metrics, 'success': True,\n",
    "                'time_seconds': batch_time, 'final_pops': batch_pops,\n",
    "            })\n",
    "            print(f\"Batch {batch_number} done in {batch_time/60:.1f} min\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR in batch {batch_number}: {e}\")\n",
    "            import traceback; traceback.print_exc()\n",
    "            all_results.append({\n",
    "                'batch': batch_number, 'seed_ids': seed_ids,\n",
    "                'success': False, 'error': str(e),\n",
    "            })\n",
    "        finally:\n",
    "            try: del trainer\n",
    "            except: pass\n",
    "            gc.collect()\n",
    "            jax.clear_caches()\n",
    "\n",
    "    cond_time = time.time() - cond_start\n",
    "    summary_path = f'{checkpoint_dir_base}/training_summary.pkl'\n",
    "    with open(summary_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'all_results': all_results,\n",
    "            'total_time_seconds': cond_time,\n",
    "            'condition': condition_name,\n",
    "            'total_steps': TOTAL_STEPS,\n",
    "            'config': {\n",
    "                'grid_size': config.env.grid_size,\n",
    "                'num_food': config.env.num_food,\n",
    "                'food_energy': config.evolution.food_energy,\n",
    "                'num_hidden': config.env.hidden_food.num_hidden,\n",
    "                'required_agents': config.env.hidden_food.required_agents,\n",
    "                'hidden_food_value_multiplier': config.env.hidden_food.hidden_food_value_multiplier,\n",
    "                'field_enabled': field_enabled,\n",
    "                'max_agents': config.evolution.max_agents,\n",
    "            },\n",
    "        }, f)\n",
    "    print(f\"\\n{condition_name} COMPLETE in {cond_time/3600:.1f} hours\")\n",
    "    print(f\"Summary saved to {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3 — Analysis\n",
    "\n",
    "Load checkpoints from both conditions, run evaluation episodes, compute statistics, and generate publication-quality figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json, pickle, gc, os\n",
    "import glob as glob_mod\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from src.configs import (\n",
    "    Config, TrainingMode, HiddenFoodConfig, EnvConfig, FieldConfig,\n",
    "    AgentConfig, TrainConfig, LogConfig, AnalysisConfig,\n",
    "    EvolutionConfig, SpecializationConfig, FreezeEvolveConfig, ArchiveConfig,\n",
    ")\n",
    "from src.agents.network import ActorCritic\n",
    "from src.agents.policy import get_deterministic_actions\n",
    "from src.environment.env import reset, step\n",
    "from src.environment.obs import get_observations\n",
    "from src.training.checkpointing import load_checkpoint\n",
    "from src.analysis.specialization import compute_weight_divergence\n",
    "from src.analysis.statistics import (\n",
    "    compute_iqm, compare_methods, welch_t_test,\n",
    "    mann_whitney_test, probability_of_improvement,\n",
    ")\n",
    "from src.analysis.paper_figures import setup_publication_style, save_figure\n",
    "\n",
    "setup_publication_style()\n",
    "%matplotlib inline\n",
    "\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/emergence-lab/survival_pressure_results'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def reconstruct_config(d):\n",
    "    \"\"\"Convert plain dict from load_checkpoint() back to Config dataclass.\"\"\"\n",
    "    if isinstance(d, Config):\n",
    "        return d\n",
    "\n",
    "    # Env config - handle nested hidden_food dict\n",
    "    env_d = dict(d.get('env', {}))\n",
    "    if 'hidden_food' in env_d and isinstance(env_d['hidden_food'], dict):\n",
    "        env_d['hidden_food'] = HiddenFoodConfig(**env_d['hidden_food'])\n",
    "\n",
    "    # Train config - handle TrainingMode enum\n",
    "    train_d = dict(d.get('train', {}))\n",
    "    if 'training_mode' in train_d and isinstance(train_d['training_mode'], str):\n",
    "        train_d['training_mode'] = TrainingMode(train_d['training_mode'])\n",
    "\n",
    "    # Agent config - handle hidden_dims tuple\n",
    "    agent_d = dict(d.get('agent', {}))\n",
    "    if 'hidden_dims' in agent_d and isinstance(agent_d['hidden_dims'], list):\n",
    "        agent_d['hidden_dims'] = tuple(agent_d['hidden_dims'])\n",
    "\n",
    "    return Config(\n",
    "        env=EnvConfig(**env_d),\n",
    "        field=FieldConfig(**d.get('field', {})),\n",
    "        agent=AgentConfig(**agent_d),\n",
    "        train=TrainConfig(**train_d),\n",
    "        log=LogConfig(**d.get('log', {})),\n",
    "        analysis=AnalysisConfig(**d.get('analysis', {})),\n",
    "        evolution=EvolutionConfig(**d.get('evolution', {})),\n",
    "        specialization=SpecializationConfig(**d.get('specialization', {})),\n",
    "        freeze_evolve=FreezeEvolveConfig(**d.get('freeze_evolve', {})),\n",
    "        archive=ArchiveConfig(**d.get('archive', {})),\n",
    "    )\n",
    "\n",
    "\n",
    "def discover_checkpoints(base_dir):\n",
    "    \"\"\"Find all checkpoint paths under base_dir/batch_*/seed_*/step_*.pkl.\"\"\"\n",
    "    paths = []\n",
    "    for batch_idx in range(10):\n",
    "        batch_dir = os.path.join(base_dir, f'batch_{batch_idx}')\n",
    "        if not os.path.exists(batch_dir):\n",
    "            continue\n",
    "        for seed_dir_name in sorted(os.listdir(batch_dir)):\n",
    "            seed_path = os.path.join(batch_dir, seed_dir_name)\n",
    "            if not os.path.isdir(seed_path):\n",
    "                continue\n",
    "            pkl_files = glob_mod.glob(os.path.join(seed_path, 'step_*.pkl'))\n",
    "            if pkl_files:\n",
    "                paths.append(sorted(pkl_files)[-1])  # Latest step\n",
    "    return paths\n",
    "\n",
    "\n",
    "def load_training_summary(base_dir):\n",
    "    \"\"\"Load training_summary.pkl and extract per-seed rewards + populations.\"\"\"\n",
    "    summary_path = os.path.join(base_dir, 'training_summary.pkl')\n",
    "    if not os.path.exists(summary_path):\n",
    "        return None, None\n",
    "    with open(summary_path, 'rb') as f:\n",
    "        summary = pickle.load(f)\n",
    "    rewards = []\n",
    "    populations = []\n",
    "    for batch in summary['all_results']:\n",
    "        if not batch.get('success', True):\n",
    "            continue\n",
    "        if 'metrics' in batch and 'mean_reward' in batch['metrics']:\n",
    "            rewards.extend(batch['metrics']['mean_reward'])\n",
    "        if 'metrics' in batch and 'population_size' in batch['metrics']:\n",
    "            populations.extend(batch['metrics']['population_size'])\n",
    "    if rewards:\n",
    "        return np.array(rewards), np.array(populations, dtype=float)\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def load_seed_data(ckpt_path):\n",
    "    \"\"\"Load checkpoint, extract network + config, free full checkpoint.\"\"\"\n",
    "    ckpt = load_checkpoint(ckpt_path)\n",
    "    config = reconstruct_config(ckpt['config'])\n",
    "    # agent_params from checkpoint: (num_envs, max_agents, ...)\n",
    "    # Take env 0 for divergence analysis: (max_agents, ...)\n",
    "    agent_params_env0 = jax.tree_util.tree_map(lambda x: x[0], ckpt['agent_params'])\n",
    "    network = ActorCritic(\n",
    "        hidden_dims=tuple(config.agent.hidden_dims), num_actions=6\n",
    "    )\n",
    "    result = {\n",
    "        'params': ckpt['params'],\n",
    "        'agent_params': agent_params_env0,\n",
    "        'config': config,\n",
    "        'network': network,\n",
    "        'seed_id': ckpt.get('seed_id', -1),\n",
    "    }\n",
    "    del ckpt\n",
    "    return result\n",
    "\n",
    "\n",
    "# run_hidden_food_eval() already defined in Cell 3\n",
    "print(\"Analysis utilities loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Training Data ---\n",
    "CHECKPOINT_BASE = '/content/drive/MyDrive/emergence-lab'\n",
    "\n",
    "field_on_dir = f'{CHECKPOINT_BASE}/survival_pressure_on'\n",
    "field_off_dir = f'{CHECKPOINT_BASE}/survival_pressure_off'\n",
    "\n",
    "field_on_ckpt_paths = discover_checkpoints(field_on_dir)\n",
    "field_off_ckpt_paths = discover_checkpoints(field_off_dir)\n",
    "\n",
    "print(f\"Found {len(field_on_ckpt_paths)}/30 Field ON seeds\")\n",
    "print(f\"Found {len(field_off_ckpt_paths)}/30 Field OFF seeds\")\n",
    "\n",
    "# Load training summaries\n",
    "on_rewards, on_populations = load_training_summary(field_on_dir)\n",
    "off_rewards, off_populations = load_training_summary(field_off_dir)\n",
    "\n",
    "if on_rewards is not None:\n",
    "    print(f\"\\nField ON  training rewards: {len(on_rewards)} seeds, mean={np.mean(on_rewards):.3f}\")\n",
    "if off_rewards is not None:\n",
    "    print(f\"Field OFF training rewards: {len(off_rewards)} seeds, mean={np.mean(off_rewards):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Reward Statistics ---\n",
    "if on_rewards is not None and off_rewards is not None:\n",
    "    comparison = compare_methods(\n",
    "        {'Field ON': on_rewards, 'Field OFF': off_rewards},\n",
    "        alpha=0.05, n_bootstrap=10000, seed=42,\n",
    "    )\n",
    "    print(comparison.summary)\n",
    "\n",
    "    # Additional tests\n",
    "    welch = welch_t_test(on_rewards, off_rewards)\n",
    "    mw = mann_whitney_test(on_rewards, off_rewards)\n",
    "    poi = probability_of_improvement(on_rewards, off_rewards, seed=42)\n",
    "\n",
    "    print(f\"\\nWelch's t-test: t={welch.statistic:.3f}, p={welch.p_value:.4f}, d={welch.effect_size:.3f}\")\n",
    "    print(f\"Mann-Whitney U: U={mw.statistic:.1f}, p={mw.p_value:.4f}\")\n",
    "    print(f\"P(Field ON > Field OFF): {poi['prob_x_better']:.3f}\")\n",
    "    print(f\"P(Field OFF > Field ON): {poi['prob_y_better']:.3f}\")\n",
    "else:\n",
    "    print(\"Training summaries not found. Run training cells first.\")\n",
    "    comparison = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Comparison Plots ---\n",
    "COLOR_ON = '#009988'\n",
    "COLOR_OFF = '#BBBBBB'\n",
    "\n",
    "if on_rewards is not None and off_rewards is not None:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "    # Panel 1: IQM bars with CI\n",
    "    ax = axes[0]\n",
    "    iqm_on = compute_iqm(on_rewards, seed=42)\n",
    "    iqm_off = compute_iqm(off_rewards, seed=42)\n",
    "    bars = ax.bar(\n",
    "        ['Field ON', 'Field OFF'],\n",
    "        [iqm_on.iqm, iqm_off.iqm],\n",
    "        color=[COLOR_ON, COLOR_OFF],\n",
    "        edgecolor='black', linewidth=0.8,\n",
    "    )\n",
    "    ax.errorbar(\n",
    "        [0, 1],\n",
    "        [iqm_on.iqm, iqm_off.iqm],\n",
    "        yerr=[\n",
    "            [iqm_on.iqm - iqm_on.ci_lower, iqm_off.iqm - iqm_off.ci_lower],\n",
    "            [iqm_on.ci_upper - iqm_on.iqm, iqm_off.ci_upper - iqm_off.iqm],\n",
    "        ],\n",
    "        fmt='none', color='black', capsize=5,\n",
    "    )\n",
    "    ax.set_ylabel('IQM Reward')\n",
    "    ax.set_title('Training Reward (IQM + 95% CI)')\n",
    "\n",
    "    # Panel 2: Violin + swarm\n",
    "    ax = axes[1]\n",
    "    parts = ax.violinplot(\n",
    "        [on_rewards, off_rewards], positions=[0, 1],\n",
    "        showmeans=True, showmedians=True,\n",
    "    )\n",
    "    for i, pc in enumerate(parts['bodies']):\n",
    "        pc.set_facecolor([COLOR_ON, COLOR_OFF][i])\n",
    "        pc.set_alpha(0.6)\n",
    "    # Swarm overlay\n",
    "    jitter_on = np.random.default_rng(42).uniform(-0.1, 0.1, len(on_rewards))\n",
    "    jitter_off = np.random.default_rng(43).uniform(-0.1, 0.1, len(off_rewards))\n",
    "    ax.scatter(jitter_on, on_rewards, c=COLOR_ON, s=15, alpha=0.7, zorder=3)\n",
    "    ax.scatter(1 + jitter_off, off_rewards, c=COLOR_OFF, s=15, alpha=0.7, zorder=3, edgecolors='gray')\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels(['Field ON', 'Field OFF'])\n",
    "    ax.set_ylabel('Final Reward')\n",
    "    ax.set_title('Reward Distribution')\n",
    "\n",
    "    # Panel 3: Population histogram\n",
    "    ax = axes[2]\n",
    "    if on_populations is not None and off_populations is not None:\n",
    "        bins = np.linspace(\n",
    "            min(on_populations.min(), off_populations.min()),\n",
    "            max(on_populations.max(), off_populations.max()),\n",
    "            20,\n",
    "        )\n",
    "        ax.hist(on_populations, bins=bins, alpha=0.6, color=COLOR_ON, label='Field ON', edgecolor='black')\n",
    "        ax.hist(off_populations, bins=bins, alpha=0.6, color=COLOR_OFF, label='Field OFF', edgecolor='black')\n",
    "        ax.set_xlabel('Final Population')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_title('Population Distribution')\n",
    "        ax.legend()\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No population data', ha='center', va='center', transform=ax.transAxes)\n",
    "\n",
    "    fig.suptitle('Survival Pressure: Training Comparison', fontsize=16, y=1.02)\n",
    "    fig.tight_layout()\n",
    "    save_figure(fig, os.path.join(OUTPUT_DIR, 'training_comparison'))\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training data to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load & Eval All Checkpoints ---\n",
    "hf_eval_on = []\n",
    "hf_eval_off = []\n",
    "divergence_on = []\n",
    "divergence_off = []\n",
    "\n",
    "for cond_name, ckpt_paths, eval_list, div_list in [\n",
    "    (\"Field ON\", field_on_ckpt_paths, hf_eval_on, divergence_on),\n",
    "    (\"Field OFF\", field_off_ckpt_paths, hf_eval_off, divergence_off),\n",
    "]:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Evaluating {cond_name}: {len(ckpt_paths)} seeds\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    for i, ckpt_path in enumerate(ckpt_paths):\n",
    "        print(f\"  [{cond_name}] Eval seed {i+1}/{len(ckpt_paths)}: {os.path.basename(ckpt_path)}\")\n",
    "        seed_data = load_seed_data(ckpt_path)\n",
    "        key = jax.random.PRNGKey(42 + i)\n",
    "\n",
    "        hf_result = run_hidden_food_eval(\n",
    "            seed_data['network'], seed_data['params'],\n",
    "            seed_data['config'], key, num_episodes=1,\n",
    "        )\n",
    "        hf_result['seed_id'] = seed_data['seed_id']\n",
    "        eval_list.append(hf_result)\n",
    "\n",
    "        # Compute weight divergence: agent_params is (max_agents, ...) from env 0\n",
    "        max_agents = seed_data['config'].evolution.max_agents\n",
    "        alive_mask = np.ones(max_agents, dtype=bool)\n",
    "        div = compute_weight_divergence(seed_data['agent_params'], alive_mask)\n",
    "        div_list.append({\n",
    "            'seed_id': seed_data['seed_id'],\n",
    "            'mean_divergence': float(div['mean_divergence']),\n",
    "            'max_divergence': float(div['max_divergence']),\n",
    "        })\n",
    "\n",
    "        del seed_data\n",
    "        gc.collect()\n",
    "\n",
    "        # Intermediate save - survives crashes\n",
    "        progress_path = os.path.join(\n",
    "            OUTPUT_DIR,\n",
    "            f'eval_progress_{cond_name.lower().replace(\" \", \"_\")}.pkl'\n",
    "        )\n",
    "        with open(progress_path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'condition': cond_name,\n",
    "                'completed_seeds': i + 1,\n",
    "                'total_seeds': len(ckpt_paths),\n",
    "                'hf_eval': list(eval_list),\n",
    "                'divergence': list(div_list),\n",
    "            }, f)\n",
    "\n",
    "# Extract numpy arrays for analysis\n",
    "on_hf_revealed = np.array([r['hidden_food_revealed'] for r in hf_eval_on])\n",
    "off_hf_revealed = np.array([r['hidden_food_revealed'] for r in hf_eval_off])\n",
    "on_hf_collected = np.array([r['hidden_food_collected'] for r in hf_eval_on])\n",
    "off_hf_collected = np.array([r['hidden_food_collected'] for r in hf_eval_off])\n",
    "on_regular_food = np.array([r['regular_food_collected'] for r in hf_eval_on])\n",
    "off_regular_food = np.array([r['regular_food_collected'] for r in hf_eval_off])\n",
    "on_hf_energy = np.array([r['hidden_food_energy'] for r in hf_eval_on])\n",
    "off_hf_energy = np.array([r['hidden_food_energy'] for r in hf_eval_off])\n",
    "on_regular_energy = np.array([r['regular_food_energy'] for r in hf_eval_on])\n",
    "off_regular_energy = np.array([r['regular_food_energy'] for r in hf_eval_off])\n",
    "on_total_reward = np.array([r['total_reward'] for r in hf_eval_on])\n",
    "off_total_reward = np.array([r['total_reward'] for r in hf_eval_off])\n",
    "on_final_pop = np.array([r['final_population'] for r in hf_eval_on])\n",
    "off_final_pop = np.array([r['final_population'] for r in hf_eval_off])\n",
    "on_mean_div = np.array([d['mean_divergence'] for d in divergence_on])\n",
    "off_mean_div = np.array([d['mean_divergence'] for d in divergence_off])\n",
    "\n",
    "print(f\"\\nEval complete: {len(hf_eval_on)} ON seeds, {len(hf_eval_off)} OFF seeds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hidden Food Statistics ---\n",
    "metrics_to_test = [\n",
    "    ('hidden_food_revealed', on_hf_revealed, off_hf_revealed),\n",
    "    ('hidden_food_collected', on_hf_collected, off_hf_collected),\n",
    "    ('regular_food_collected', on_regular_food, off_regular_food),\n",
    "    ('hidden_food_energy', on_hf_energy, off_hf_energy),\n",
    "    ('total_reward', on_total_reward, off_total_reward),\n",
    "    ('final_population', on_final_pop.astype(float), off_final_pop.astype(float)),\n",
    "]\n",
    "\n",
    "print(f\"{'Metric':<30} {'ON mean':>10} {'OFF mean':>10} {'t':>8} {'p':>10} {'d':>8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "stat_results = {}\n",
    "for name, on_vals, off_vals in metrics_to_test:\n",
    "    on_mean = np.mean(on_vals)\n",
    "    off_mean = np.mean(off_vals)\n",
    "    on_std = np.std(on_vals)\n",
    "    off_std = np.std(off_vals)\n",
    "\n",
    "    # Handle all-zero case gracefully\n",
    "    if on_std == 0 and off_std == 0:\n",
    "        t_val, p_val, d_val = 0.0, 1.0, 0.0\n",
    "    else:\n",
    "        welch = welch_t_test(on_vals, off_vals)\n",
    "        t_val = welch.statistic\n",
    "        p_val = welch.p_value\n",
    "        d_val = welch.effect_size\n",
    "\n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"\"\n",
    "    print(f\"{name:<30} {on_mean:>10.2f} {off_mean:>10.2f} {t_val:>8.2f} {p_val:>10.4f} {d_val:>8.2f} {sig}\")\n",
    "\n",
    "    stat_results[name] = {\n",
    "        'on_mean': on_mean, 'on_std': on_std,\n",
    "        'off_mean': off_mean, 'off_std': off_std,\n",
    "        't': t_val, 'p': p_val, 'd': d_val,\n",
    "    }\n",
    "\n",
    "print(\"\\n* p<0.05  ** p<0.01  *** p<0.001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hidden Food Coordination Plots ---\n",
    "fig, axes = plt.subplots(1, 5, figsize=(24, 5))\n",
    "\n",
    "# Panel 1: HF revealed\n",
    "ax = axes[0]\n",
    "means = [np.mean(on_hf_revealed), np.mean(off_hf_revealed)]\n",
    "stds = [np.std(on_hf_revealed), np.std(off_hf_revealed)]\n",
    "bars = ax.bar(['Field ON', 'Field OFF'], means, yerr=stds, color=[COLOR_ON, COLOR_OFF],\n",
    "              edgecolor='black', linewidth=0.8, capsize=5)\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Hidden Food Revealed')\n",
    "if means[0] == 0 and means[1] == 0:\n",
    "    ax.annotate('Both zero - coordination\\nnot yet achieved',\n",
    "                xy=(0.5, 0.5), xycoords='axes fraction', ha='center', va='center',\n",
    "                fontsize=9, color='gray')\n",
    "\n",
    "# Panel 2: HF collected\n",
    "ax = axes[1]\n",
    "means = [np.mean(on_hf_collected), np.mean(off_hf_collected)]\n",
    "stds = [np.std(on_hf_collected), np.std(off_hf_collected)]\n",
    "ax.bar(['Field ON', 'Field OFF'], means, yerr=stds, color=[COLOR_ON, COLOR_OFF],\n",
    "       edgecolor='black', linewidth=0.8, capsize=5)\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Hidden Food Collected')\n",
    "\n",
    "# Panel 3: Regular food\n",
    "ax = axes[2]\n",
    "means = [np.mean(on_regular_food), np.mean(off_regular_food)]\n",
    "stds = [np.std(on_regular_food), np.std(off_regular_food)]\n",
    "ax.bar(['Field ON', 'Field OFF'], means, yerr=stds, color=[COLOR_ON, COLOR_OFF],\n",
    "       edgecolor='black', linewidth=0.8, capsize=5)\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Regular Food Collected')\n",
    "\n",
    "# Panel 4: Energy breakdown (stacked bar)\n",
    "ax = axes[3]\n",
    "on_reg_e = np.mean(on_regular_energy)\n",
    "on_hf_e = np.mean(on_hf_energy)\n",
    "off_reg_e = np.mean(off_regular_energy)\n",
    "off_hf_e = np.mean(off_hf_energy)\n",
    "ax.bar(['Field ON', 'Field OFF'], [on_reg_e, off_reg_e],\n",
    "       color=[COLOR_ON, COLOR_OFF], edgecolor='black', linewidth=0.8, label='Regular food')\n",
    "ax.bar(['Field ON', 'Field OFF'], [on_hf_e, off_hf_e],\n",
    "       bottom=[on_reg_e, off_reg_e],\n",
    "       color=['#00665f', '#888888'], edgecolor='black', linewidth=0.8, label='Hidden food')\n",
    "ax.set_ylabel('Energy')\n",
    "ax.set_title('Energy Breakdown')\n",
    "ax.legend()\n",
    "\n",
    "# Panel 5: HF collected violin\n",
    "ax = axes[4]\n",
    "if np.any(on_hf_collected > 0) or np.any(off_hf_collected > 0):\n",
    "    parts = ax.violinplot(\n",
    "        [on_hf_collected, off_hf_collected], positions=[0, 1],\n",
    "        showmeans=True, showmedians=True,\n",
    "    )\n",
    "    for i, pc in enumerate(parts['bodies']):\n",
    "        pc.set_facecolor([COLOR_ON, COLOR_OFF][i])\n",
    "        pc.set_alpha(0.6)\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels(['Field ON', 'Field OFF'])\n",
    "else:\n",
    "    ax.bar(['Field ON', 'Field OFF'], [0, 0], color=[COLOR_ON, COLOR_OFF],\n",
    "           edgecolor='black', linewidth=0.8)\n",
    "    ax.annotate('No hidden food collected\\nin either condition',\n",
    "                xy=(0.5, 0.5), xycoords='axes fraction', ha='center', va='center',\n",
    "                fontsize=9, color='gray')\n",
    "ax.set_ylabel('Hidden Food Collected')\n",
    "ax.set_title('HF Collected Distribution')\n",
    "\n",
    "fig.suptitle('Survival Pressure: Hidden Food Coordination', fontsize=16, y=1.02)\n",
    "fig.tight_layout()\n",
    "save_figure(fig, os.path.join(OUTPUT_DIR, 'hidden_food_coordination'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Divergence & Correlation Plots ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Panel 1: Weight divergence histogram\n",
    "ax = axes[0, 0]\n",
    "if len(on_mean_div) > 0 and len(off_mean_div) > 0:\n",
    "    bins = np.linspace(\n",
    "        min(on_mean_div.min(), off_mean_div.min()),\n",
    "        max(on_mean_div.max(), off_mean_div.max()),\n",
    "        20,\n",
    "    )\n",
    "    ax.hist(on_mean_div, bins=bins, alpha=0.6, color=COLOR_ON, label='Field ON', edgecolor='black')\n",
    "    ax.hist(off_mean_div, bins=bins, alpha=0.6, color=COLOR_OFF, label='Field OFF', edgecolor='black')\n",
    "    ax.set_xlabel('Mean Weight Divergence')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Weight Divergence Distribution')\n",
    "    ax.legend()\n",
    "\n",
    "# Panel 2: Eval population histogram\n",
    "ax = axes[0, 1]\n",
    "if len(on_final_pop) > 0 and len(off_final_pop) > 0:\n",
    "    all_pops = np.concatenate([on_final_pop, off_final_pop])\n",
    "    bins = np.linspace(all_pops.min(), all_pops.max(), 20)\n",
    "    ax.hist(on_final_pop, bins=bins, alpha=0.6, color=COLOR_ON, label='Field ON', edgecolor='black')\n",
    "    ax.hist(off_final_pop, bins=bins, alpha=0.6, color=COLOR_OFF, label='Field OFF', edgecolor='black')\n",
    "    ax.set_xlabel('Final Population (eval)')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Eval Population Distribution')\n",
    "    ax.legend()\n",
    "\n",
    "# Panel 3: HF collected vs divergence scatter (with Pearson r)\n",
    "ax = axes[1, 0]\n",
    "all_hf = np.concatenate([on_hf_collected, off_hf_collected])\n",
    "all_div = np.concatenate([on_mean_div, off_mean_div])\n",
    "colors = [COLOR_ON] * len(on_hf_collected) + [COLOR_OFF] * len(off_hf_collected)\n",
    "ax.scatter(all_div, all_hf, c=colors, s=30, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "if len(all_div) > 2 and np.std(all_div) > 0 and np.std(all_hf) > 0:\n",
    "    r, p = scipy_stats.pearsonr(all_div, all_hf)\n",
    "    ax.set_title(f'HF Collected vs Divergence (r={r:.3f}, p={p:.3f})')\n",
    "else:\n",
    "    ax.set_title('HF Collected vs Divergence')\n",
    "ax.set_xlabel('Mean Weight Divergence')\n",
    "ax.set_ylabel('Hidden Food Collected')\n",
    "# Legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor=COLOR_ON, markersize=8, label='Field ON'),\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor=COLOR_OFF, markersize=8, label='Field OFF'),\n",
    "]\n",
    "ax.legend(handles=legend_elements)\n",
    "\n",
    "# Panel 4: Reward vs population scatter\n",
    "ax = axes[1, 1]\n",
    "all_rew = np.concatenate([on_total_reward, off_total_reward])\n",
    "all_pop = np.concatenate([on_final_pop.astype(float), off_final_pop.astype(float)])\n",
    "ax.scatter(all_pop, all_rew, c=colors, s=30, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "if len(all_pop) > 2 and np.std(all_pop) > 0 and np.std(all_rew) > 0:\n",
    "    r, p = scipy_stats.pearsonr(all_pop, all_rew)\n",
    "    ax.set_title(f'Reward vs Population (r={r:.3f}, p={p:.3f})')\n",
    "else:\n",
    "    ax.set_title('Reward vs Population')\n",
    "ax.set_xlabel('Final Population')\n",
    "ax.set_ylabel('Total Reward')\n",
    "ax.legend(handles=legend_elements)\n",
    "\n",
    "fig.suptitle('Survival Pressure: Divergence & Correlations', fontsize=16, y=1.02)\n",
    "fig.tight_layout()\n",
    "save_figure(fig, os.path.join(OUTPUT_DIR, 'divergence_correlation'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Population Trajectory Plot ---\n",
    "# Extract per-batch population data from training summaries\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "has_trajectory = False\n",
    "for cond_name, base_dir, color, label in [\n",
    "    ('field_on', field_on_dir, COLOR_ON, 'Field ON'),\n",
    "    ('field_off', field_off_dir, COLOR_OFF, 'Field OFF'),\n",
    "]:\n",
    "    summary_path = os.path.join(base_dir, 'training_summary.pkl')\n",
    "    if not os.path.exists(summary_path):\n",
    "        continue\n",
    "    with open(summary_path, 'rb') as f:\n",
    "        summary = pickle.load(f)\n",
    "\n",
    "    # Collect final populations per batch (each batch has SEEDS_PER_BATCH values)\n",
    "    batch_pops = []\n",
    "    for batch in summary['all_results']:\n",
    "        if batch.get('success', False) and 'final_pops' in batch:\n",
    "            batch_pops.append(batch['final_pops'])\n",
    "\n",
    "    if batch_pops:\n",
    "        has_trajectory = True\n",
    "        # Each entry is a list of per-seed populations\n",
    "        all_pops = np.array(batch_pops)  # (num_batches, seeds_per_batch)\n",
    "        batch_means = all_pops.mean(axis=1)\n",
    "        batch_stds = all_pops.std(axis=1)\n",
    "        batch_indices = np.arange(len(batch_means))\n",
    "\n",
    "        ax.plot(batch_indices, batch_means, color=color, label=label, linewidth=2)\n",
    "        ax.fill_between(\n",
    "            batch_indices, batch_means - batch_stds, batch_means + batch_stds,\n",
    "            color=color, alpha=0.2,\n",
    "        )\n",
    "\n",
    "if has_trajectory:\n",
    "    ax.set_xlabel('Batch Index')\n",
    "    ax.set_ylabel('Final Population (mean across seeds in batch)')\n",
    "    ax.set_title('Population Trajectory Across Batches')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    save_figure(fig, os.path.join(OUTPUT_DIR, 'population_trajectory'))\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No population trajectory data available.\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Summary Report ---\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "report = f\"\"\"# Survival Pressure Experiment: Results Summary\n",
    "\n",
    "**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n",
    "\n",
    "## Experimental Setup\n",
    "- **Grid**: 40x40, **Regular food**: 10 (scarce), **Food energy**: 60 (reduced)\n",
    "- **Hidden food**: 8 items, require 5 agents within distance 3, value 10x (600 energy each)\n",
    "- **Max agents**: 64, **Starting energy**: 200, **Reproduce threshold**: 120\n",
    "- **Steps**: 10M per seed, **Seeds**: {len(hf_eval_on)} ON + {len(hf_eval_off)} OFF\n",
    "\n",
    "## Training Rewards\n",
    "\"\"\"\n",
    "\n",
    "if on_rewards is not None and off_rewards is not None:\n",
    "    report += f\"\"\"| Metric | Field ON | Field OFF |\n",
    "|--------|----------|----------|\n",
    "| Mean reward | {np.mean(on_rewards):.4f} | {np.mean(off_rewards):.4f} |\n",
    "| Std reward | {np.std(on_rewards):.4f} | {np.std(off_rewards):.4f} |\n",
    "| IQM | {compute_iqm(on_rewards, seed=42).iqm:.4f} | {compute_iqm(off_rewards, seed=42).iqm:.4f} |\n",
    "\"\"\"\n",
    "\n",
    "report += f\"\"\"\n",
    "## Hidden Food Coordination (Eval Episodes)\n",
    "\n",
    "| Metric | Field ON | Field OFF | t-stat | p-value | Cohen's d |\n",
    "|--------|----------|----------|--------|---------|----------|\n",
    "\"\"\"\n",
    "\n",
    "for name, sr in stat_results.items():\n",
    "    sig = \"***\" if sr['p'] < 0.001 else \"**\" if sr['p'] < 0.01 else \"*\" if sr['p'] < 0.05 else \"\"\n",
    "    report += f\"| {name} | {sr['on_mean']:.2f} +/- {sr['on_std']:.2f} | {sr['off_mean']:.2f} +/- {sr['off_std']:.2f} | {sr['t']:.2f} | {sr['p']:.4f}{sig} | {sr['d']:.2f} |\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "## Weight Divergence\n",
    "| Metric | Field ON | Field OFF |\n",
    "|--------|----------|----------|\n",
    "| Mean divergence | {np.mean(on_mean_div):.4f} +/- {np.std(on_mean_div):.4f} | {np.mean(off_mean_div):.4f} +/- {np.std(off_mean_div):.4f} |\n",
    "\n",
    "## Key Findings\n",
    "\"\"\"\n",
    "\n",
    "# Auto-generate key findings\n",
    "if 'total_reward' in stat_results:\n",
    "    sr = stat_results['total_reward']\n",
    "    if sr['p'] < 0.05:\n",
    "        winner = \"Field ON\" if sr['on_mean'] > sr['off_mean'] else \"Field OFF\"\n",
    "        report += f\"- **{winner} significantly outperforms** on total reward (p={sr['p']:.4f}, d={sr['d']:.2f})\\n\"\n",
    "    else:\n",
    "        report += f\"- No significant difference in total reward (p={sr['p']:.4f})\\n\"\n",
    "\n",
    "if 'hidden_food_collected' in stat_results:\n",
    "    sr = stat_results['hidden_food_collected']\n",
    "    if sr['on_mean'] > 0 or sr['off_mean'] > 0:\n",
    "        if sr['p'] < 0.05:\n",
    "            winner = \"Field ON\" if sr['on_mean'] > sr['off_mean'] else \"Field OFF\"\n",
    "            report += f\"- **{winner} collects more hidden food** (p={sr['p']:.4f}, d={sr['d']:.2f})\\n\"\n",
    "        else:\n",
    "            report += f\"- No significant difference in hidden food collection (p={sr['p']:.4f})\\n\"\n",
    "    else:\n",
    "        report += \"- Neither condition achieved hidden food collection in eval episodes\\n\"\n",
    "\n",
    "if 'final_population' in stat_results:\n",
    "    sr = stat_results['final_population']\n",
    "    if sr['p'] < 0.05:\n",
    "        winner = \"Field ON\" if sr['on_mean'] > sr['off_mean'] else \"Field OFF\"\n",
    "        report += f\"- **{winner} sustains larger populations** (p={sr['p']:.4f}, d={sr['d']:.2f})\\n\"\n",
    "    else:\n",
    "        report += f\"- No significant population difference (p={sr['p']:.4f})\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "## Output Files\n",
    "- Figures: `{OUTPUT_DIR}/training_comparison.{{pdf,png}}`\n",
    "- Figures: `{OUTPUT_DIR}/hidden_food_coordination.{{pdf,png}}`\n",
    "- Figures: `{OUTPUT_DIR}/divergence_correlation.{{pdf,png}}`\n",
    "- Figures: `{OUTPUT_DIR}/population_trajectory.{{pdf,png}}`\n",
    "- Results: `{OUTPUT_DIR}/all_results.json`\n",
    "- Results: `{OUTPUT_DIR}/all_results.pkl`\n",
    "- Report: `{OUTPUT_DIR}/summary_report.md`\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save All Results ---\n",
    "\n",
    "# JSON-serializable results\n",
    "json_results = {\n",
    "    'experiment': 'survival_pressure',\n",
    "    'date': datetime.now().isoformat(),\n",
    "    'config': {\n",
    "        'grid_size': 40, 'num_food': 10, 'food_energy': 60,\n",
    "        'num_hidden': 8, 'required_agents': 5,\n",
    "        'hidden_food_value_multiplier': 10.0,\n",
    "        'max_agents': 64, 'total_steps': TOTAL_STEPS,\n",
    "    },\n",
    "    'n_seeds': {\n",
    "        'field_on': len(hf_eval_on),\n",
    "        'field_off': len(hf_eval_off),\n",
    "    },\n",
    "    'training_rewards': {},\n",
    "    'eval_stats': {},\n",
    "    'divergence': {\n",
    "        'field_on_mean': float(np.mean(on_mean_div)) if len(on_mean_div) > 0 else None,\n",
    "        'field_on_std': float(np.std(on_mean_div)) if len(on_mean_div) > 0 else None,\n",
    "        'field_off_mean': float(np.mean(off_mean_div)) if len(off_mean_div) > 0 else None,\n",
    "        'field_off_std': float(np.std(off_mean_div)) if len(off_mean_div) > 0 else None,\n",
    "    },\n",
    "}\n",
    "\n",
    "if on_rewards is not None and off_rewards is not None:\n",
    "    json_results['training_rewards'] = {\n",
    "        'field_on_mean': float(np.mean(on_rewards)),\n",
    "        'field_on_std': float(np.std(on_rewards)),\n",
    "        'field_off_mean': float(np.mean(off_rewards)),\n",
    "        'field_off_std': float(np.std(off_rewards)),\n",
    "    }\n",
    "\n",
    "for name, sr in stat_results.items():\n",
    "    json_results['eval_stats'][name] = {\n",
    "        'on_mean': float(sr['on_mean']),\n",
    "        'on_std': float(sr['on_std']),\n",
    "        'off_mean': float(sr['off_mean']),\n",
    "        'off_std': float(sr['off_std']),\n",
    "        't_statistic': float(sr['t']),\n",
    "        'p_value': float(sr['p']),\n",
    "        'cohens_d': float(sr['d']),\n",
    "    }\n",
    "\n",
    "# Save JSON\n",
    "json_path = os.path.join(OUTPUT_DIR, 'all_results.json')\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(json_results, f, indent=2)\n",
    "print(f\"JSON saved: {json_path}\")\n",
    "\n",
    "# Save pickle (includes numpy arrays)\n",
    "pkl_results = {\n",
    "    **json_results,\n",
    "    'hf_eval_on': hf_eval_on,\n",
    "    'hf_eval_off': hf_eval_off,\n",
    "    'divergence_on': divergence_on,\n",
    "    'divergence_off': divergence_off,\n",
    "    'on_rewards': on_rewards,\n",
    "    'off_rewards': off_rewards,\n",
    "    'on_populations': on_populations,\n",
    "    'off_populations': off_populations,\n",
    "}\n",
    "pkl_path = os.path.join(OUTPUT_DIR, 'all_results.pkl')\n",
    "with open(pkl_path, 'wb') as f:\n",
    "    pickle.dump(pkl_results, f)\n",
    "print(f\"Pickle saved: {pkl_path}\")\n",
    "\n",
    "# Save markdown report\n",
    "md_path = os.path.join(OUTPUT_DIR, 'summary_report.md')\n",
    "with open(md_path, 'w') as f:\n",
    "    f.write(report)\n",
    "print(f\"Report saved: {md_path}\")\n",
    "\n",
    "print(f\"\\nAll results saved to {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}